{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](https://drive.corp.amazon.com/view/bwernes@/MLU_Logo.png?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP2 Lecture 2 Support Notebook\n",
    "\n",
    "#### NOTE: run this with conda_pytorch_p36 kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "<p>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#MLP-with-Time-Step\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>\n",
    "        MLP with Time Step\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-multilayer-perceptron-models-for-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with Time Step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate MLP Models\n",
    "Multilayer Perceptrons, or MLPs for short, can be used to model univariate time series forecasting problems.\n",
    "\n",
    "Univariate time series are a dataset comprised of a single series of observations with a temporal ordering and a model is required to learn from the series of past observations to predict the next value in the sequence.\n",
    "\n",
    "This section is divided into two parts; they are:\n",
    "\n",
    "Data Preparation\n",
    "MLP Model\n",
    "Data Preparation\n",
    "Before a univariate series can be modeled, it must be prepared.\n",
    "\n",
    "The MLP model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the model can learn.\n",
    "\n",
    "Consider a given univariate sequence:\n",
    "[10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned.\n",
    "\n",
    "|X|y|\n",
    "|---|---|\n",
    "10, 20, 30|\t\t40\n",
    "20, 30, 40|\t\t50\n",
    "30, 40, 50|\t\t60\n",
    "...\n",
    "\n",
    "The split_sequence() function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate output mlp example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device : Tesla K80\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Default GPU Device : {}'.format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    print('No GPU available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can demonstrate this function on our small contrived dataset above.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30] 40\n",
      "[20 30 40] 50\n",
      "[30 40 50] 60\n",
      "[40 50 60] 70\n",
      "[50 60 70] 80\n",
      "[60 70 80] 90\n"
     ]
    }
   ],
   "source": [
    "# univariate data preparation\n",
    "from numpy import array\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example splits the univariate series into six samples where each sample has three input time steps and one output time step.\n",
    "\n",
    "Now that we know how to prepare a univariate series for modeling, let’s look at developing an MLP model that can learn the mapping of inputs to outputs.\n",
    "\n",
    "## MLP Model\n",
    "\n",
    "A simple MLP model has a single hidden layer of nodes, and an output layer used to make a prediction.\n",
    "\n",
    "We can define an MLP for univariate time series forecasting as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "model = nn.Sequential(nn.Linear(n_steps, 100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(100, 1))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important in the definition is the shape of the input; that is what the model expects as input for each sample in terms of the number of time steps.\n",
    "\n",
    "The number of time steps as input is the number we chose when preparing our dataset as an argument to the split_sequence() function.\n",
    "\n",
    "The input dimension for each sample is specified in the input_dim argument on the definition of first hidden layer. Technically, the model will view each time step as a separate feature instead of separate time steps.\n",
    "\n",
    "We almost always have multiple samples, therefore, the model will expect the input component of training data to have the dimensions or shape:\n",
    "\n",
    "[samples, features]\n",
    "\n",
    "Our split_sequence() function in the previous section outputs the X with the shape [samples, features] ready to use for modeling.\n",
    "\n",
    "The model is fit using the efficient Adam version of stochastic gradient descent and optimized using the mean squared error, or ‘mse‘, loss function.\n",
    "\n",
    "Once the model is defined, we can fit it on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "\n",
    "def train(features, labels):\n",
    "    epochs = 2000\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "\n",
    "        for feature, label in zip(features, labels):\n",
    "            # Zero grad on every pass because stochastic gradient descent        \n",
    "\n",
    "            feature_tensor = torch.Tensor(feature)\n",
    "            label_tensor = torch.Tensor([label])\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(feature_tensor)\n",
    "            loss = criterion(output, label_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "# model.fit(X, y, epochs=2000, verbose=0)\n",
    "train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is fit, we can use it to make a prediction.\n",
    "\n",
    "We can predict the next value in the sequence by providing the input:\n",
    "\n",
    "[70, 80, 90]\n",
    "\n",
    "And expecting the model to predict something like:\n",
    "\n",
    "[100]\n",
    "\n",
    "The model expects the input shape to be two-dimensional with [samples, features], therefore, we must reshape the single input sample before making the prediction, e.g with the shape [1, 3] for 1 sample and 3 time steps used as input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([102.0702], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = torch.Tensor([70, 80, 90])\n",
    "yhat = model.forward(x_input)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tie all of this together and demonstrate how to develop an MLP for univariate time series forecasting and make a single prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([101.4625], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# univariate mlp example\n",
    "from numpy import array\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "\n",
    "# define model\n",
    "model = nn.Sequential(nn.Linear(n_steps, 100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(100, 1))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# fit model\n",
    "train(X, y)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = torch.Tensor([70, 80, 90])\n",
    "yhat = model.forward(x_input)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prepares the data, fits the model, and makes a prediction.\n",
    "\n",
    "Your results may vary given the stochastic nature of the algorithm; try running the example a few times.\n",
    "\n",
    "We can see that the model predicts the next value in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate MLP Models\n",
    "\n",
    "Multivariate time series data means data where there is more than one observation for each time step.\n",
    "\n",
    "There are two main models that we may require with multivariate time series data; they are:\n",
    "\n",
    "Multiple Input Series.\n",
    "Multiple Parallel Series.\n",
    "Let’s take a look at each in turn.\n",
    "\n",
    "## Multiple Input Series\n",
    "\n",
    "A problem may have two or more parallel input time series and an output time series that is dependent on the input time series.\n",
    "\n",
    "The input time series are parallel because each series has an observation at the same time step.\n",
    "\n",
    "We can demonstrate this with a simple example of two parallel input time series where the output series is the simple addition of the input series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape these three arrays of data as a single dataset where each row is a time step and each column is a separate time series. This is a standard way of storing parallel time series in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example is listed below. (FUTUTE VERSION: USE ONLY BELOW?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  15  25]\n",
      " [ 20  25  45]\n",
      " [ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]\n",
      " [ 90  95 185]]\n"
     ]
    }
   ],
   "source": [
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the dataset with one row per time step and one column for each of the two input and one output parallel time series.\n",
    "\n",
    "As with the univariate time series, we must structure these data into samples with input and output samples.\n",
    "\n",
    "We need to split the data into samples maintaining the order of observations across the two input sequences.\n",
    "\n",
    "If we chose three input time steps, then the first sample would look as follows:\n",
    "\n",
    "Input:\n",
    "\n",
    "10, 15\n",
    "20, 25\n",
    "30, 35\n",
    "\n",
    "Output:\n",
    "65\n",
    "\n",
    "That is, the first three time steps of each parallel series are provided as input to the model and the model associates this with the value in the output series at the third time step, in this case 65.\n",
    "\n",
    "We can see that, in transforming the time series into input/output samples to train the model, that we will have to discard some values from the output time series where we do not have values in the input time series at prior time steps. In turn, the choice of the size of the number of input time steps will have an important effect on how much of the training data is used.\n",
    "\n",
    "We can define a function named split_sequences() that will take a dataset as we have defined it with rows for time steps and columns for parallel series and return input/output samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function on our dataset using three time steps for each input time series as input.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 3, 2) (7,)\n",
      "[[10 15]\n",
      " [20 25]\n",
      " [30 35]] 65\n",
      "[[20 25]\n",
      " [30 35]\n",
      " [40 45]] 85\n",
      "[[30 35]\n",
      " [40 45]\n",
      " [50 55]] 105\n",
      "[[40 45]\n",
      " [50 55]\n",
      " [60 65]] 125\n",
      "[[50 55]\n",
      " [60 65]\n",
      " [70 75]] 145\n",
      "[[60 65]\n",
      " [70 75]\n",
      " [80 85]] 165\n",
      "[[70 75]\n",
      " [80 85]\n",
      " [90 95]] 185\n"
     ]
    }
   ],
   "source": [
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "print(X.shape, y.shape)\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first prints the shape of the X and y components.\n",
    "\n",
    "We can see that the X component has a three-dimensional structure.\n",
    "\n",
    "The first dimension is the number of samples, in this case 7. The second dimension is the number of time steps per sample, in this case 3, the value specified to the function. Finally, the last dimension specifies the number of parallel time series or the number of variables, in this case 2 for the two parallel series.\n",
    "\n",
    "We can then see that the input and output for each sample is printed, showing the three time steps for each of the two input series and the associated output for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig = X.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can fit an MLP on this data, we must flatten the shape of the input samples.\n",
    "\n",
    "MLPs require that the shape of the input portion of each sample is a vector. With a multivariate input, we will have multiple vectors, one for each time step.\n",
    "\n",
    "We can flatten the temporal structure of each input sample, so that:\n",
    "\n",
    "[[10 15]\n",
    "[20 25]\n",
    "[30 35]]\n",
    "\n",
    "Becomes:\n",
    "\n",
    "[10, 15, 20, 25, 30, 35]\n",
    "\n",
    "First, we can calculate the length of each input vector as the number of time steps multiplied by the number of features or time series. We can then use this vector size to reshape the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 15 20 25 30 35]\n",
      " [20 25 30 35 40 45]\n",
      " [30 35 40 45 50 55]\n",
      " [40 45 50 55 60 65]\n",
      " [50 55 60 65 70 75]\n",
      " [60 65 70 75 80 85]\n",
      " [70 75 80 85 90 95]]\n"
     ]
    }
   ],
   "source": [
    "# flatten input\n",
    "n_input = X.shape[1] * X.shape[2]\n",
    "X = X.reshape((X.shape[0], n_input))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define an MLP model for the multivariate input where the vector length is used for the input dimension argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_input, 100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(100, 1))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making a prediction, the model expects three time steps for two input time series.\n",
    "\n",
    "We can predict the next value in the output series proving the input values of:\n",
    "\n",
    "80,\t 85\n",
    "90,\t 95\n",
    "100, 105\n",
    "\n",
    "The shape of the 1 sample with 3 time steps and 2 variables would be [1, 3, 2]. We must again reshape this to be 1 sample with a vector of 6 elements or [1, 6]\n",
    "\n",
    "We would expect the next value in the sequence to be 100 + 105 or 205."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[204.9899]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = torch.Tensor([[80, 85], [90, 95], [100, 105]])\n",
    "x_input = x_input.reshape((1, n_input))\n",
    "yhat = model.forward(x_input)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example is listed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[[10 15 20 25 30 35]\n",
      " [20 25 30 35 40 45]\n",
      " [30 35 40 45 50 55]\n",
      " [40 45 50 55 60 65]\n",
      " [50 55 60 65 70 75]\n",
      " [60 65 70 75 80 85]\n",
      " [70 75 80 85 90 95]]\n"
     ]
    }
   ],
   "source": [
    "# multivariate mlp example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "# flatten input\n",
    "n_input = X.shape[1] * X.shape[2]\n",
    "print(n_input)\n",
    "X = X.reshape((X.shape[0], n_input))\n",
    "print(X)\n",
    "# define model\n",
    "model = nn.Sequential(nn.Linear(n_input, 100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(100, 1))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# fit model\n",
    "train(X, y)\n",
    "\n",
    "# demonstrate prediction\n",
    "x_input = torch.Tensor([[80, 85], [90, 95], [100, 105]])\n",
    "x_input = x_input.reshape((1, n_input))\n",
    "yhat = model.forward(x_input)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = split_sequences(dataset, n_steps)\n",
    "X.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prepares the data, fits the model, and makes a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Example.  Return to Slides\n",
    "\n",
    "![MLU Logo](https://drive.corp.amazon.com/view/bwernes@/MLU_Logo.png?download=true)\n",
    "\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#NLP2-Lecture-2-Support-Notebook\">\n",
    "        <span class=\"toc-item-num\">&nbsp;&nbsp;</span>\n",
    "        Go to TOP\n",
    "    </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
