{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fa467d",
   "metadata": {},
   "source": [
    "![MLU Logo](../images/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8ad80",
   "metadata": {},
   "source": [
    "# MLU-NLP2 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da8f5a",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "The project focuses on answer selection and uses the WikiQA dataset. Each record in the dataset has a question, answer and relevance score. The relevance score is binary, 1/0 indicating whether the answer is relevant to the question. \n",
    "\n",
    "Each question can be repeated multiple times and can have multiple relevant answer statements. \n",
    "\n",
    "To make the problem less complex, we have considered only questions which have at least 1 relevant answer. This simplification results in train, validation and test datasets with 873, 126 and 243 questions respectively.\n",
    "\n",
    "## Project Objective\n",
    "\n",
    "In this notebook, you will start our jorney. It contains a baseline model that will give you a first performance score and ourse and all code necessary ready for your first submission.\n",
    "\n",
    "__IMPORTANT__ \n",
    "\n",
    "Make sure you submit this notebook to get to know better how Leaderboard works and, also, make sure your completion will be granted :) .\n",
    "\n",
    "## The Baseline Model\n",
    "\n",
    "Here we are using Torchtext: an NLP specific package in Torch. \n",
    "\n",
    "We will generate 100 dim vector embeddings for each word using Glove and build a basic convolutional network which takes the text embeddings as input (50 * 100). The training dataset is trained in batches using this network and the losses in each epoch are backpropagated to update the weights and minimize losses in future iterations.\n",
    "\n",
    "The trained model is then used to make predictions on test dataset and finally, a result dataset with the list of predictions and sequential ID is created for your first leaderboard submission\n",
    "\n",
    "Notebook has been inspired from https://www.kaggle.com/ziliwang/pytorch-text-cnn\n",
    "\n",
    "### __Dataset:__\n",
    "The originial train and test datasets have questions for which there are no answers with relevance 1. To make the problem simpler, we have considered only questions which have atleast 1 answer with relevance score 1. This updated version of the datasets are used in the project\n",
    "\n",
    "### __Table of Contents__\n",
    "Here is the plan for this assignment.\n",
    "<p>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Reading the dataset\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>\n",
    "        Reading the dataset\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Data-Preparation\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>\n",
    "        Data Preparation\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Model-Building\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>\n",
    "        Model Building\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Training\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>\n",
    "        Training\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Prediction\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>\n",
    "        Prediction\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Submit-Results\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>\n",
    "        Submit Results\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e00e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "##torchtext is a package within pytorch consisting of data processing utilities and popular datasets for natural language\n",
    "!pip -q install torchtext==0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ac3845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import torchtext\n",
    "from nltk import word_tokenize\n",
    "import random\n",
    "from torch import optim\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d122bb",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "The datasets are in our MLU datalake and can be downloaded to your local instance here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be80b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "bucketname = 'mlu-courses-datalake' \n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.Bucket(bucketname).download_file('NLP2/data/training.csv', \n",
    "                                         './training.csv') \n",
    "s3.Bucket(bucketname).download_file('NLP2/data/public_test_features.csv', \n",
    "                                         './public_test_features.csv')\n",
    "s3.Bucket(bucketname).download_file('NLP2/data/glove.6B.100d.txt', \n",
    "                                         './glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28317716",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILE ='./training.csv'\n",
    "TEST_DATA_FILE = './public_test_features.csv'\n",
    "GLOVE_DATA_FILE = './glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517db86",
   "metadata": {},
   "source": [
    "Below, we are combining question and answer in each row as 1 single text column for simplicity. Alternatively, we can run two parallel networks for question and answer, merge the output of the 2 networks and have a classification layer as output. You may choose to save the files for ease of use, in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "475c7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_original=pd.read_csv(TEST_DATA_FILE)\n",
    "test = test_original.copy()\n",
    "train['text']=train[['question','answer']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "test['text']=test[['question','answer']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "train=train[['text','relevance']].rename(columns={'relevance':'label'})\n",
    "test=test[['text']]\n",
    "train.to_csv('train.csv',index=False)\n",
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22f20d",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97304b",
   "metadata": {},
   "source": [
    "In the below steps, we are converting the pandas dataset into Torch datasets. \n",
    "1. Define the datatypes of columns: \n",
    "We define text column as field type text and set the parameters: \n",
    "   lower=True: sets the words in text to lowercase\n",
    "   batch_first=True: set the dimensions such that batch dimension is first. For example, if length of text is fixed at 50 and batch size is 512, then the dimensions will be (512,50)\n",
    "   tokenize=word_tokenize: the method used for tokenizing words. You can try other methods such as spacy here\n",
    "   fix_length=50: all examples will be fixed or padded to this length\n",
    "We define label column as field type label and set the parameters:\n",
    "   sequential=False: if false, no tokenization will be applied\n",
    "   use_vocab=False: if false, the data in this field is known to be numerical\n",
    "   is_target=True: if true, this field is the target variable\n",
    "\n",
    "\n",
    "2. Read both train and test datasets as tabular datasets in torch and define the text and label columns based on previously defined field types. Tabular Dataset is used for datasets of columns stored in CSV, TSV or JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eacb7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = torchtext.data.Field(lower=True, batch_first=True, tokenize=word_tokenize, fix_length=50)\n",
    "label = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\n",
    "train = torchtext.data.TabularDataset(path='train.csv', format='csv',\n",
    "                                      fields={'text': ('text',text),\n",
    "                                              'label': ('label',label)})\n",
    "test = torchtext.data.TabularDataset(path='test.csv', format='csv',\n",
    "                                     fields={'text': ('text', text)\n",
    "                                             })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677cb77c",
   "metadata": {},
   "source": [
    "In the next step, we build the vocabulary from the text columns in train and test data. \n",
    "\n",
    "text.build_vocab:\n",
    "In order to build vocabulary, we pass datasets for which vocabulary has to be built and additional parameters such as min_freq and max_size. \n",
    "min_freq: minimum number of times a word has to occur in the datasets\n",
    "max_size: maximum size of vocabulary to be created (not used in the example)\n",
    "\n",
    "torchtext.vocab.Vectors\n",
    "We define and load 100 dim glove embeddings to use for the vector representation\n",
    "\n",
    "text.vocab.set_vectors\n",
    "Maps each word in the vocabulary to glove embeddings. glove.stoi will have numerical identifier mapped for each token string and glove.vectors will have the 100 dim vector mapped for each token string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bde9e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.build_vocab(train, test, min_freq=3)\n",
    "glove = torchtext.vocab.Vectors(GLOVE_DATA_FILE)\n",
    "text.vocab.set_vectors(glove.stoi, glove.vectors, dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f6403",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe87bc",
   "metadata": {},
   "source": [
    "Below is the neural network architecture:\n",
    "1. The input to the neural network is the padded glove embeddings for each word in text. The identifier for padding is also passed as a parameter\n",
    "2. The input embeddings are passed through a convolutional layer with specified number of kernels and kernel size. You may want to experiment with more layers and more kernels/different kernel sizes\n",
    "3. Convolutional layer is followed by a 2 * 2 max pooling layer and linear layer which generates 1 output\n",
    "4. If static is set to True, then the weights of the embedding layer are not updated through gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e69b39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, lm, padding_idx, static=True, kernel_num=128, fixed_length=50, kernel_size=5, dropout=0.2):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embedding = nn.Embedding.from_pretrained(lm)\n",
    "        if static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(1, kernel_num, (kernel_size, self.embedding.embedding_dim))])\n",
    "        self.maxpools = [nn.MaxPool2d((fixed_length+1-kernel_size,1))]\n",
    "        self.fc = nn.Linear(kernel_num, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.embedding(input).unsqueeze(1)  \n",
    "        x = [self.maxpools[i](torch.tanh(cov(x))).squeeze(3).squeeze(2) for i, cov in enumerate(self.conv)]  \n",
    "        x = torch.cat(x, dim=1)  \n",
    "        y = self.fc(self.dropout(x))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52a7de",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f220725",
   "metadata": {},
   "source": [
    "Below is the training loop. Each batch of training data is read, predictions are computed through forward propagation of batch inputs. Losses are computed between predictions and actual labels and back propagated to update the weights. In each epoch, we compute the f1 score with a preset threshold of 0.15 (this can be a tunable parameter and could provide better performance of other thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b267dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, loss_func, optimizer, train_iter):\n",
    "    e = 0\n",
    "    \n",
    "    while e < epoch:\n",
    "        train_iter.init_epoch()\n",
    "        losses, preds, true = [], [], []\n",
    "        for train_batch in tqdm(list(iter(train_iter)), 'epoch {} training'.format(e)):\n",
    "            model.train()\n",
    "            x = train_batch.text.cuda()\n",
    "            y = train_batch.label.type(torch.Tensor).cuda()\n",
    "            true.append(train_batch.label.numpy())\n",
    "            model.zero_grad()\n",
    "            pred = model.forward(x).view(-1)\n",
    "            loss = loss_function(pred, y)\n",
    "            preds.append(torch.sigmoid(pred).cpu().data.numpy())\n",
    "            losses.append(loss.cpu().data.numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_f1=f1_score([j for i in true for j in i],np.array([j for i in preds for j in i])>0.15)\n",
    "        alpha_train=0.15\n",
    "        print('epoch {:02} - train_loss {:.4f} - train f1 {:.4f} - threshold {:.4f}'.format(\n",
    "                            e, np.mean(losses), train_f1, alpha_train))\n",
    "                \n",
    "        e += 1\n",
    "    return alpha_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1164b0c",
   "metadata": {},
   "source": [
    "Setting random seed for reproducibility. Batch size is set as 512 and is a hyperparameter that can be varied. Torch uses bucket iterator for language modelling tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fb87397",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "batch_size = 512\n",
    "train_iter = torchtext.data.BucketIterator(dataset=train,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a4101",
   "metadata": {},
   "source": [
    "Below function helps with weight initialization for model parameters. Xavier is typically used as a standard for weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8e4c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(model, method='xavier', exclude='embedding', seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    for name, w in model.named_parameters():\n",
    "        if not exclude in name:\n",
    "            if 'weight' in name:\n",
    "                if method is 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method is 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0.0)\n",
    "            else: \n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5832ff50",
   "metadata": {},
   "source": [
    "Below steps are used to define the model, initialize the model, the optimization algorithm and loss function. Learning rate is a tunable hyperparameter here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3cc57003",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.fix_length = 50\n",
    "model = TextCNN(text.vocab.vectors, padding_idx=text.vocab.stoi[text.pad_token], kernel_size=5, kernel_num=128, static=False, fixed_length=text.fix_length, dropout=0.1).cuda()\n",
    "init_network(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba19911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 training: 100%|██████████| 14/14 [00:00<00:00, 81.77it/s]\n",
      "epoch 1 training:   0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00 - train_loss 0.4256 - train f1 0.1556 - threshold 0.1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 training: 100%|██████████| 14/14 [00:00<00:00, 82.77it/s]\n",
      "epoch 2 training:   0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 - train_loss 0.3671 - train f1 0.2247 - threshold 0.1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 training: 100%|██████████| 14/14 [00:00<00:00, 93.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 02 - train_loss 0.3414 - train f1 0.3149 - threshold 0.1500\n",
      "CPU times: user 809 ms, sys: 104 ms, total: 913 ms\n",
      "Wall time: 905 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "training(3, model, loss_function, optimizer, train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2b4d0",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae59bd2",
   "metadata": {},
   "source": [
    "Below function is used to predict on test dataset using trained model. It returns a list of predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57178e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_list):\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_list:\n",
    "            model.eval()\n",
    "            x = test_batch.text.cuda()\n",
    "            pred += torch.sigmoid(model.forward(x).view(-1)).cpu().data.numpy().tolist()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae3bbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = list(torchtext.data.BucketIterator(dataset=test,\n",
    "                                    batch_size=batch_size,\n",
    "                                    sort=False,\n",
    "                                    train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ac8870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(model, test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f31e3",
   "metadata": {},
   "source": [
    "### Submit Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50f2cf",
   "metadata": {},
   "source": [
    "Create a new dataframe for submission. The list of predicted probabilities are converted to labels using the pre-defined threshold of 0.15 (can be tuned for better performance). The list of labels is concatenated with the original sequential ID from the test file downloaded from Leaderboard, to generate the final submission\n",
    "\n",
    "For submission, follow these steps:\n",
    "1. Go to the folder where your notebook is in Sagemaker\n",
    "2. Donwload the file __test_submission_nlp2.csv__ to your local machine\n",
    "3. On NLP2 Leaderboard contest, select option __My Submissions\"__ and upload your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c970414",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=[\"ID\", \"relevance\"])\n",
    "result_df[\"ID\"] = test_original[\"ID\"].tolist()\n",
    "labels=[1 if pred>0.15 else 0 for pred in preds]\n",
    "result_df[\"relevance\"] = labels\n",
    "result_df.to_csv(\"test_submission_nlp2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf3ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
