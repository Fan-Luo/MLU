{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae2d552",
   "metadata": {},
   "source": [
    "![MLU Logo](../images/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27568daa",
   "metadata": {},
   "source": [
    "# Ansel Blume MLU-NLP2 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a4bc0",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "The project focuses on answer selection and uses the WikiQA dataset. Each record in the dataset has a question, answer and relevance score. The relevance score is binary, 1/0 indicating whether the answer is relevant to the question. \n",
    "\n",
    "Each question can be repeated multiple times and can have multiple relevant answer statements. \n",
    "\n",
    "To make the problem less complex, we have considered only questions which have at least 1 relevant answer. This simplification results in train, validation and test datasets with 873, 126 and 243 questions respectively.\n",
    "\n",
    "## Project Objective\n",
    "\n",
    "In this notebook, you will start our jorney. It contains a baseline model that will give you a first performance score and ourse and all code necessary ready for your first submission.\n",
    "\n",
    "__IMPORTANT__ \n",
    "\n",
    "Make sure you submit this notebook to get to know better how Leaderboard works and, also, make sure your completion will be granted :) .\n",
    "\n",
    "## The Baseline Model\n",
    "\n",
    "Here we are using Torchtext: an NLP specific package in Torch. \n",
    "\n",
    "We will generate 100 dim vector embeddings for each word using Glove and build a basic convolutional network which takes the text embeddings as input (50 * 100). The training dataset is trained in batches using this network and the losses in each epoch are backpropagated to update the weights and minimize losses in future iterations.\n",
    "\n",
    "The trained model is then used to make predictions on test dataset and finally, a result dataset with the list of predictions and sequential ID is created for your first leaderboard submission\n",
    "\n",
    "Notebook has been inspired from https://www.kaggle.com/ziliwang/pytorch-text-cnn\n",
    "\n",
    "### __Dataset:__\n",
    "The originial train and test datasets have questions for which there are no answers with relevance 1. To make the problem simpler, we have considered only questions which have atleast 1 answer with relevance score 1. This updated version of the datasets are used in the project\n",
    "\n",
    "### __Table of Contents__\n",
    "Here is the plan for this assignment.\n",
    "<p>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Reading the dataset\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>\n",
    "        Reading the dataset\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Data-Preparation\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>\n",
    "        Data Preparation\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Model-Building\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>\n",
    "        Model Building\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Training\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>\n",
    "        Training\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Prediction\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>\n",
    "        Prediction\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Submit-Results\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>\n",
    "        Submit Results\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e546a961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2d84c",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "The datasets are in our MLU datalake and can be downloaded to your local instance here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25eb495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILE ='./training.csv'\n",
    "TEST_DATA_FILE = './public_test_features.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d9f03a",
   "metadata": {},
   "source": [
    "Below, we are combining question and answer in each row as 1 single text column for simplicity. Alternatively, we can run two parallel networks for question and answer, merge the output of the 2 networks and have a classification layer as output. You may choose to save the files for ease of use, in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d67313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import RobertaTokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large-mnli')\n",
    "\n",
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "class RelevanceDataset(Dataset):        \n",
    "    def __init__(self, df, use_weights=False):\n",
    "        self.use_weights = use_weights\n",
    "        self.df = df.copy()\n",
    "        self.df.rename(columns={'relevance': 'labels'}, inplace=True)\n",
    "        \n",
    "        # Encodes <s>question</s></s>answer</s> pairwise\n",
    "        tokenized = tokenizer(\n",
    "            list(self.df['question']), \n",
    "            list(self.df['answer']),\n",
    "            return_tensors='pt',\n",
    "            padding='max_length'\n",
    "        )\n",
    "        \n",
    "        # Need to wrap in list as otherwise pd gets confused and breaks up tensor\n",
    "        self.df['input_ids'] = list(tokenized['input_ids'])\n",
    "        self.df['attention_mask'] = list(tokenized['attention_mask'])\n",
    "                \n",
    "        # Build weights to increase recall\n",
    "        if 'labels' in self.df:      \n",
    "            self.weights = self._compute_weights() if use_weights else [1, 1]\n",
    "            self.df['weights'] = self.df['labels'].apply(lambda x: self.weights[x])\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return dict(self.df.iloc[i])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _compute_weights(self):\n",
    "        value_counts = self.df['labels'].value_counts()\n",
    "        return value_counts.max() / value_counts\n",
    "    \n",
    "def get_datasets(args):\n",
    "    train_dataset = RelevanceDataset(train, use_weights=args.use_weights)\n",
    "    test_set = RelevanceDataset(test)\n",
    "\n",
    "    train_frac = args.train_frac if 'train_frac' in args else .9\n",
    "    train_set_len = int(len(train_dataset) * train_frac)\n",
    "    val_set_len = len(train_dataset) - train_set_len\n",
    "    train_set, val_set = random_split(\n",
    "        train_dataset, \n",
    "        [train_set_len, val_set_len],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    #train_set, val_set, _ = random_split(train_dataset, [100, 30, len(train_dataset) - 130])\n",
    "    \n",
    "    return train_set, val_set, test_set, train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68afa214",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904d035b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "def collate_fn(dicts):\n",
    "    batch_dict = defaultdict(list)\n",
    "    \n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            batch_dict[k].append(v)\n",
    "                        \n",
    "    for k in ['input_ids', 'attention_mask']:\n",
    "        batch_dict[k] = torch.stack(batch_dict[k])\n",
    "        \n",
    "    if 'labels' in batch_dict:\n",
    "        for k in ['labels', 'weights']:\n",
    "            batch_dict[k] = torch.tensor(batch_dict[k])\n",
    "    \n",
    "    return dict(batch_dict)\n",
    "\n",
    "def batch_to_device(batch, device):\n",
    "    for k, v in batch.items():\n",
    "        if type(v) == torch.Tensor:\n",
    "            batch[k] = v.to(device)\n",
    "            \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f13c1c",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "352d9f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class RobertaRelevanceClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = RobertaModel.from_pretrained('roberta-large-mnli')\n",
    "        self.embed_dim = self.model.embeddings.word_embeddings.weight.shape[-1]\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embed_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(output.pooler_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a413b1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161afbd",
   "metadata": {},
   "source": [
    "Below is the training loop. Each batch of training data is read, predictions are computed through forward propagation of batch inputs. Losses are computed between predictions and actual labels and back propagated to update the weights. In each epoch, we compute the f1 score with a preset threshold of 0.15 (this can be a tunable parameter and could provide better performance of other thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbfcc4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_constant_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torchmetrics as tm\n",
    "import json\n",
    "import re\n",
    "\n",
    "class TrainArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.hparams = kwargs\n",
    "        self.__dict__.update(**self.hparams)\n",
    "        \n",
    "    def __contains__(self, arg):\n",
    "        return arg in self.hparams\n",
    "    \n",
    "    def __getitem__(self, arg):\n",
    "        return self.hparams[arg]\n",
    "    \n",
    "    def __setitem__(self, arg, val):\n",
    "        self.hparams[arg] = val\n",
    "        setattr(self, arg, val)\n",
    "\n",
    "def run_training(args, model, train_ds, val_ds, writer):\n",
    "    # Helper function to compute logits and loss\n",
    "    def logits_and_loss(batch, model, args):\n",
    "        batch_to_device(batch, args.device)\n",
    "        with autocast():\n",
    "            logits = model(batch['input_ids'], batch['attention_mask']).flatten()\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                logits, \n",
    "                batch['labels'].float(),\n",
    "                weight=batch['weights'] if args.use_weights else None\n",
    "            )\n",
    "            loss = loss / args.accum_grad_batches\n",
    "        \n",
    "        return logits.detach(), loss\n",
    "    \n",
    "    # Helper function to take an optimizer step\n",
    "    def optimizer_step(optimizer, scheduler, scaler):\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Metrics\n",
    "    pred_threshold = args.pred_threshold if 'pred_threshold' in args else .5\n",
    "    train_metrics = tm.MetricCollection([\n",
    "        tm.Accuracy(threshold=pred_threshold),\n",
    "        tm.Precision(threshold=pred_threshold),\n",
    "        tm.Recall(threshold=pred_threshold),\n",
    "        tm.F1(threshold=pred_threshold),\n",
    "        tm.AveragePrecision()\n",
    "    ]).to(args.device)\n",
    "    val_metrics = train_metrics.clone().to(args.device)\n",
    "        \n",
    "    # Set up dataloaders\n",
    "    dl_kwargs = {\n",
    "        'batch_size': args.batch_size,\n",
    "        'collate_fn': collate_fn,\n",
    "        'shuffle': True\n",
    "    }\n",
    "    train_dl = DataLoader(train_ds, **dl_kwargs)\n",
    "    val_dl = DataLoader(val_ds, **dl_kwargs)\n",
    "    \n",
    "    # Training loop\n",
    "    model.to(args.device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay if 'weight_decay' in args else .01\n",
    "    )\n",
    "    scheduler = get_constant_schedule_with_warmup(optimizer, args.warmup_steps)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    starting_epoch = 0 if 'starting_epoch' not in args else args.starting_epoch    \n",
    "    for epoch in range(starting_epoch, starting_epoch + args.epochs):\n",
    "        print(f'Epoch {epoch}')\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        losses = []\n",
    "        print('Training')\n",
    "        for i, batch in enumerate(tqdm(train_dl)):\n",
    "            logits, loss = logits_and_loss(batch, model, args)\n",
    "            \n",
    "            # Metrics\n",
    "            losses.append(loss.item())\n",
    "            predictions = logits.sigmoid()\n",
    "            train_metrics(predictions, batch['labels'])\n",
    "            \n",
    "            # Optimization\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % args.accum_grad_batches == 0 or i == len(train_dl) - 1:\n",
    "                tqdm.write('Taking a step')\n",
    "                optimizer_step(optimizer, scheduler, scaler)\n",
    "            \n",
    "                # Print metrics for the accumulated batch\n",
    "                tqdm.write(f'Predictions: {predictions}')\n",
    "                tqdm.write(f'Labels: {batch[\"labels\"]}')\n",
    "                tqdm.write(str(train_metrics.compute()))\n",
    "            \n",
    "        writer.add_scalar('Loss/train', torch.tensor(losses).mean(), epoch)\n",
    "        writer.add_scalars('Train', train_metrics.compute(), epoch)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        print('Validating')\n",
    "        for batch in tqdm(val_dl):\n",
    "            with torch.no_grad():\n",
    "                logits, loss = logits_and_loss(batch, model, args)\n",
    "                \n",
    "            losses.append(loss.item())\n",
    "            predictions = logits.sigmoid()\n",
    "            val_metrics(predictions, batch['labels'])\n",
    "        \n",
    "        writer.add_scalar('Loss/val', torch.tensor(losses).mean(), epoch)\n",
    "        writer.add_scalars('Val', val_metrics.compute(), epoch)\n",
    "        tqdm.write(str(val_metrics.compute()))\n",
    "\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), f'checkpoints/epoch={epoch}_state_dict.pkl')\n",
    "        \n",
    "        # Reset metrics\n",
    "        train_metrics.reset()\n",
    "        val_metrics.reset()\n",
    "        \n",
    "    writer.flush()\n",
    "    \n",
    "def load_model(args):\n",
    "    checkpoint_re = r'.+=(\\d+).+'\n",
    "    model = RobertaRelevanceClassifier()\n",
    "    \n",
    "    if 'checkpoint' in args:\n",
    "        saved_epoch = int(re.match(checkpoint_re, args.checkpoint).group(1))\n",
    "        args['starting_epoch'] = saved_epoch + 1\n",
    "        model.load_state_dict(torch.load(args.checkpoint))\n",
    "        \n",
    "    if 'freeze_roberta' in args:\n",
    "        for parameter in model.model.parameters():\n",
    "            parameter.requires_grad = not args.freeze_roberta\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25becc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainArgs(\n",
    "    batch_size=2,\n",
    "    epochs=10,\n",
    "    device='cuda:0' if torch.cuda.is_available() else 'cpu',\n",
    "    lr=1e-5,\n",
    "    use_weights=True,\n",
    "    accum_grad_batches=256,\n",
    "    warmup_steps=10,\n",
    "    freeze_roberta=False,\n",
    "    pred_threshold=.5,\n",
    "    train_frac=.9,\n",
    "    weight_decay=.01,\n",
    "    checkpoint='checkpoints/epoch=10_state_dict.pkl'\n",
    ")\n",
    "\n",
    "# Tensorboard logging\n",
    "# https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html\n",
    "# https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks\n",
    "writer = SummaryWriter()\n",
    "\n",
    "model = load_model(args)\n",
    "train_set, val_set, test_set, train_dataset = get_datasets(args)\n",
    "run_training(args, model, train_set, val_set, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f67213",
   "metadata": {},
   "source": [
    "Setting random seed for reproducibility. Batch size is set as 512 and is a hyperparameter that can be varied. Torch uses bucket iterator for language modelling tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c823c4",
   "metadata": {},
   "source": [
    "Below steps are used to define the model, initialize the model, the optimization algorithm and loss function. Learning rate is a tunable hyperparameter here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95458c",
   "metadata": {},
   "source": [
    "%%time\n",
    "training(3, model, loss_function, optimizer, train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d6830",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6e536",
   "metadata": {},
   "source": [
    "Below function is used to predict on test dataset using trained model. It returns a list of predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da44b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(args, model, test_ds):\n",
    "    # Model to correct device\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Set up data and metrics\n",
    "    dl = DataLoader(test_ds, batch_size=args.batch_size)\n",
    "    pred_l = []\n",
    "    \n",
    "    metrics = tm.MetricCollection([\n",
    "        tm.Accuracy(threshold=args.pred_threshold),\n",
    "        tm.Precision(threshold=args.pred_threshold),\n",
    "        tm.Recall(threshold=args.pred_threshold),\n",
    "        tm.F1(threshold=args.pred_threshold),\n",
    "        tm.AveragePrecision()\n",
    "    ]).to(args.device)\n",
    "    \n",
    "    has_labels = 'labels' in next(iter(dl))\n",
    "    \n",
    "    # Predict\n",
    "    for batch in tqdm(dl):\n",
    "        batch_to_device(batch, args.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs = model(batch['input_ids'], batch['attention_mask']).flatten().sigmoid()\n",
    "            \n",
    "        preds = (probs > args.pred_threshold).int()\n",
    "        pred_l.append(preds.cpu())\n",
    "            \n",
    "        if has_labels:\n",
    "            metrics(probs, batch['labels'])\n",
    "    \n",
    "    return torch.cat(pred_l).cpu(), metrics.cpu().compute() if has_labels else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1ff7ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 6/6 [01:28<00:00, 14.78s/it]\n"
     ]
    }
   ],
   "source": [
    "args = TrainArgs(\n",
    "    checkpoint='checkpoints/epoch=10_state_dict.pkl',\n",
    "    pred_threshold=.5,\n",
    "    use_weights=False,\n",
    "    device='cuda:0' if torch.cuda.is_available() else 'cpu',\n",
    "    batch_size=500\n",
    ")\n",
    "train_set, val_set, test_set, train_dataset = get_datasets(args)\n",
    "model = load_model(args)\n",
    "preds, metrics = predict(args, model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667ba55-3b16-49dd-a780-95bc50eb03ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae8810",
   "metadata": {},
   "source": [
    "### Submit Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fdd2e6",
   "metadata": {},
   "source": [
    "Create a new dataframe for submission. The list of predicted probabilities are converted to labels using the pre-defined threshold of 0.15 (can be tuned for better performance). The list of labels is concatenated with the original sequential ID from the test file downloaded from Leaderboard, to generate the final submission\n",
    "\n",
    "For submission, follow these steps:\n",
    "1. Go to the folder where your notebook is in Sagemaker\n",
    "2. Donwload the file __test_submission_nlp2.csv__ to your local machine\n",
    "3. On NLP2 Leaderboard contest, select option __My Submissions\"__ and upload your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "54db7ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=[\"ID\", \"relevance\"])\n",
    "result_df[\"ID\"] = test_set.df[\"ID\"].tolist()\n",
    "result_df[\"relevance\"] = preds\n",
    "result_df.to_csv(\"test_submission_nlp2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461cc16-1e40-42ce-8a33-3f1f323a3cd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random forest on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "90fdef36-97b3-4eee-8718-ec07830415ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeds_and_labels(args, model, ds):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    dl = DataLoader(ds, batch_size=args.batch_size)\n",
    "    for batch in tqdm(dl):\n",
    "        batch_to_device(batch, args.device)\n",
    "        with torch.no_grad():\n",
    "            batch_embedding = model.model(\n",
    "                batch['input_ids'], \n",
    "                attention_mask=batch['attention_mask']\n",
    "            ).pooler_output\n",
    "        embeddings.append(batch_embedding.cpu())\n",
    "        \n",
    "        if 'labels' in batch:\n",
    "            labels.append(batch['labels'].cpu())\n",
    "        \n",
    "    return torch.cat(embeddings), torch.cat(labels) if len(labels) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57dea419-001a-4f51-ade1-ba19cc7fbf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6174, 1024])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeds, train_labels = get_embeds_and_labels(args, model, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50bfe318-83a9-4dda-98c5-13a6e161fc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_embeddings, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4aee3188-f783-450f-9979-14ac0877bcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:20<00:00, 10.34s/it]\n"
     ]
    }
   ],
   "source": [
    "val_embeds, val_labels = get_embeds_and_labels(args, model, val_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "568ac2c8-1dc0-471d-801b-d27ca4905d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "outputs = clf.predict(val_embeds)\n",
    "results = precision_recall_fscore_support(val_labels, outputs, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64832de6-0df4-4579-b18e-77f04f84435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:28<00:00, 14.70s/it]\n"
     ]
    }
   ],
   "source": [
    "test_embeds, None = get_embeds_and_labels(args, model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "20be78ce-d045-4a36-867f-b807a80d60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_embeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
