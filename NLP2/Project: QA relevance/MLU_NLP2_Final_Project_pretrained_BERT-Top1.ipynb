{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf27b746",
   "metadata": {},
   "source": [
    "![MLU Logo](../images/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7aba8",
   "metadata": {},
   "source": [
    "# MLU-NLP2 Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0563f710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##torchtext is a package within pytorch consisting of data processing utilities and popular datasets for natural language\n",
    "!pip install -q torchtext torch\n",
    "# !pip install -qU regex requests hydra-core omegaconf\n",
    "# !pip install -qU cython\n",
    "!pip install -q transformers\n",
    "!pip install -q fsspec\n",
    "!pip install -q datasets\n",
    "!pip install -q torch torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89011d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import torchtext\n",
    "from nltk import word_tokenize\n",
    "import random\n",
    "from torch import optim\n",
    "\n",
    "from pprint import pprint as pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dbe59b",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "The datasets are in our MLU datalake and can be downloaded to your local instance here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56778518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "bucketname = 'mlu-courses-datalake' \n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.Bucket(bucketname).download_file('NLP2/data/training.csv', \n",
    "                                         './training.csv')\n",
    "s3.Bucket(bucketname).download_file('NLP2/data/public_test_features.csv', \n",
    "                                         './public_test_features.csv')\n",
    "s3.Bucket(bucketname).download_file('NLP2/data/glove.6B.100d.txt', \n",
    "                                         './glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a91fc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILE ='./training.csv'\n",
    "TEST_DATA_FILE = './public_test_features.csv'\n",
    "GLOVE_DATA_FILE = './glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf613093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-004aa1dfe52d198e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/ec2-user/.cache/huggingface/datasets/csv/default-004aa1dfe52d198e/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-111060e8804a730b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/csv/default-004aa1dfe52d198e/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/ec2-user/.cache/huggingface/datasets/csv/default-111060e8804a730b/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/csv/default-111060e8804a730b/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Organize inputs into Datasets\n",
    "from datasets import load_dataset\n",
    "train_dataset_original, val_dataset_original = load_dataset('csv', \n",
    "                                                            data_files=[TRAIN_DATA_FILE], \n",
    "                                                            split=['train[:85%]', 'train[85%:]'])\n",
    "test_dataset_original = load_dataset('csv', data_files=[TEST_DATA_FILE], split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc421f9a",
   "metadata": {},
   "source": [
    "Below, we are combining question and answer in each row as 1 single text column for simplicity. Alternatively, we can run two parallel networks for question and answer, merge the output of the 2 networks and have a classification layer as output. You may choose to save the files for ease of use, in future steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f17d0e",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Tokenize data and put into DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently using huggingface API. Most pretrained LMs at https://huggingface.co/models can be subbed out with\n",
    "# only minor modifications\n",
    "# Initially tried using RoBERTa, got ~0.6 F1 score on val set after adjusting hyperparams.\n",
    "# Best submissions were with BERT-large, previously fine-tuned on SQuAD dataset. Got ~0.7 F1 score on val sets. \n",
    "PRETRAINED_MODEL = 'bert-large-uncased-whole-word-masking-finetuned-squad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581800a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991cf9f75a2f4abc92acc8ebb89c9cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffd1a5f987549c4acc2af4d3af4d2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14485556b8e84a1eba34cc8a818ec253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751201c8e47f4c81b8f6ebe9279c8f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef0f4b47a98486f9fc99154850c499c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff6e2c35a53420ea6b45a35660bfe89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f40ef1d10a48489c9820bde4981f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b965d917724af18875538d52296ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53f920e9b7d41ad87ae9efaea424364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
      "    num_rows: 6792\n",
      "}) Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
      "    num_rows: 69\n",
      "}) Dataset({\n",
      "    features: ['ID', 'attention_mask', 'input_ids', 'token_type_ids'],\n",
      "    num_rows: 2941\n",
      "})\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
      "    num_rows: 300\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Value\n",
    "\n",
    "# RoBERTa uses byte-pair encoding tokenizer, accounts for unicode/special chars\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "def tokenize_qa(dataset):\n",
    "    \"\"\"Tokenize question + answer with <sep> token between.\"\"\"\n",
    "    return tokenizer(dataset['question'], dataset['answer'], padding='max_length', truncation=True)\n",
    "\n",
    "train_dataset = train_dataset_original.map(tokenize_qa, batched=\"True\")\n",
    "val_dataset = val_dataset_original.map(tokenize_qa, batched=\"True\")\n",
    "test_dataset = test_dataset_original.map(tokenize_qa, batched=\"True\")\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(['question', 'answer', 'ID'])\n",
    "val_dataset = val_dataset.remove_columns(['question', 'answer', 'ID'])\n",
    "test_dataset = test_dataset.remove_columns(['question', 'answer'])\n",
    "\n",
    "train_dataset = train_dataset.rename_column('relevance', 'labels')\n",
    "val_dataset = val_dataset.rename_column('relevance', 'labels')\n",
    "\n",
    "train_features = train_dataset.features.copy()\n",
    "train_features['labels'] = Value(\"float32\")\n",
    "train_dataset = train_dataset.cast(train_features)\n",
    "val_dataset = val_dataset.cast(train_features)\n",
    "\n",
    "train_dataset.set_format('torch')\n",
    "val_dataset.set_format('torch')\n",
    "test_dataset.set_format('torch')\n",
    "\n",
    "mini_train_dataset = train_dataset.shuffle(seed=1234).select(range(300))\n",
    "\n",
    "print(train_dataset, val_dataset, test_dataset)\n",
    "print(mini_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d5bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "mini_train_dataloader = DataLoader(mini_train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65bd8df",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "Load a pre-trained RoBERTa model for binary classification and fine-tune on our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8111a855",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9a7eacb1479e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Only run if out of GPU space. Sometimes can help.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Only run if out of GPU space. Sometimes can help.\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6ea06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c147b7837745bcb0f9451aae7c5308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForSequenceClassification: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 GPUs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 1024)\n",
       "        (token_type_embeddings): Embedding(2, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (18): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (19): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (20): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (21): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (22): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (23): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Download pretrained model for sequence classification. \n",
    "# Pool outputs from transformer and run through one-layer NN with dropout.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=1)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs.\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e677fa",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f61a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = train_dataloader\n",
    "# train_dl = mini_train_dataloader\n",
    "val_dl = val_dataloader\n",
    "test_dl = test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f637cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Start with training hyperparams from https://arxiv.org/pdf/1905.05583v3.pdf\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5684df26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c4bd9919d14a9c99115f2747d0cfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "from tqdm.auto import tqdm\n",
    "progress_bar = tqdm(range(num_training_steps * train_dl.batch_size))\n",
    "\n",
    "# Additional loss penalty for positive examples, ended up unnecessary on most models. \n",
    "POS_LOSS_WEIGHT = 1\n",
    "\n",
    "# Override loss fn for BCE rather than using default in model\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([POS_LOSS_WEIGHT]).to(device))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_losses = []\n",
    "    for batch in train_dl:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        logits = outputs.logits.view(-1)\n",
    "        loss = loss_fn(logits, batch['labels']).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(batch['attention_mask'].shape[0])\n",
    "        running_losses.append(loss.sum())\n",
    "        progress_bar.set_description(f'{sum(running_losses) / len(running_losses)}', refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "214afc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d20eea390945c497d13fca99d64127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "progress_bar = tqdm(range(len(val_dl)))\n",
    "\n",
    "yhats, labels = [], []\n",
    "\n",
    "# Collect sigmoid outputs directly to compare different cutoffs\n",
    "model.eval()\n",
    "for batch in val_dl:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**batch).logits.view(-1)\n",
    "    \n",
    "    yhat_batch = torch.sigmoid(logits).cpu().data.numpy().tolist()\n",
    "    yhats.extend(yhat_batch)\n",
    "    labels.extend(batch['labels'])\n",
    "    \n",
    "    progress_bar.update(1)\n",
    "\n",
    "#     preds = torch.sigmoid(yhat) > CUTOFF\n",
    "#     print(preds)\n",
    "#     print(batch['labels'])\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ebec28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534811e55964466281a761969f947028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0:\n",
      " [{'f1': 0.23076923076923078}, {'accuracy': 0.13043478260869565}, {'precision': 0.13043478260869565}, {'recall': 1.0}]\n",
      "0.05:\n",
      " [{'f1': 0.782608695652174}, {'accuracy': 0.927536231884058}, {'precision': 0.6428571428571429}, {'recall': 1.0}]\n",
      "0.1:\n",
      " [{'f1': 0.761904761904762}, {'accuracy': 0.927536231884058}, {'precision': 0.6666666666666666}, {'recall': 0.8888888888888888}]\n",
      "0.15:\n",
      " [{'f1': 0.761904761904762}, {'accuracy': 0.927536231884058}, {'precision': 0.6666666666666666}, {'recall': 0.8888888888888888}]\n",
      "0.2:\n",
      " [{'f1': 0.8421052631578948}, {'accuracy': 0.9565217391304348}, {'precision': 0.8}, {'recall': 0.8888888888888888}]\n",
      "0.25:\n",
      " [{'f1': 0.75}, {'accuracy': 0.9420289855072463}, {'precision': 0.8571428571428571}, {'recall': 0.6666666666666666}]\n",
      "0.3:\n",
      " [{'f1': 0.75}, {'accuracy': 0.9420289855072463}, {'precision': 0.8571428571428571}, {'recall': 0.6666666666666666}]\n",
      "0.35:\n",
      " [{'f1': 0.8}, {'accuracy': 0.9565217391304348}, {'precision': 1.0}, {'recall': 0.6666666666666666}]\n",
      "0.4:\n",
      " [{'f1': 0.8}, {'accuracy': 0.9565217391304348}, {'precision': 1.0}, {'recall': 0.6666666666666666}]\n",
      "0.45:\n",
      " [{'f1': 0.8}, {'accuracy': 0.9565217391304348}, {'precision': 1.0}, {'recall': 0.6666666666666666}]\n",
      "0.5:\n",
      " [{'f1': 0.8}, {'accuracy': 0.9565217391304348}, {'precision': 1.0}, {'recall': 0.6666666666666666}]\n",
      "0.55:\n",
      " [{'f1': 0.8}, {'accuracy': 0.9565217391304348}, {'precision': 1.0}, {'recall': 0.6666666666666666}]\n",
      "0.6:\n",
      " [{'f1': 0.8}, {'accuracy': 0.9565217391304348}, {'precision': 1.0}, {'recall': 0.6666666666666666}]\n",
      "0.65:\n",
      " [{'f1': 0.8}, {'accuracy': 0.9565217391304348}, {'precision': 1.0}, {'recall': 0.6666666666666666}]\n",
      "0.7:\n",
      " [{'f1': 0.8}, {'accuracy': 0.9565217391304348}, {'precision': 1.0}, {'recall': 0.6666666666666666}]\n",
      "0.75:\n",
      " [{'f1': 0.8}, {'accuracy': 0.9565217391304348}, {'precision': 1.0}, {'recall': 0.6666666666666666}]\n",
      "0.8:\n",
      " [{'f1': 0.8}, {'accuracy': 0.9565217391304348}, {'precision': 1.0}, {'recall': 0.6666666666666666}]\n",
      "0.85:\n",
      " [{'f1': 0.8}, {'accuracy': 0.9565217391304348}, {'precision': 1.0}, {'recall': 0.6666666666666666}]\n",
      "0.9:\n",
      " [{'f1': 0.7142857142857143}, {'accuracy': 0.9420289855072463}, {'precision': 1.0}, {'recall': 0.5555555555555556}]\n",
      "0.95:\n",
      " [{'f1': 0.6153846153846153}, {'accuracy': 0.927536231884058}, {'precision': 1.0}, {'recall': 0.4444444444444444}]\n",
      "\n",
      "Best cutoff at 0.2:\n",
      "{'f1': 0.8421052631578948}{'accuracy': 0.9565217391304348}{'precision': 0.8}{'recall': 0.8888888888888888}\n"
     ]
    }
   ],
   "source": [
    "# Determine best cutoff to maximize f1 on val\n",
    "# (maybe can use binary search to balance precision/recall but seems overly specific on val)\n",
    "best_cutoff = None\n",
    "best_metrics = None\n",
    "best_metrics_scores = None\n",
    "for cutoff in tqdm(range(0, 100, 5)):\n",
    "    cutoff = cutoff / 100\n",
    "\n",
    "    metrics = []\n",
    "    metrics.append(load_metric(\"f1\"))\n",
    "    metrics.append(load_metric(\"accuracy\"))\n",
    "    metrics.append(load_metric(\"precision\"))\n",
    "    metrics.append(load_metric(\"recall\"))\n",
    "\n",
    "    preds = [yhat > cutoff for yhat in yhats]\n",
    "    for metric in metrics:\n",
    "        metric.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "    metrics_scores = [metric.compute() for metric in metrics]\n",
    "#     print(f'{cutoff}:\\n {metrics_scores}')\n",
    "    \n",
    "    if best_metrics_scores is None or metrics_scores[0]['f1'] > best_metrics_scores[0]['f1']:\n",
    "        best_cutoff = cutoff\n",
    "        best_metrics = metrics\n",
    "        best_metrics_scores = metrics_scores\n",
    "    \n",
    "print(f\"\\nBest cutoff at {best_cutoff}:\")\n",
    "print(''.join([str(score) for score in best_metrics_scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26364cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Used when training on full dataset as there's no val set to tune cutoff)\n",
    "DEFAULT_CUTOFF = 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4883368a",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0bf39",
   "metadata": {},
   "source": [
    "Below function is used to predict on test dataset using trained model. It returns a list of predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e411d940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c68c486eb784f20b6d72ca8e781b327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID relevance\n",
      "0      917         0\n",
      "1     6587         0\n",
      "2     5227         0\n",
      "3     4707         0\n",
      "4      700         0\n",
      "...    ...       ...\n",
      "2936  5590         0\n",
      "2937  5320         0\n",
      "2938  1664         0\n",
      "2939  1245         0\n",
      "2940  4278         0\n",
      "\n",
      "[2941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make prediction on test dataset\n",
    "result_df = pd.DataFrame(columns=[\"ID\", \"relevance\"])\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(test_dl):\n",
    "    ids = batch['ID']\n",
    "    batch = {k: v.to(device) for k, v in batch.items() if k != 'ID'}\n",
    "    with torch.no_grad():\n",
    "        yhat = model(**batch).logits.view(-1)\n",
    "        \n",
    "    preds = torch.sigmoid(yhat) > best_cutoff\n",
    "\n",
    "    for ID, pred in zip(ids, preds):\n",
    "        result_df = result_df.append({'ID': int(ID), 'relevance': int(pred)}, ignore_index=True)\n",
    "    \n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea0642c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions 0.11730703842230533% positives\n"
     ]
    }
   ],
   "source": [
    "num_positives = result_df.relevance.sum()\n",
    "print(f\"Test predictions {num_positives / len(result_df)}% positives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6f10d",
   "metadata": {},
   "source": [
    "### Submit Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55664245",
   "metadata": {},
   "source": [
    "Create a new dataframe for submission. The list of predicted probabilities are converted to labels using the pre-defined threshold of 0.15 (can be tuned for better performance). The list of labels is concatenated with the original sequential ID from the test file downloaded from Leaderboard, to generate the final submission\n",
    "\n",
    "For submission, follow these steps:\n",
    "1. Go to the folder where your notebook is in Sagemaker\n",
    "2. Donwload the file __test_submission_nlp2.csv__ to your local machine\n",
    "3. On NLP2 Leaderboard contest, select option __My Submissions\"__ and upload your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94d2475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"test_submission_nlp2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f441e1",
   "metadata": {},
   "source": [
    "### Model details for submission paste\n",
    "Fine-tuned BERT-large with whole-word masking, previously fine-tuned on SQuAD QA. (see https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad)\n",
    "BCE with no additional positive weighting, 0.5 cutoff. Trained on full train dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
