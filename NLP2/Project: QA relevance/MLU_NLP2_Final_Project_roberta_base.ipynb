{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b28f89",
   "metadata": {},
   "source": [
    "![MLU Logo](../images/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a22de0",
   "metadata": {},
   "source": [
    "# MLU-NLP2 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ae7fb",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "The project focuses on answer selection and uses the WikiQA dataset. Each record in the dataset has a question, answer and relevance score. The relevance score is binary, 1/0 indicating whether the answer is relevant to the question. \n",
    "\n",
    "Each question can be repeated multiple times and can have multiple relevant answer statements. \n",
    "\n",
    "To make the problem less complex, we have considered only questions which have at least 1 relevant answer. This simplification results in train, validation and test datasets with 873, 126 and 243 questions respectively.\n",
    "\n",
    "## Project Objective\n",
    "\n",
    "In this notebook, you will start our jorney. It contains a baseline model that will give you a first performance score and ourse and all code necessary ready for your first submission.\n",
    "\n",
    "__IMPORTANT__ \n",
    "\n",
    "Make sure you submit this notebook to get to know better how Leaderboard works and, also, make sure your completion will be granted :) .\n",
    "\n",
    "## The Baseline Model\n",
    "\n",
    "Here we are using Torchtext: an NLP specific package in Torch. \n",
    "\n",
    "We will generate 100 dim vector embeddings for each word using Glove and build a basic convolutional network which takes the text embeddings as input (50 * 100). The training dataset is trained in batches using this network and the losses in each epoch are backpropagated to update the weights and minimize losses in future iterations.\n",
    "\n",
    "The trained model is then used to make predictions on test dataset and finally, a result dataset with the list of predictions and sequential ID is created for your first leaderboard submission\n",
    "\n",
    "Notebook has been inspired from https://www.kaggle.com/ziliwang/pytorch-text-cnn\n",
    "\n",
    "### __Dataset:__\n",
    "The originial train and test datasets have questions for which there are no answers with relevance 1. To make the problem simpler, we have considered only questions which have atleast 1 answer with relevance score 1. This updated version of the datasets are used in the project\n",
    "\n",
    "### __Table of Contents__\n",
    "Here is the plan for this assignment.\n",
    "<p>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Reading the dataset\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>\n",
    "        Reading the dataset\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Data-Preparation\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>\n",
    "        Data Preparation\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Model-Building\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>\n",
    "        Model Building\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Training\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>\n",
    "        Training\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Prediction\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>\n",
    "        Prediction\n",
    "    </a>\n",
    "</div>\n",
    "<div class=\"lev1\">\n",
    "    <a href=\"#Submit-Results\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>\n",
    "        Submit Results\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296c635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##torchtext is a package within pytorch consisting of data processing utilities and popular datasets for natural language\n",
    "!pip -q install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11c24ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import torchtext\n",
    "from nltk import word_tokenize\n",
    "import random\n",
    "from torch import optim\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f848cf",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "The datasets are in our MLU datalake and can be downloaded to your local instance here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed5f1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "bucketname = 'mlu-courses-datalake' \n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.Bucket(bucketname).download_file('NLP2/data/training.csv', \n",
    "                                         './training.csv') \n",
    "s3.Bucket(bucketname).download_file('NLP2/data/public_test_features.csv', \n",
    "                                         './public_test_features.csv')\n",
    "s3.Bucket(bucketname).download_file('NLP2/data/glove.6B.100d.txt', \n",
    "                                         './glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee0f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILE ='./training.csv'\n",
    "TEST_DATA_FILE = './public_test_features.csv'\n",
    "GLOVE_DATA_FILE = './glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f298e379",
   "metadata": {},
   "source": [
    "Below, we are combining question and answer in each row as 1 single text column for simplicity. Alternatively, we can run two parallel networks for question and answer, merge the output of the 2 networks and have a classification layer as output. You may choose to save the files for ease of use, in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e0028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(TRAIN_DATA_FILE)\n",
    "test=pd.read_csv(TEST_DATA_FILE)\n",
    "#test = test_original.copy()\n",
    "#train['text']=train[['question','answer']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "#test['text']=test[['question','answer']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "#train=train[['text','relevance']].rename(columns={'relevance':'label'})\n",
    "#test=test[['text']]\n",
    "#train.to_csv('train.csv',index=False)\n",
    "#test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc062ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2788</td>\n",
       "      <td>who kill franz ferdinand ww1</td>\n",
       "      <td>A plaque commemorating the location of the Sar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8166</td>\n",
       "      <td>what is a medallion guarantee</td>\n",
       "      <td>Sample of a Medallion signature guarantee stampIn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4289</td>\n",
       "      <td>what does a vote to table a motion mean ?</td>\n",
       "      <td>The difference is the idea of what the table i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8180</td>\n",
       "      <td>when was the lady gaga judas song released</td>\n",
       "      <td>`` Judas '' is a song by American recording ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>725</td>\n",
       "      <td>How did Edgar Allan Poe die ?</td>\n",
       "      <td>His work forced him to move among several citi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                    question  \\\n",
       "0  2788                who kill franz ferdinand ww1   \n",
       "1  8166               what is a medallion guarantee   \n",
       "2  4289   what does a vote to table a motion mean ?   \n",
       "3  8180  when was the lady gaga judas song released   \n",
       "4   725               How did Edgar Allan Poe die ?   \n",
       "\n",
       "                                              answer  relevance  \n",
       "0  A plaque commemorating the location of the Sar...          0  \n",
       "1  Sample of a Medallion signature guarantee stampIn          0  \n",
       "2  The difference is the idea of what the table i...          0  \n",
       "3  `` Judas '' is a song by American recording ar...          1  \n",
       "4  His work forced him to move among several citi...          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef444752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'question', 'answer', 'relevance'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528bcdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917</td>\n",
       "      <td>when does the electoral college votes</td>\n",
       "      <td>The Twelfth Amendment specifies how a Presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6587</td>\n",
       "      <td>what year lord of rings made ?</td>\n",
       "      <td>Tolkien 's work has been the subject of extens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5227</td>\n",
       "      <td>what countries are under the buddhism religion</td>\n",
       "      <td>Estimate of the worldwide Buddhist population ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4707</td>\n",
       "      <td>what does ( sic ) mean ?</td>\n",
       "      <td>Sic may also refer to:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>700</td>\n",
       "      <td>when is it memorial day</td>\n",
       "      <td>In cases involving a family graveyard where re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                        question  \\\n",
       "0   917           when does the electoral college votes   \n",
       "1  6587                  what year lord of rings made ?   \n",
       "2  5227  what countries are under the buddhism religion   \n",
       "3  4707                        what does ( sic ) mean ?   \n",
       "4   700                         when is it memorial day   \n",
       "\n",
       "                                              answer  \n",
       "0  The Twelfth Amendment specifies how a Presiden...  \n",
       "1  Tolkien 's work has been the subject of extens...  \n",
       "2  Estimate of the worldwide Buddhist population ...  \n",
       "3                             Sic may also refer to:  \n",
       "4  In cases involving a family graveyard where re...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802c646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 22.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 54.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.57.0)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 50.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.7.0)\n",
      "Collecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch) (0.8)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Building wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=c1f97f79801e66ff4752dfbe130c403ce3996a2329b104dae99bf651e2af9302\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/7f/26/a3/33f2079871e2bebb3f53a2b21c3ec64129b8efdd18a6263a52\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3\n",
      "Successfully installed nvidia-ml-py3-7.352.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install --upgrade torch\n",
    "!pip install nvidia-ml-py3\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed61277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2788</td>\n",
       "      <td>who kill franz ferdinand ww1</td>\n",
       "      <td>A plaque commemorating the location of the Sar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8166</td>\n",
       "      <td>what is a medallion guarantee</td>\n",
       "      <td>Sample of a Medallion signature guarantee stampIn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4289</td>\n",
       "      <td>what does a vote to table a motion mean ?</td>\n",
       "      <td>The difference is the idea of what the table i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8180</td>\n",
       "      <td>when was the lady gaga judas song released</td>\n",
       "      <td>`` Judas '' is a song by American recording ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>725</td>\n",
       "      <td>How did Edgar Allan Poe die ?</td>\n",
       "      <td>His work forced him to move among several citi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6856</th>\n",
       "      <td>1310</td>\n",
       "      <td>when is the wv state fair</td>\n",
       "      <td>Free parking is provided adjacent to the fairg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6857</th>\n",
       "      <td>3413</td>\n",
       "      <td>what are square diamonds called ?</td>\n",
       "      <td>However , while displaying the same high degre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6858</th>\n",
       "      <td>9631</td>\n",
       "      <td>what is direct marketing channel</td>\n",
       "      <td>Direct marketing is practiced by businesses of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>581</td>\n",
       "      <td>who was charged with murder after the massacre...</td>\n",
       "      <td>They received hate mail and death threats and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860</th>\n",
       "      <td>862</td>\n",
       "      <td>when does air bag deploy</td>\n",
       "      <td>Broad commercial adoption of airbags occurred ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6861 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                           question  \\\n",
       "0     2788                       who kill franz ferdinand ww1   \n",
       "1     8166                      what is a medallion guarantee   \n",
       "2     4289          what does a vote to table a motion mean ?   \n",
       "3     8180         when was the lady gaga judas song released   \n",
       "4      725                      How did Edgar Allan Poe die ?   \n",
       "...    ...                                                ...   \n",
       "6856  1310                          when is the wv state fair   \n",
       "6857  3413                  what are square diamonds called ?   \n",
       "6858  9631                   what is direct marketing channel   \n",
       "6859   581  who was charged with murder after the massacre...   \n",
       "6860   862                           when does air bag deploy   \n",
       "\n",
       "                                                 answer  relevance  \n",
       "0     A plaque commemorating the location of the Sar...          0  \n",
       "1     Sample of a Medallion signature guarantee stampIn          0  \n",
       "2     The difference is the idea of what the table i...          0  \n",
       "3     `` Judas '' is a song by American recording ar...          1  \n",
       "4     His work forced him to move among several citi...          0  \n",
       "...                                                 ...        ...  \n",
       "6856  Free parking is provided adjacent to the fairg...          0  \n",
       "6857  However , while displaying the same high degre...          0  \n",
       "6858  Direct marketing is practiced by businesses of...          0  \n",
       "6859  They received hate mail and death threats and ...          0  \n",
       "6860  Broad commercial adoption of airbags occurred ...          0  \n",
       "\n",
       "[6861 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a80adcd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917</td>\n",
       "      <td>when does the electoral college votes</td>\n",
       "      <td>The Twelfth Amendment specifies how a Presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6587</td>\n",
       "      <td>what year lord of rings made ?</td>\n",
       "      <td>Tolkien 's work has been the subject of extens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5227</td>\n",
       "      <td>what countries are under the buddhism religion</td>\n",
       "      <td>Estimate of the worldwide Buddhist population ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4707</td>\n",
       "      <td>what does ( sic ) mean ?</td>\n",
       "      <td>Sic may also refer to:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>700</td>\n",
       "      <td>when is it memorial day</td>\n",
       "      <td>In cases involving a family graveyard where re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>5590</td>\n",
       "      <td>how many ports are there in networking</td>\n",
       "      <td>That is , data packets are routed across the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>5320</td>\n",
       "      <td>what genre is bloody beetroots</td>\n",
       "      <td>In fact , the only identifying public feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>1664</td>\n",
       "      <td>where is green bay packers from</td>\n",
       "      <td>They are members of the North Division of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>1245</td>\n",
       "      <td>when did the civil war start and where</td>\n",
       "      <td>The Union marshaled the resources and manpower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>4278</td>\n",
       "      <td>what are the three ossicles</td>\n",
       "      <td>The term `` ossicles '' literally means `` tin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2941 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                        question  \\\n",
       "0      917           when does the electoral college votes   \n",
       "1     6587                  what year lord of rings made ?   \n",
       "2     5227  what countries are under the buddhism religion   \n",
       "3     4707                        what does ( sic ) mean ?   \n",
       "4      700                         when is it memorial day   \n",
       "...    ...                                             ...   \n",
       "2936  5590          how many ports are there in networking   \n",
       "2937  5320                  what genre is bloody beetroots   \n",
       "2938  1664                 where is green bay packers from   \n",
       "2939  1245          when did the civil war start and where   \n",
       "2940  4278                     what are the three ossicles   \n",
       "\n",
       "                                                 answer  \n",
       "0     The Twelfth Amendment specifies how a Presiden...  \n",
       "1     Tolkien 's work has been the subject of extens...  \n",
       "2     Estimate of the worldwide Buddhist population ...  \n",
       "3                                Sic may also refer to:  \n",
       "4     In cases involving a family graveyard where re...  \n",
       "...                                                 ...  \n",
       "2936  That is , data packets are routed across the n...  \n",
       "2937  In fact , the only identifying public feature ...  \n",
       "2938  They are members of the North Division of the ...  \n",
       "2939  The Union marshaled the resources and manpower...  \n",
       "2940  The term `` ossicles '' literally means `` tin...  \n",
       "\n",
       "[2941 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9c12459",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train[['question','answer']].apply(lambda row: ' [SEP] '.join(row.values.astype(str)), axis=1)\n",
    "test['text'] = test[['question','answer']].apply(lambda row: ' [SEP] '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c27cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/ec2-user/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/ec2-user/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/ec2-user/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model_name = 'roberta-base'#'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_, val_ = train_test_split(train, test_size=0.1, random_state = 17)\n",
    "\n",
    "#train_encodings = tokenizer(train['text'].tolist(), truncation=True, padding=True)\n",
    "#val_encodings = tokenizer(test['text'].tolist(), truncation=True, padding=True)\n",
    "train_encodings = tokenizer(train_['text'].tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_['text'].tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test['text'].tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "776b2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45be9e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevance</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5303</th>\n",
       "      <td>8296</td>\n",
       "      <td>what is vitamin a for</td>\n",
       "      <td>Both structural features are essential for vit...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is vitamin a for [SEP] Both structural fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>9188</td>\n",
       "      <td>How Works Diaphragm Pump</td>\n",
       "      <td>Those employing one or more unsealed diaphragm...</td>\n",
       "      <td>0</td>\n",
       "      <td>How Works Diaphragm Pump [SEP] Those employing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>1709</td>\n",
       "      <td>who is inventor of the radio</td>\n",
       "      <td>It is considered likely that the first intenti...</td>\n",
       "      <td>0</td>\n",
       "      <td>who is inventor of the radio [SEP] It is consi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5665</th>\n",
       "      <td>4602</td>\n",
       "      <td>what day is the feast of st joseph 's ?</td>\n",
       "      <td>March 19 was dedicated to Saint Joseph in seve...</td>\n",
       "      <td>0</td>\n",
       "      <td>what day is the feast of st joseph 's ? [SEP] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5784</th>\n",
       "      <td>9072</td>\n",
       "      <td>what area code is 949</td>\n",
       "      <td>Area code 949 is an area code in California th...</td>\n",
       "      <td>1</td>\n",
       "      <td>what area code is 949 [SEP] Area code 949 is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>2808</td>\n",
       "      <td>how deep can be drill for deep underwater</td>\n",
       "      <td>“Not all oil is accessible on land or in shall...</td>\n",
       "      <td>0</td>\n",
       "      <td>how deep can be drill for deep underwater [SEP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>7344</td>\n",
       "      <td>how much does united states spend on health care</td>\n",
       "      <td>A 2013 study found that about 25 % of all seni...</td>\n",
       "      <td>0</td>\n",
       "      <td>how much does united states spend on health ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510</th>\n",
       "      <td>8044</td>\n",
       "      <td>what is lean manufacturing and who developed</td>\n",
       "      <td>TPS is renowned for its focus on reduction of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is lean manufacturing and who developed [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>4501</td>\n",
       "      <td>what did ronald reagan do as president</td>\n",
       "      <td>A conservative icon , he ranks highly in publi...</td>\n",
       "      <td>0</td>\n",
       "      <td>what did ronald reagan do as president [SEP] A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>6913</td>\n",
       "      <td>how old is kirk douglas , the actor ?</td>\n",
       "      <td>He is No.17 on the American Film Institute 's ...</td>\n",
       "      <td>0</td>\n",
       "      <td>how old is kirk douglas , the actor ? [SEP] He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6174 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                          question  \\\n",
       "5303  8296                             what is vitamin a for   \n",
       "426   9188                          How Works Diaphragm Pump   \n",
       "3003  1709                      who is inventor of the radio   \n",
       "5665  4602           what day is the feast of st joseph 's ?   \n",
       "5784  9072                             what area code is 949   \n",
       "...    ...                                               ...   \n",
       "1337  2808         how deep can be drill for deep underwater   \n",
       "406   7344  how much does united states spend on health care   \n",
       "5510  8044      what is lean manufacturing and who developed   \n",
       "2191  4501            what did ronald reagan do as president   \n",
       "2671  6913             how old is kirk douglas , the actor ?   \n",
       "\n",
       "                                                 answer  relevance  \\\n",
       "5303  Both structural features are essential for vit...          0   \n",
       "426   Those employing one or more unsealed diaphragm...          0   \n",
       "3003  It is considered likely that the first intenti...          0   \n",
       "5665  March 19 was dedicated to Saint Joseph in seve...          0   \n",
       "5784  Area code 949 is an area code in California th...          1   \n",
       "...                                                 ...        ...   \n",
       "1337  “Not all oil is accessible on land or in shall...          0   \n",
       "406   A 2013 study found that about 25 % of all seni...          0   \n",
       "5510  TPS is renowned for its focus on reduction of ...          0   \n",
       "2191  A conservative icon , he ranks highly in publi...          0   \n",
       "2671  He is No.17 on the American Film Institute 's ...          0   \n",
       "\n",
       "                                                   text  \n",
       "5303  what is vitamin a for [SEP] Both structural fe...  \n",
       "426   How Works Diaphragm Pump [SEP] Those employing...  \n",
       "3003  who is inventor of the radio [SEP] It is consi...  \n",
       "5665  what day is the feast of st joseph 's ? [SEP] ...  \n",
       "5784  what area code is 949 [SEP] Area code 949 is a...  \n",
       "...                                                 ...  \n",
       "1337  how deep can be drill for deep underwater [SEP...  \n",
       "406   how much does united states spend on health ca...  \n",
       "5510  what is lean manufacturing and who developed [...  \n",
       "2191  what did ronald reagan do as president [SEP] A...  \n",
       "2671  how old is kirk douglas , the actor ? [SEP] He...  \n",
       "\n",
       "[6174 rows x 5 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2cab3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_Dataset = CustomDataset(train_encodings, train.relevance.values)\n",
    "#val_Dataset = CustomDataset(val_encodings, np.zeros(test.shape[0]))\n",
    "train_Dataset = CustomDataset(train_encodings, train_.relevance.values)\n",
    "val_Dataset = CustomDataset(val_encodings, val_.relevance.values)\n",
    "tmp =np.zeros(test.shape[0]).astype(int)\n",
    "tmp[0] = 1\n",
    "test_Dataset = CustomDataset(test_encodings, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4a7e75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df5f9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision_P, recall_P, f1_P, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    precision_N, recall_N, f1_N, _ = precision_recall_fscore_support(labels, preds, average='binary', pos_label=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    #fpr, tpr, thresholds = roc_curve(labels, pred, pos_label=2)\n",
    "    #AUC = auc(fpr, tpr)\n",
    "    AUC = roc_auc_score(labels, pred.predictions[:,1])\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'auc': AUC,\n",
    "        'f1_P': f1_P,\n",
    "        'precision_P': precision_P,\n",
    "        'recall_P': recall_P,\n",
    "        'f1_N': f1_N,\n",
    "        'precision_N': precision_N,\n",
    "        'recall_N': recall_N,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5f03dbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    #logging_dir='./logs',            # directory for storing logs\n",
    "    #logging_steps=10,\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_P', #'AUC'\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_Dataset,         # training dataset\n",
    "    eval_dataset=val_Dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e64c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 6174\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1930\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='757' max='1930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 757/1930 28:16 < 43:55, 0.45 it/s, Epoch 1.96/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc</th>\n",
       "      <th>F1 P</th>\n",
       "      <th>Precision P</th>\n",
       "      <th>Recall P</th>\n",
       "      <th>F1 N</th>\n",
       "      <th>Precision N</th>\n",
       "      <th>Recall N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.640084</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>0.525740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929907</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.484446</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>0.542397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929907</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.388281</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>0.748576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929907</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.365145</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>0.788200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929907</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.316300</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>0.831379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929907</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.313986</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>0.834003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929907</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.335820</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>0.812265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929907</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.290632</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>0.850717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929907</td>\n",
       "      <td>0.868996</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.328692</td>\n",
       "      <td>0.871907</td>\n",
       "      <td>0.827731</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.931250</td>\n",
       "      <td>0.872621</td>\n",
       "      <td>0.998325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.282899</td>\n",
       "      <td>0.898108</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.942997</td>\n",
       "      <td>0.917591</td>\n",
       "      <td>0.969849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.365798</td>\n",
       "      <td>0.879185</td>\n",
       "      <td>0.860785</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.934800</td>\n",
       "      <td>0.880178</td>\n",
       "      <td>0.996650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.313092</td>\n",
       "      <td>0.896652</td>\n",
       "      <td>0.879918</td>\n",
       "      <td>0.523490</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.942041</td>\n",
       "      <td>0.918790</td>\n",
       "      <td>0.966499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.289680</td>\n",
       "      <td>0.895197</td>\n",
       "      <td>0.882393</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>0.940984</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>0.961474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.265624</td>\n",
       "      <td>0.895197</td>\n",
       "      <td>0.891439</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.941080</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.963149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.278335</td>\n",
       "      <td>0.895197</td>\n",
       "      <td>0.872902</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>0.344444</td>\n",
       "      <td>0.941935</td>\n",
       "      <td>0.908243</td>\n",
       "      <td>0.978224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.325225</td>\n",
       "      <td>0.899563</td>\n",
       "      <td>0.884497</td>\n",
       "      <td>0.473282</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.344444</td>\n",
       "      <td>0.944489</td>\n",
       "      <td>0.908669</td>\n",
       "      <td>0.983250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.278868</td>\n",
       "      <td>0.890830</td>\n",
       "      <td>0.883845</td>\n",
       "      <td>0.581006</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.937238</td>\n",
       "      <td>0.936455</td>\n",
       "      <td>0.938023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.268238</td>\n",
       "      <td>0.880640</td>\n",
       "      <td>0.888442</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.932007</td>\n",
       "      <td>0.922824</td>\n",
       "      <td>0.941374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.287446</td>\n",
       "      <td>0.902475</td>\n",
       "      <td>0.875824</td>\n",
       "      <td>0.544218</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.945395</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.971524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.275908</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>0.883157</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>0.949593</td>\n",
       "      <td>0.922591</td>\n",
       "      <td>0.978224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.259749</td>\n",
       "      <td>0.905386</td>\n",
       "      <td>0.902140</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.947111</td>\n",
       "      <td>0.920886</td>\n",
       "      <td>0.974874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.252155</td>\n",
       "      <td>0.898108</td>\n",
       "      <td>0.900130</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.941860</td>\n",
       "      <td>0.934102</td>\n",
       "      <td>0.949749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.906841</td>\n",
       "      <td>0.901117</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.947712</td>\n",
       "      <td>0.925040</td>\n",
       "      <td>0.971524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.241354</td>\n",
       "      <td>0.899563</td>\n",
       "      <td>0.901414</td>\n",
       "      <td>0.560510</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.943303</td>\n",
       "      <td>0.925806</td>\n",
       "      <td>0.961474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.247838</td>\n",
       "      <td>0.899563</td>\n",
       "      <td>0.896836</td>\n",
       "      <td>0.605714</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.942452</td>\n",
       "      <td>0.938538</td>\n",
       "      <td>0.946399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.236352</td>\n",
       "      <td>0.912664</td>\n",
       "      <td>0.902810</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.950577</td>\n",
       "      <td>0.935170</td>\n",
       "      <td>0.966499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.241434</td>\n",
       "      <td>0.893741</td>\n",
       "      <td>0.906533</td>\n",
       "      <td>0.613757</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.938397</td>\n",
       "      <td>0.945578</td>\n",
       "      <td>0.931323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.226100</td>\n",
       "      <td>0.912664</td>\n",
       "      <td>0.913065</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.950739</td>\n",
       "      <td>0.932367</td>\n",
       "      <td>0.969849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.231392</td>\n",
       "      <td>0.912664</td>\n",
       "      <td>0.919877</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.950900</td>\n",
       "      <td>0.929600</td>\n",
       "      <td>0.973199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.236658</td>\n",
       "      <td>0.912664</td>\n",
       "      <td>0.921980</td>\n",
       "      <td>0.589041</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.951140</td>\n",
       "      <td>0.925515</td>\n",
       "      <td>0.978224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-25\n",
      "Configuration saved in ./results/checkpoint-25/config.json\n",
      "Model weights saved in ./results/checkpoint-25/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-850] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-50\n",
      "Configuration saved in ./results/checkpoint-50/config.json\n",
      "Model weights saved in ./results/checkpoint-50/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1175] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-75\n",
      "Configuration saved in ./results/checkpoint-75/config.json\n",
      "Model weights saved in ./results/checkpoint-75/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-100\n",
      "Configuration saved in ./results/checkpoint-100/config.json\n",
      "Model weights saved in ./results/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1225] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-125\n",
      "Configuration saved in ./results/checkpoint-125/config.json\n",
      "Model weights saved in ./results/checkpoint-125/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-150\n",
      "Configuration saved in ./results/checkpoint-150/config.json\n",
      "Model weights saved in ./results/checkpoint-150/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-175\n",
      "Configuration saved in ./results/checkpoint-175/config.json\n",
      "Model weights saved in ./results/checkpoint-175/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-75] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-200\n",
      "Configuration saved in ./results/checkpoint-200/config.json\n",
      "Model weights saved in ./results/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-225\n",
      "Configuration saved in ./results/checkpoint-225/config.json\n",
      "Model weights saved in ./results/checkpoint-225/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-25] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-250\n",
      "Configuration saved in ./results/checkpoint-250/config.json\n",
      "Model weights saved in ./results/checkpoint-250/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-125] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-275\n",
      "Configuration saved in ./results/checkpoint-275/config.json\n",
      "Model weights saved in ./results/checkpoint-275/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-300\n",
      "Configuration saved in ./results/checkpoint-300/config.json\n",
      "Model weights saved in ./results/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-175] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-325\n",
      "Configuration saved in ./results/checkpoint-325/config.json\n",
      "Model weights saved in ./results/checkpoint-325/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-350\n",
      "Configuration saved in ./results/checkpoint-350/config.json\n",
      "Model weights saved in ./results/checkpoint-350/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-225] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-375\n",
      "Configuration saved in ./results/checkpoint-375/config.json\n",
      "Model weights saved in ./results/checkpoint-375/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-400\n",
      "Configuration saved in ./results/checkpoint-400/config.json\n",
      "Model weights saved in ./results/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-275] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-425\n",
      "Configuration saved in ./results/checkpoint-425/config.json\n",
      "Model weights saved in ./results/checkpoint-425/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-450\n",
      "Configuration saved in ./results/checkpoint-450/config.json\n",
      "Model weights saved in ./results/checkpoint-450/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-325] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-475\n",
      "Configuration saved in ./results/checkpoint-475/config.json\n",
      "Model weights saved in ./results/checkpoint-475/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-375] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-525\n",
      "Configuration saved in ./results/checkpoint-525/config.json\n",
      "Model weights saved in ./results/checkpoint-525/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-550\n",
      "Configuration saved in ./results/checkpoint-550/config.json\n",
      "Model weights saved in ./results/checkpoint-550/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-425] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-575\n",
      "Configuration saved in ./results/checkpoint-575/config.json\n",
      "Model weights saved in ./results/checkpoint-575/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-600\n",
      "Configuration saved in ./results/checkpoint-600/config.json\n",
      "Model weights saved in ./results/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-475] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-625\n",
      "Configuration saved in ./results/checkpoint-625/config.json\n",
      "Model weights saved in ./results/checkpoint-625/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-650\n",
      "Configuration saved in ./results/checkpoint-650/config.json\n",
      "Model weights saved in ./results/checkpoint-650/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-525] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-675\n",
      "Configuration saved in ./results/checkpoint-675/config.json\n",
      "Model weights saved in ./results/checkpoint-675/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-700\n",
      "Configuration saved in ./results/checkpoint-700/config.json\n",
      "Model weights saved in ./results/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-575] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-725\n",
      "Configuration saved in ./results/checkpoint-725/config.json\n",
      "Model weights saved in ./results/checkpoint-725/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-750\n",
      "Configuration saved in ./results/checkpoint-750/config.json\n",
      "Model weights saved in ./results/checkpoint-750/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-625] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f921a3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2941\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(test_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5902af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ad079",
   "metadata": {},
   "source": [
    "### Submit Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebffddf3",
   "metadata": {},
   "source": [
    "Create a new dataframe for submission. The list of predicted probabilities are converted to labels using the pre-defined threshold of 0.15 (can be tuned for better performance). The list of labels is concatenated with the original sequential ID from the test file downloaded from Leaderboard, to generate the final submission\n",
    "\n",
    "For submission, follow these steps:\n",
    "1. Go to the folder where your notebook is in Sagemaker\n",
    "2. Donwload the file __test_submission_nlp2.csv__ to your local machine\n",
    "3. On NLP2 Leaderboard contest, select option __My Submissions\"__ and upload your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3f1de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=[\"ID\", \"relevance\"])\n",
    "result_df[\"ID\"] = test[\"ID\"].tolist()\n",
    "labels=np.argmax(pred.predictions, axis=1)\n",
    "result_df[\"relevance\"] = labels\n",
    "result_df.to_csv(\"test_submission_nlp2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ff943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
