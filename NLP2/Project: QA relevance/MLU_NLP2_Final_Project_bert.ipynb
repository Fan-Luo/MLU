{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Sentence Pair Classification with BERT\n",
    "\n",
    "Pre-trained language representations have been shown to improve many downstream NLP tasks such as\n",
    "question answering, and natural language inference. To apply pre-trained\n",
    "representations to these tasks, there are two main strategies:\n",
    "\n",
    "1. The *feature-based* approach, which uses the pre-trained representations as additional\n",
    "features to the downstream task.\n",
    "2. Or the *fine-tuning*-based approach, which trains the downstream tasks by\n",
    "fine-tuning pre-trained parameters.\n",
    "\n",
    "While feature-based approaches such as ELMo [3] (introduced in the previous tutorial) are effective\n",
    "in improving many downstream tasks, they require task-specific architectures.\n",
    "Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations\n",
    "from Transformers), which *fine-tunes* deep bi-directional representations on a\n",
    "wide range of tasks with minimal task-specific parameters, and obtains state-\n",
    "of-the-art results.\n",
    "\n",
    "In this tutorial, we will focus on fine-tuning with the\n",
    "pre-trained BERT model to classify semantically equivalent sentence pairs.\n",
    "\n",
    "Specifically, we will:\n",
    "\n",
    "1. Load the state-of-the-art pre-trained BERT model and attach an additional layer for classification\n",
    "2. Process and transform sentence-pair data for the task at hand\n",
    "3. Fine-tune the BERT model for sentence classification\n",
    "\n",
    "## Setup\n",
    "\n",
    "To use this tutorial, please download the required files from the above download link, and install\n",
    "GluonNLP.\n",
    "\n",
    "### Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gluonnlp in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from gluonnlp) (1.19.5)\n",
      "Requirement already satisfied: cython in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from gluonnlp) (0.29.22)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from gluonnlp) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from packaging->gluonnlp) (2.4.7)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipdb in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (0.13.9)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from ipdb) (4.4.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from ipdb) (49.6.0.post20210108)\n",
      "Requirement already satisfied: toml>=0.10.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from ipdb) (0.10.2)\n",
      "Requirement already satisfied: ipython<7.17.0,>=7.10.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from ipdb) (7.16.1)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from ipython<7.17.0,>=7.10.0->ipdb) (0.17.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from ipython<7.17.0,>=7.10.0->ipdb) (4.3.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from ipython<7.17.0,>=7.10.0->ipdb) (3.0.5)\n",
      "Requirement already satisfied: pexpect in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from ipython<7.17.0,>=7.10.0->ipdb) (4.8.0)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from ipython<7.17.0,>=7.10.0->ipdb) (2.8.0)\n",
      "Requirement already satisfied: backcall in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from ipython<7.17.0,>=7.10.0->ipdb) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from ipython<7.17.0,>=7.10.0->ipdb) (0.7.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from jedi>=0.10->ipython<7.17.0,>=7.10.0->ipdb) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<7.17.0,>=7.10.0->ipdb) (0.2.5)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from traitlets>=4.2->ipython<7.17.0,>=7.10.0->ipdb) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from traitlets>=4.2->ipython<7.17.0,>=7.10.0->ipdb) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from pexpect->ipython<7.17.0,>=7.10.0->ipdb) (0.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# module_path = os.path.abspath(os.path.join('..'))\n",
    "# # if module_path not in sys.path:\n",
    "# #     sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp.calibration import BertLayerCollector\n",
    "# this notebook assumes that all required scripts are already\n",
    "# downloaded from the corresponding tutorial webpage on http://gluon-nlp.mxnet.io\n",
    "from bert import data\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nlp.utils.check_version('0.8.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "\n",
    "Please note the comment in the code if no GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "# change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the pre-trained BERT model\n",
    "\n",
    "The list of pre-trained BERT models available\n",
    "in GluonNLP can be found\n",
    "[here](../../model_zoo/bert/index.rst).\n",
    "\n",
    "In this\n",
    "tutorial, the BERT model we will use is BERT\n",
    "BASE trained on an uncased corpus of books and\n",
    "the English Wikipedia dataset in the\n",
    "GluonNLP model zoo.\n",
    "\n",
    "### Get BERT\n",
    "\n",
    "Let's first take\n",
    "a look at the BERT model\n",
    "architecture for sentence pair classification below:\n",
    "<div style=\"width:\n",
    "500px;\">![bert-sentence-pair](bert-sentence-pair.png)</div>\n",
    "where the model takes a pair of\n",
    "sequences and pools the representation of the\n",
    "first token in the sequence.\n",
    "Note that the original BERT model was trained for a\n",
    "masked language model and next-sentence prediction tasks, which includes layers\n",
    "for language model decoding and\n",
    "classification. These layers will not be used\n",
    "for fine-tuning the sentence pair classification.\n",
    "\n",
    "We can load the\n",
    "pre-trained BERT fairly easily\n",
    "using the model API in GluonNLP, which returns the vocabulary\n",
    "along with the\n",
    "model. We include the pooler layer of the pre-trained model by setting\n",
    "`use_pooler` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): DotProductSelfAttentionCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(30522 -> 768, float32)\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                             dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                             use_decoder=False, use_classifier=False)\n",
    "print(bert_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the model for `SentencePair` classification\n",
    "\n",
    "Now that we have loaded\n",
    "the BERT model, we only need to attach an additional layer for classification.\n",
    "The `BERTClassifier` class uses a BERT base model to encode sentence\n",
    "representation, followed by a `nn.Dense` layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier = nlp.model.BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "# only need to initialize the classifier layer.\n",
    "bert_classifier.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "bert_classifier.hybridize(static_alloc=True)\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = mx.gluon.loss.SoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "metric = mx.metric.Accuracy()\n",
    "# metric = mx.metric.F1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for BERT\n",
    "\n",
    "For this tutorial, we need to do a bit of preprocessing before feeding our data introduced\n",
    "the BERT model. Here we want to leverage the dataset included in the downloaded archive at the\n",
    "beginning of this tutorial.\n",
    "\n",
    "### Loading the dataset\n",
    "\n",
    "We use\n",
    "the dev set of the\n",
    "Microsoft Research Paraphrase Corpus dataset. The file is\n",
    "named 'dev.tsv'. Let's take a look at the first few lines of the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsv_file = io.open('dev.tsv', encoding='utf-8')\n",
    "# for i in range(5):\n",
    "#     print(tsv_file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains 5 columns, separated by tabs.\n",
    "The header of\n",
    "the file explains each of these columns, although an explanation for each is included\n",
    "here:\n",
    "0. The label indicating whether the two\n",
    "sentences are semantically equivalent\n",
    "1. The id of the first sentence in this\n",
    "sample\n",
    "2. The id of the second sentence in this sample\n",
    "3. The content of the\n",
    "first sentence\n",
    "4. The content of the second sentence\n",
    "\n",
    "For our task, we are\n",
    "interested in the 0th, 3rd and 4th columns.\n",
    "To load this dataset, we can use the\n",
    "`TSVDataset` API and skip the first line because it's just the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2788</td>\n",
       "      <td>who kill franz ferdinand ww1</td>\n",
       "      <td>A plaque commemorating the location of the Sar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8166</td>\n",
       "      <td>what is a medallion guarantee</td>\n",
       "      <td>Sample of a Medallion signature guarantee stampIn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4289</td>\n",
       "      <td>what does a vote to table a motion mean ?</td>\n",
       "      <td>The difference is the idea of what the table i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                   question  \\\n",
       "0  2788               who kill franz ferdinand ww1   \n",
       "1  8166              what is a medallion guarantee   \n",
       "2  4289  what does a vote to table a motion mean ?   \n",
       "\n",
       "                                              answer  relevance  \n",
       "0  A plaque commemorating the location of the Sar...          0  \n",
       "1  Sample of a Medallion signature guarantee stampIn          0  \n",
       "2  The difference is the idea of what the table i...          0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who kill franz ferdinand ww1\n",
      "A plaque commemorating the location of the Sarajevo assassination ( image taken in 2009 )\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_df['question'] = train_df['question'].astype(str)\n",
    "train_df['answer'] = train_df['answer'].astype(str)\n",
    "\n",
    "train_data_raw = train_df[['question', 'answer', 'relevance']].values\n",
    "data_train_raw = mx.gluon.data.SimpleDataset(train_data_raw)\n",
    "\n",
    "sample_id = 0\n",
    "# Sentence A\n",
    "print(data_train_raw[sample_id][0])\n",
    "# Sentence B\n",
    "print(data_train_raw[sample_id][1])\n",
    "# 1 means equivalent, 0 means not equivalent\n",
    "print(data_train_raw[sample_id][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6861"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "838"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_df['relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12213962979157557"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "838/6861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the pre-trained BERT model, we need to pre-process the data in the same\n",
    "way it was trained. The following figure shows the input representation in BERT:\n",
    "<div style=\"width: 500px;\">![bert-embed](bert-embed.png)</div>\n",
    "\n",
    "We will use\n",
    "`BERTDatasetTransform` to perform the following transformations:\n",
    "- tokenize\n",
    "the\n",
    "input sequences\n",
    "- insert [CLS] at the beginning\n",
    "- insert [SEP] between sentence\n",
    "A and sentence B, and at the end\n",
    "- generate segment ids to indicate whether\n",
    "a token belongs to the first sequence or the second sequence.\n",
    "- generate valid length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2  2040  3102  8965  9684  1059  2860  2487     3  1037 11952 20646\n",
      "  1996  3295  1997  1996 18354 10102  1006  3746  2579  1999  2268  1007\n",
      "     3     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "valid length = \n",
      "25\n",
      "label = \n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Use the vocabulary from pre-trained model for tokenization\n",
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "\n",
    "# The maximum length of an input sequence\n",
    "# max_len = 64\n",
    "max_len = 128\n",
    "# The labels for the two classes \n",
    "all_labels = [0, 1]\n",
    "\n",
    "# whether to transform the data as sentence pairs.\n",
    "# for single sentence classification, set pair=False\n",
    "# for regression task, set class_labels=None\n",
    "# for inference without label available, set has_label=False\n",
    "pair = True\n",
    "transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                                class_labels=all_labels,\n",
    "                                                has_label=True,\n",
    "                                                pad=True,\n",
    "                                                pair=pair)\n",
    "data_train = data_train_raw.transform(transform)\n",
    "\n",
    "print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "print('%s token id = %s'%(vocabulary.padding_token, vocabulary[vocabulary.padding_token]))\n",
    "print('%s token id = %s'%(vocabulary.cls_token, vocabulary[vocabulary.cls_token]))\n",
    "print('%s token id = %s'%(vocabulary.sep_token, vocabulary[vocabulary.sep_token]))\n",
    "print('token ids = \\n%s'%data_train[sample_id][0])\n",
    "print('segment ids = \\n%s'%data_train[sample_id][1])\n",
    "print('valid length = \\n%s'%data_train[sample_id][2])\n",
    "print('label = \\n%s'%data_train[sample_id][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "# The FixedBucketSampler and the DataLoader for making the mini-batches\n",
    "train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[2]) for item in data_train],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2788</td>\n",
       "      <td>who kill franz ferdinand ww1</td>\n",
       "      <td>A plaque commemorating the location of the Sar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8166</td>\n",
       "      <td>what is a medallion guarantee</td>\n",
       "      <td>Sample of a Medallion signature guarantee stampIn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4289</td>\n",
       "      <td>what does a vote to table a motion mean ?</td>\n",
       "      <td>The difference is the idea of what the table i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8180</td>\n",
       "      <td>when was the lady gaga judas song released</td>\n",
       "      <td>`` Judas '' is a song by American recording ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>725</td>\n",
       "      <td>How did Edgar Allan Poe die ?</td>\n",
       "      <td>His work forced him to move among several citi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6856</th>\n",
       "      <td>1310</td>\n",
       "      <td>when is the wv state fair</td>\n",
       "      <td>Free parking is provided adjacent to the fairg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6857</th>\n",
       "      <td>3413</td>\n",
       "      <td>what are square diamonds called ?</td>\n",
       "      <td>However , while displaying the same high degre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6858</th>\n",
       "      <td>9631</td>\n",
       "      <td>what is direct marketing channel</td>\n",
       "      <td>Direct marketing is practiced by businesses of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>581</td>\n",
       "      <td>who was charged with murder after the massacre...</td>\n",
       "      <td>They received hate mail and death threats and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860</th>\n",
       "      <td>862</td>\n",
       "      <td>when does air bag deploy</td>\n",
       "      <td>Broad commercial adoption of airbags occurred ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6861 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                           question  \\\n",
       "0     2788                       who kill franz ferdinand ww1   \n",
       "1     8166                      what is a medallion guarantee   \n",
       "2     4289          what does a vote to table a motion mean ?   \n",
       "3     8180         when was the lady gaga judas song released   \n",
       "4      725                      How did Edgar Allan Poe die ?   \n",
       "...    ...                                                ...   \n",
       "6856  1310                          when is the wv state fair   \n",
       "6857  3413                  what are square diamonds called ?   \n",
       "6858  9631                   what is direct marketing channel   \n",
       "6859   581  who was charged with murder after the massacre...   \n",
       "6860   862                           when does air bag deploy   \n",
       "\n",
       "                                                 answer  relevance  \n",
       "0     A plaque commemorating the location of the Sar...          0  \n",
       "1     Sample of a Medallion signature guarantee stampIn          0  \n",
       "2     The difference is the idea of what the table i...          0  \n",
       "3     `` Judas '' is a song by American recording ar...          1  \n",
       "4     His work forced him to move among several citi...          0  \n",
       "...                                                 ...        ...  \n",
       "6856  Free parking is provided adjacent to the fairg...          0  \n",
       "6857  However , while displaying the same high degre...          0  \n",
       "6858  Direct marketing is practiced by businesses of...          0  \n",
       "6859  They received hate mail and death threats and ...          0  \n",
       "6860  Broad commercial adoption of airbags occurred ...          0  \n",
       "\n",
       "[6861 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(train_df, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5488"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1373"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data_raw = X_train[['question', 'answer', 'relevance']].values\n",
    "X_val_data_raw = X_val[['question', 'answer', 'relevance']].values\n",
    "X_data_train_raw = mx.gluon.data.SimpleDataset(X_train_data_raw)\n",
    "X_data_val_raw = mx.gluon.data.SimpleDataset(X_val_data_raw)\n",
    "\n",
    "X_data_train = X_data_train_raw.transform(transform)\n",
    "X_data_val = X_data_val_raw.transform(transform)\n",
    "\n",
    "\n",
    "# The training set dataloader\n",
    "X_train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[2]) for item in X_data_train],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "X_train_dataloader = mx.gluon.data.DataLoader(X_data_train, batch_sampler=X_train_sampler)\n",
    "\n",
    "# validation set dataloader\n",
    "X_val_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[2]) for item in X_data_val],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "X_val_dataloader = mx.gluon.data.DataLoader(X_data_val, batch_sampler=X_val_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 4/347] loss=0.6525, lr=0.0000050, acc=0.692\n",
      "[Epoch 0 Batch 8/347] loss=0.6371, lr=0.0000050, acc=0.690\n",
      "[Epoch 0 Batch 12/347] loss=0.6757, lr=0.0000050, acc=0.684\n",
      "[Epoch 0 Batch 16/347] loss=0.5760, lr=0.0000050, acc=0.735\n",
      "[Epoch 0 Batch 20/347] loss=0.5911, lr=0.0000050, acc=0.745\n",
      "[Epoch 0 Batch 24/347] loss=0.5841, lr=0.0000050, acc=0.754\n",
      "[Epoch 0 Batch 28/347] loss=0.4885, lr=0.0000050, acc=0.777\n",
      "[Epoch 0 Batch 32/347] loss=0.5244, lr=0.0000050, acc=0.783\n",
      "[Epoch 0 Batch 36/347] loss=0.4782, lr=0.0000050, acc=0.797\n",
      "[Epoch 0 Batch 40/347] loss=0.4970, lr=0.0000050, acc=0.802\n",
      "[Epoch 0 Batch 44/347] loss=0.4691, lr=0.0000050, acc=0.809\n",
      "[Epoch 0 Batch 48/347] loss=0.4600, lr=0.0000050, acc=0.812\n",
      "[Epoch 0 Batch 52/347] loss=0.4026, lr=0.0000050, acc=0.818\n",
      "[Epoch 0 Batch 56/347] loss=0.5692, lr=0.0000050, acc=0.813\n",
      "[Epoch 0 Batch 60/347] loss=0.3906, lr=0.0000050, acc=0.819\n",
      "[Epoch 0 Batch 64/347] loss=0.3705, lr=0.0000050, acc=0.824\n",
      "[Epoch 0 Batch 68/347] loss=0.4175, lr=0.0000050, acc=0.826\n",
      "[Epoch 0 Batch 72/347] loss=0.3222, lr=0.0000050, acc=0.832\n",
      "[Epoch 0 Batch 76/347] loss=0.3221, lr=0.0000050, acc=0.837\n",
      "[Epoch 0 Batch 80/347] loss=0.3672, lr=0.0000050, acc=0.840\n",
      "[Epoch 0 Batch 84/347] loss=0.3238, lr=0.0000050, acc=0.843\n",
      "[Epoch 0 Batch 88/347] loss=0.3636, lr=0.0000050, acc=0.845\n",
      "[Epoch 0 Batch 92/347] loss=0.3778, lr=0.0000050, acc=0.846\n",
      "[Epoch 0 Batch 96/347] loss=0.4008, lr=0.0000050, acc=0.846\n",
      "[Epoch 0 Batch 100/347] loss=0.3276, lr=0.0000050, acc=0.848\n",
      "[Epoch 0 Batch 104/347] loss=0.5229, lr=0.0000050, acc=0.846\n",
      "[Epoch 0 Batch 108/347] loss=0.3690, lr=0.0000050, acc=0.847\n",
      "[Epoch 0 Batch 112/347] loss=0.2645, lr=0.0000050, acc=0.850\n",
      "[Epoch 0 Batch 116/347] loss=0.3276, lr=0.0000050, acc=0.851\n",
      "[Epoch 0 Batch 120/347] loss=0.3344, lr=0.0000050, acc=0.852\n",
      "[Epoch 0 Batch 124/347] loss=0.3879, lr=0.0000050, acc=0.852\n",
      "[Epoch 0 Batch 128/347] loss=0.4774, lr=0.0000050, acc=0.851\n",
      "[Epoch 0 Batch 132/347] loss=0.4579, lr=0.0000050, acc=0.850\n",
      "[Epoch 0 Batch 136/347] loss=0.4429, lr=0.0000050, acc=0.849\n",
      "[Epoch 0 Batch 140/347] loss=0.3532, lr=0.0000050, acc=0.850\n",
      "[Epoch 0 Batch 144/347] loss=0.3293, lr=0.0000050, acc=0.851\n",
      "[Epoch 0 Batch 148/347] loss=0.3464, lr=0.0000050, acc=0.851\n",
      "[Epoch 0 Batch 152/347] loss=0.2311, lr=0.0000050, acc=0.853\n",
      "[Epoch 0 Batch 156/347] loss=0.3653, lr=0.0000050, acc=0.854\n",
      "[Epoch 0 Batch 160/347] loss=0.3001, lr=0.0000050, acc=0.854\n",
      "[Epoch 0 Batch 164/347] loss=0.2343, lr=0.0000050, acc=0.856\n",
      "[Epoch 0 Batch 168/347] loss=0.5202, lr=0.0000050, acc=0.855\n",
      "[Epoch 0 Batch 172/347] loss=0.4501, lr=0.0000050, acc=0.855\n",
      "[Epoch 0 Batch 176/347] loss=0.3545, lr=0.0000050, acc=0.856\n",
      "[Epoch 0 Batch 180/347] loss=0.3622, lr=0.0000050, acc=0.856\n",
      "[Epoch 0 Batch 184/347] loss=0.2224, lr=0.0000050, acc=0.857\n",
      "[Epoch 0 Batch 188/347] loss=0.2077, lr=0.0000050, acc=0.859\n",
      "[Epoch 0 Batch 192/347] loss=0.4827, lr=0.0000050, acc=0.858\n",
      "[Epoch 0 Batch 196/347] loss=0.2297, lr=0.0000050, acc=0.859\n",
      "[Epoch 0 Batch 200/347] loss=0.3321, lr=0.0000050, acc=0.859\n",
      "[Epoch 0 Batch 204/347] loss=0.4458, lr=0.0000050, acc=0.858\n",
      "[Epoch 0 Batch 208/347] loss=0.4498, lr=0.0000050, acc=0.858\n",
      "[Epoch 0 Batch 212/347] loss=0.2216, lr=0.0000050, acc=0.859\n",
      "[Epoch 0 Batch 216/347] loss=0.2819, lr=0.0000050, acc=0.860\n",
      "[Epoch 0 Batch 220/347] loss=0.3228, lr=0.0000050, acc=0.860\n",
      "[Epoch 0 Batch 224/347] loss=0.2864, lr=0.0000050, acc=0.861\n",
      "[Epoch 0 Batch 228/347] loss=0.3570, lr=0.0000050, acc=0.861\n",
      "[Epoch 0 Batch 232/347] loss=0.3075, lr=0.0000050, acc=0.862\n",
      "[Epoch 0 Batch 236/347] loss=0.2534, lr=0.0000050, acc=0.863\n",
      "[Epoch 0 Batch 240/347] loss=0.3857, lr=0.0000050, acc=0.862\n",
      "[Epoch 0 Batch 244/347] loss=0.3682, lr=0.0000050, acc=0.861\n",
      "[Epoch 0 Batch 248/347] loss=0.2478, lr=0.0000050, acc=0.862\n",
      "[Epoch 0 Batch 252/347] loss=0.2778, lr=0.0000050, acc=0.863\n",
      "[Epoch 0 Batch 256/347] loss=0.3509, lr=0.0000050, acc=0.863\n",
      "[Epoch 0 Batch 260/347] loss=0.2956, lr=0.0000050, acc=0.864\n",
      "[Epoch 0 Batch 264/347] loss=0.2385, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 268/347] loss=0.2071, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 272/347] loss=0.2967, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 276/347] loss=0.1980, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 280/347] loss=0.4288, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 284/347] loss=0.2151, lr=0.0000050, acc=0.867\n",
      "[Epoch 0 Batch 288/347] loss=0.2723, lr=0.0000050, acc=0.867\n",
      "[Epoch 0 Batch 292/347] loss=0.1632, lr=0.0000050, acc=0.868\n",
      "[Epoch 0 Batch 296/347] loss=0.1707, lr=0.0000050, acc=0.868\n",
      "[Epoch 0 Batch 300/347] loss=0.3640, lr=0.0000050, acc=0.868\n",
      "[Epoch 0 Batch 304/347] loss=0.1838, lr=0.0000050, acc=0.869\n",
      "[Epoch 0 Batch 308/347] loss=0.3532, lr=0.0000050, acc=0.869\n",
      "[Epoch 0 Batch 312/347] loss=0.3777, lr=0.0000050, acc=0.869\n",
      "[Epoch 0 Batch 316/347] loss=0.2982, lr=0.0000050, acc=0.869\n",
      "[Epoch 0 Batch 320/347] loss=0.2606, lr=0.0000050, acc=0.870\n",
      "[Epoch 0 Batch 324/347] loss=0.3843, lr=0.0000050, acc=0.870\n",
      "[Epoch 0 Batch 328/347] loss=0.5096, lr=0.0000050, acc=0.869\n",
      "[Epoch 0 Batch 332/347] loss=0.2471, lr=0.0000050, acc=0.869\n",
      "[Epoch 0 Batch 336/347] loss=0.1275, lr=0.0000050, acc=0.870\n",
      "[Epoch 0 Batch 340/347] loss=0.1731, lr=0.0000050, acc=0.872\n",
      "[Epoch 0 Batch 344/347] loss=0.4025, lr=0.0000050, acc=0.872\n",
      "Test F1: 0.4761904761904762, Accuracy Score: 0.8958485069191552\n",
      "[Epoch 1 Batch 4/347] loss=0.1643, lr=0.0000050, acc=0.969\n",
      "[Epoch 1 Batch 8/347] loss=0.4388, lr=0.0000050, acc=0.914\n",
      "[Epoch 1 Batch 12/347] loss=0.2545, lr=0.0000050, acc=0.901\n",
      "[Epoch 1 Batch 16/347] loss=0.2829, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 20/347] loss=0.1516, lr=0.0000050, acc=0.912\n",
      "[Epoch 1 Batch 24/347] loss=0.1731, lr=0.0000050, acc=0.919\n",
      "[Epoch 1 Batch 28/347] loss=0.1841, lr=0.0000050, acc=0.917\n",
      "[Epoch 1 Batch 32/347] loss=0.3846, lr=0.0000050, acc=0.904\n",
      "[Epoch 1 Batch 36/347] loss=0.1888, lr=0.0000050, acc=0.908\n",
      "[Epoch 1 Batch 40/347] loss=0.2442, lr=0.0000050, acc=0.908\n",
      "[Epoch 1 Batch 44/347] loss=0.3122, lr=0.0000050, acc=0.906\n",
      "[Epoch 1 Batch 48/347] loss=0.2003, lr=0.0000050, acc=0.908\n",
      "[Epoch 1 Batch 52/347] loss=0.2299, lr=0.0000050, acc=0.909\n",
      "[Epoch 1 Batch 56/347] loss=0.1974, lr=0.0000050, acc=0.911\n",
      "[Epoch 1 Batch 60/347] loss=0.1076, lr=0.0000050, acc=0.912\n",
      "[Epoch 1 Batch 64/347] loss=0.1356, lr=0.0000050, acc=0.915\n",
      "[Epoch 1 Batch 68/347] loss=0.1822, lr=0.0000050, acc=0.916\n",
      "[Epoch 1 Batch 72/347] loss=0.2755, lr=0.0000050, acc=0.914\n",
      "[Epoch 1 Batch 76/347] loss=0.3139, lr=0.0000050, acc=0.914\n",
      "[Epoch 1 Batch 80/347] loss=0.3210, lr=0.0000050, acc=0.913\n",
      "[Epoch 1 Batch 84/347] loss=0.2818, lr=0.0000050, acc=0.911\n",
      "[Epoch 1 Batch 88/347] loss=0.3230, lr=0.0000050, acc=0.910\n",
      "[Epoch 1 Batch 92/347] loss=0.2617, lr=0.0000050, acc=0.909\n",
      "[Epoch 1 Batch 96/347] loss=0.2204, lr=0.0000050, acc=0.908\n",
      "[Epoch 1 Batch 100/347] loss=0.3039, lr=0.0000050, acc=0.907\n",
      "[Epoch 1 Batch 104/347] loss=0.3366, lr=0.0000050, acc=0.905\n",
      "[Epoch 1 Batch 108/347] loss=0.2267, lr=0.0000050, acc=0.907\n",
      "[Epoch 1 Batch 112/347] loss=0.2635, lr=0.0000050, acc=0.906\n",
      "[Epoch 1 Batch 116/347] loss=0.2893, lr=0.0000050, acc=0.905\n",
      "[Epoch 1 Batch 120/347] loss=0.2795, lr=0.0000050, acc=0.904\n",
      "[Epoch 1 Batch 124/347] loss=0.2720, lr=0.0000050, acc=0.904\n",
      "[Epoch 1 Batch 128/347] loss=0.3348, lr=0.0000050, acc=0.903\n",
      "[Epoch 1 Batch 132/347] loss=0.2587, lr=0.0000050, acc=0.903\n",
      "[Epoch 1 Batch 136/347] loss=0.2957, lr=0.0000050, acc=0.903\n",
      "[Epoch 1 Batch 140/347] loss=0.3458, lr=0.0000050, acc=0.901\n",
      "[Epoch 1 Batch 144/347] loss=0.2370, lr=0.0000050, acc=0.902\n",
      "[Epoch 1 Batch 148/347] loss=0.2531, lr=0.0000050, acc=0.902\n",
      "[Epoch 1 Batch 152/347] loss=0.2703, lr=0.0000050, acc=0.901\n",
      "[Epoch 1 Batch 156/347] loss=0.2722, lr=0.0000050, acc=0.901\n",
      "[Epoch 1 Batch 160/347] loss=0.2197, lr=0.0000050, acc=0.901\n",
      "[Epoch 1 Batch 164/347] loss=0.3231, lr=0.0000050, acc=0.902\n",
      "[Epoch 1 Batch 168/347] loss=0.2831, lr=0.0000050, acc=0.902\n",
      "[Epoch 1 Batch 172/347] loss=0.4112, lr=0.0000050, acc=0.901\n",
      "[Epoch 1 Batch 176/347] loss=0.2451, lr=0.0000050, acc=0.901\n",
      "[Epoch 1 Batch 180/347] loss=0.2969, lr=0.0000050, acc=0.900\n",
      "[Epoch 1 Batch 184/347] loss=0.4863, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 188/347] loss=0.3150, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 192/347] loss=0.2915, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 196/347] loss=0.2794, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 200/347] loss=0.1752, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 204/347] loss=0.2144, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 208/347] loss=0.3165, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 212/347] loss=0.3850, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 216/347] loss=0.3407, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 220/347] loss=0.2607, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 224/347] loss=0.3039, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 228/347] loss=0.2632, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 232/347] loss=0.2048, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 236/347] loss=0.2784, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 240/347] loss=0.3100, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 244/347] loss=0.0925, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 248/347] loss=0.2837, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 252/347] loss=0.3371, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 256/347] loss=0.1794, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 260/347] loss=0.1250, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 264/347] loss=0.2397, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 268/347] loss=0.2446, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 272/347] loss=0.2645, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 276/347] loss=0.1965, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 280/347] loss=0.3190, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 284/347] loss=0.3416, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 288/347] loss=0.2888, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 292/347] loss=0.3497, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 296/347] loss=0.3336, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 300/347] loss=0.2700, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 304/347] loss=0.2916, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 308/347] loss=0.5176, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 312/347] loss=0.2693, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 316/347] loss=0.2492, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 320/347] loss=0.1888, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 324/347] loss=0.1188, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 328/347] loss=0.1829, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 332/347] loss=0.3577, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 336/347] loss=0.2658, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 340/347] loss=0.3041, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 344/347] loss=0.3279, lr=0.0000050, acc=0.896\n",
      "Test F1: 0.5, Accuracy Score: 0.8922068463219228\n",
      "[Epoch 2 Batch 4/347] loss=0.2145, lr=0.0000050, acc=0.891\n",
      "[Epoch 2 Batch 8/347] loss=0.2207, lr=0.0000050, acc=0.898\n",
      "[Epoch 2 Batch 12/347] loss=0.2029, lr=0.0000050, acc=0.906\n",
      "[Epoch 2 Batch 16/347] loss=0.3119, lr=0.0000050, acc=0.895\n",
      "[Epoch 2 Batch 20/347] loss=0.2399, lr=0.0000050, acc=0.903\n",
      "[Epoch 2 Batch 24/347] loss=0.1876, lr=0.0000050, acc=0.909\n",
      "[Epoch 2 Batch 28/347] loss=0.2791, lr=0.0000050, acc=0.904\n",
      "[Epoch 2 Batch 32/347] loss=0.1825, lr=0.0000050, acc=0.906\n",
      "[Epoch 2 Batch 36/347] loss=0.1319, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 40/347] loss=0.3077, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 44/347] loss=0.2036, lr=0.0000050, acc=0.908\n",
      "[Epoch 2 Batch 48/347] loss=0.1309, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 52/347] loss=0.1902, lr=0.0000050, acc=0.907\n",
      "[Epoch 2 Batch 56/347] loss=0.1773, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 60/347] loss=0.2043, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 64/347] loss=0.1973, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 68/347] loss=0.1394, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 72/347] loss=0.2662, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 76/347] loss=0.2926, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 80/347] loss=0.2212, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 84/347] loss=0.2299, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 88/347] loss=0.0927, lr=0.0000050, acc=0.914\n",
      "[Epoch 2 Batch 92/347] loss=0.2540, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 96/347] loss=0.2843, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 100/347] loss=0.0469, lr=0.0000050, acc=0.917\n",
      "[Epoch 2 Batch 104/347] loss=0.1050, lr=0.0000050, acc=0.919\n",
      "[Epoch 2 Batch 108/347] loss=0.2709, lr=0.0000050, acc=0.918\n",
      "[Epoch 2 Batch 112/347] loss=0.4142, lr=0.0000050, acc=0.916\n",
      "[Epoch 2 Batch 116/347] loss=0.3492, lr=0.0000050, acc=0.915\n",
      "[Epoch 2 Batch 120/347] loss=0.2408, lr=0.0000050, acc=0.915\n",
      "[Epoch 2 Batch 124/347] loss=0.1584, lr=0.0000050, acc=0.916\n",
      "[Epoch 2 Batch 128/347] loss=0.1839, lr=0.0000050, acc=0.916\n",
      "[Epoch 2 Batch 132/347] loss=0.2149, lr=0.0000050, acc=0.916\n",
      "[Epoch 2 Batch 136/347] loss=0.2984, lr=0.0000050, acc=0.915\n",
      "[Epoch 2 Batch 140/347] loss=0.2978, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 144/347] loss=0.3427, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 148/347] loss=0.2093, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 152/347] loss=0.1760, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 156/347] loss=0.1405, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 160/347] loss=0.1441, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 164/347] loss=0.3367, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 168/347] loss=0.2981, lr=0.0000050, acc=0.909\n",
      "[Epoch 2 Batch 172/347] loss=0.2074, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 176/347] loss=0.1473, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 180/347] loss=0.2717, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 184/347] loss=0.2173, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 188/347] loss=0.1963, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 192/347] loss=0.2569, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 196/347] loss=0.2561, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 200/347] loss=0.4418, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 204/347] loss=0.2123, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 208/347] loss=0.1681, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 212/347] loss=0.2117, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 216/347] loss=0.1556, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 220/347] loss=0.2961, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 224/347] loss=0.2153, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 228/347] loss=0.3228, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 232/347] loss=0.2628, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 236/347] loss=0.1602, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 240/347] loss=0.1161, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 244/347] loss=0.2626, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 248/347] loss=0.1278, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 252/347] loss=0.5290, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 256/347] loss=0.2833, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 260/347] loss=0.2117, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 264/347] loss=0.3047, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 268/347] loss=0.1297, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 272/347] loss=0.1125, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 276/347] loss=0.2497, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 280/347] loss=0.1876, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 284/347] loss=0.3502, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 288/347] loss=0.2885, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 292/347] loss=0.3168, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 296/347] loss=0.2144, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 300/347] loss=0.1059, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 304/347] loss=0.2361, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 308/347] loss=0.3221, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 312/347] loss=0.1632, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 316/347] loss=0.2850, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 320/347] loss=0.2234, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 324/347] loss=0.1400, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 328/347] loss=0.1844, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 332/347] loss=0.1628, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 336/347] loss=0.3774, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 340/347] loss=0.3567, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 344/347] loss=0.4227, lr=0.0000050, acc=0.910\n",
      "Test F1: 0.5175718849840256, Accuracy Score: 0.8900218499635834\n",
      "[Epoch 3 Batch 4/347] loss=0.2907, lr=0.0000050, acc=0.891\n",
      "[Epoch 3 Batch 8/347] loss=0.0878, lr=0.0000050, acc=0.922\n",
      "[Epoch 3 Batch 12/347] loss=0.1586, lr=0.0000050, acc=0.922\n",
      "[Epoch 3 Batch 16/347] loss=0.0515, lr=0.0000050, acc=0.938\n",
      "[Epoch 3 Batch 20/347] loss=0.1036, lr=0.0000050, acc=0.941\n",
      "[Epoch 3 Batch 24/347] loss=0.1360, lr=0.0000050, acc=0.940\n",
      "[Epoch 3 Batch 28/347] loss=0.2177, lr=0.0000050, acc=0.935\n",
      "[Epoch 3 Batch 32/347] loss=0.1148, lr=0.0000050, acc=0.937\n",
      "[Epoch 3 Batch 36/347] loss=0.1638, lr=0.0000050, acc=0.938\n",
      "[Epoch 3 Batch 40/347] loss=0.1833, lr=0.0000050, acc=0.937\n",
      "[Epoch 3 Batch 44/347] loss=0.2686, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 48/347] loss=0.2847, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 52/347] loss=0.3189, lr=0.0000050, acc=0.920\n",
      "[Epoch 3 Batch 56/347] loss=0.1348, lr=0.0000050, acc=0.920\n",
      "[Epoch 3 Batch 60/347] loss=0.2197, lr=0.0000050, acc=0.919\n",
      "[Epoch 3 Batch 64/347] loss=0.1270, lr=0.0000050, acc=0.921\n",
      "[Epoch 3 Batch 68/347] loss=0.0938, lr=0.0000050, acc=0.923\n",
      "[Epoch 3 Batch 72/347] loss=0.0760, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 76/347] loss=0.1635, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 80/347] loss=0.2260, lr=0.0000050, acc=0.924\n",
      "[Epoch 3 Batch 84/347] loss=0.3872, lr=0.0000050, acc=0.922\n",
      "[Epoch 3 Batch 88/347] loss=0.0935, lr=0.0000050, acc=0.923\n",
      "[Epoch 3 Batch 92/347] loss=0.1327, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 96/347] loss=0.1553, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 100/347] loss=0.1806, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 104/347] loss=0.2638, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 108/347] loss=0.0852, lr=0.0000050, acc=0.928\n",
      "[Epoch 3 Batch 112/347] loss=0.2926, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 116/347] loss=0.1218, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 120/347] loss=0.2397, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 124/347] loss=0.2619, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 128/347] loss=0.1993, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 132/347] loss=0.1175, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 136/347] loss=0.1965, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 140/347] loss=0.0734, lr=0.0000050, acc=0.928\n",
      "[Epoch 3 Batch 144/347] loss=0.3353, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 148/347] loss=0.1412, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 152/347] loss=0.1218, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 156/347] loss=0.1142, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 160/347] loss=0.2440, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 164/347] loss=0.3808, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 168/347] loss=0.1576, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 172/347] loss=0.2437, lr=0.0000050, acc=0.924\n",
      "[Epoch 3 Batch 176/347] loss=0.1053, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 180/347] loss=0.2676, lr=0.0000050, acc=0.924\n",
      "[Epoch 3 Batch 184/347] loss=0.0611, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 188/347] loss=0.1092, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 192/347] loss=0.1179, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 196/347] loss=0.3165, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 200/347] loss=0.1401, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 204/347] loss=0.2999, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 208/347] loss=0.1376, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 212/347] loss=0.1198, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 216/347] loss=0.1602, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 220/347] loss=0.2262, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 224/347] loss=0.1287, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 228/347] loss=0.1159, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 232/347] loss=0.2944, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 236/347] loss=0.2828, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 240/347] loss=0.2332, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 244/347] loss=0.0647, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 248/347] loss=0.2406, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 252/347] loss=0.1902, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 256/347] loss=0.1980, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 260/347] loss=0.1171, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 264/347] loss=0.1225, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 268/347] loss=0.0852, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 272/347] loss=0.0567, lr=0.0000050, acc=0.928\n",
      "[Epoch 3 Batch 276/347] loss=0.3639, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 280/347] loss=0.2007, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 284/347] loss=0.1840, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 288/347] loss=0.2120, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 292/347] loss=0.1560, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 296/347] loss=0.1241, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 300/347] loss=0.1888, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 304/347] loss=0.1929, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 308/347] loss=0.1382, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 312/347] loss=0.1724, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 316/347] loss=0.1223, lr=0.0000050, acc=0.928\n",
      "[Epoch 3 Batch 320/347] loss=0.3305, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 324/347] loss=0.2561, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 328/347] loss=0.3339, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 332/347] loss=0.0885, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 336/347] loss=0.1036, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 340/347] loss=0.1135, lr=0.0000050, acc=0.926\n",
      "[Epoch 3 Batch 344/347] loss=0.3435, lr=0.0000050, acc=0.926\n",
      "Test F1: 0.4761904761904762, Accuracy Score: 0.8958485069191552\n",
      "[Epoch 4 Batch 4/347] loss=0.0519, lr=0.0000050, acc=0.969\n",
      "[Epoch 4 Batch 8/347] loss=0.1129, lr=0.0000050, acc=0.961\n",
      "[Epoch 4 Batch 12/347] loss=0.2625, lr=0.0000050, acc=0.938\n",
      "[Epoch 4 Batch 16/347] loss=0.2050, lr=0.0000050, acc=0.934\n",
      "[Epoch 4 Batch 20/347] loss=0.1874, lr=0.0000050, acc=0.932\n",
      "[Epoch 4 Batch 24/347] loss=0.1192, lr=0.0000050, acc=0.938\n",
      "[Epoch 4 Batch 28/347] loss=0.0806, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 32/347] loss=0.1789, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 36/347] loss=0.0321, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 40/347] loss=0.0653, lr=0.0000050, acc=0.947\n",
      "[Epoch 4 Batch 44/347] loss=0.1466, lr=0.0000050, acc=0.947\n",
      "[Epoch 4 Batch 48/347] loss=0.1702, lr=0.0000050, acc=0.946\n",
      "[Epoch 4 Batch 52/347] loss=0.2041, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 56/347] loss=0.1097, lr=0.0000050, acc=0.947\n",
      "[Epoch 4 Batch 60/347] loss=0.1365, lr=0.0000050, acc=0.949\n",
      "[Epoch 4 Batch 64/347] loss=0.1591, lr=0.0000050, acc=0.946\n",
      "[Epoch 4 Batch 68/347] loss=0.1177, lr=0.0000050, acc=0.944\n",
      "[Epoch 4 Batch 72/347] loss=0.0591, lr=0.0000050, acc=0.946\n",
      "[Epoch 4 Batch 76/347] loss=0.0995, lr=0.0000050, acc=0.948\n",
      "[Epoch 4 Batch 80/347] loss=0.2402, lr=0.0000050, acc=0.946\n",
      "[Epoch 4 Batch 84/347] loss=0.1906, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 88/347] loss=0.1533, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 92/347] loss=0.1370, lr=0.0000050, acc=0.944\n",
      "[Epoch 4 Batch 96/347] loss=0.1389, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 100/347] loss=0.1289, lr=0.0000050, acc=0.946\n",
      "[Epoch 4 Batch 104/347] loss=0.1481, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 108/347] loss=0.1168, lr=0.0000050, acc=0.946\n",
      "[Epoch 4 Batch 112/347] loss=0.0299, lr=0.0000050, acc=0.947\n",
      "[Epoch 4 Batch 116/347] loss=0.2564, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 120/347] loss=0.1142, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 124/347] loss=0.0816, lr=0.0000050, acc=0.946\n",
      "[Epoch 4 Batch 128/347] loss=0.1621, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 132/347] loss=0.0781, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 136/347] loss=0.1064, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 140/347] loss=0.1635, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 144/347] loss=0.1140, lr=0.0000050, acc=0.945\n",
      "[Epoch 4 Batch 148/347] loss=0.1635, lr=0.0000050, acc=0.944\n",
      "[Epoch 4 Batch 152/347] loss=0.2269, lr=0.0000050, acc=0.944\n",
      "[Epoch 4 Batch 156/347] loss=0.1273, lr=0.0000050, acc=0.944\n",
      "[Epoch 4 Batch 160/347] loss=0.2103, lr=0.0000050, acc=0.944\n",
      "[Epoch 4 Batch 164/347] loss=0.3202, lr=0.0000050, acc=0.942\n",
      "[Epoch 4 Batch 168/347] loss=0.1688, lr=0.0000050, acc=0.941\n",
      "[Epoch 4 Batch 172/347] loss=0.1483, lr=0.0000050, acc=0.941\n",
      "[Epoch 4 Batch 176/347] loss=0.2027, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 180/347] loss=0.1811, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 184/347] loss=0.1999, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 188/347] loss=0.0669, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 192/347] loss=0.1307, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 196/347] loss=0.2183, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 200/347] loss=0.1140, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 204/347] loss=0.1459, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 208/347] loss=0.2393, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 212/347] loss=0.1009, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 216/347] loss=0.1659, lr=0.0000050, acc=0.938\n",
      "[Epoch 4 Batch 220/347] loss=0.1460, lr=0.0000050, acc=0.938\n",
      "[Epoch 4 Batch 224/347] loss=0.1127, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 228/347] loss=0.1237, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 232/347] loss=0.1396, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 236/347] loss=0.2055, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 240/347] loss=0.2716, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 244/347] loss=0.2011, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 248/347] loss=0.1182, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 252/347] loss=0.0970, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 256/347] loss=0.1016, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 260/347] loss=0.0587, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 264/347] loss=0.0601, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 268/347] loss=0.1319, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 272/347] loss=0.1933, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 276/347] loss=0.2029, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 280/347] loss=0.1723, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 284/347] loss=0.0960, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 288/347] loss=0.0984, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 292/347] loss=0.2309, lr=0.0000050, acc=0.939\n",
      "[Epoch 4 Batch 296/347] loss=0.0527, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 300/347] loss=0.0993, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 304/347] loss=0.2547, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 308/347] loss=0.1247, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 312/347] loss=0.1725, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 316/347] loss=0.1160, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 320/347] loss=0.1878, lr=0.0000050, acc=0.941\n",
      "[Epoch 4 Batch 324/347] loss=0.2324, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 328/347] loss=0.1379, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 332/347] loss=0.1819, lr=0.0000050, acc=0.940\n",
      "[Epoch 4 Batch 336/347] loss=0.1391, lr=0.0000050, acc=0.941\n",
      "[Epoch 4 Batch 340/347] loss=0.0398, lr=0.0000050, acc=0.941\n",
      "[Epoch 4 Batch 344/347] loss=0.2142, lr=0.0000050, acc=0.941\n",
      "Test F1: 0.5647058823529412, Accuracy Score: 0.8922068463219228\n",
      "[Epoch 5 Batch 4/347] loss=0.0745, lr=0.0000050, acc=0.984\n",
      "[Epoch 5 Batch 8/347] loss=0.1949, lr=0.0000050, acc=0.969\n",
      "[Epoch 5 Batch 12/347] loss=0.0957, lr=0.0000050, acc=0.969\n",
      "[Epoch 5 Batch 16/347] loss=0.0274, lr=0.0000050, acc=0.977\n",
      "[Epoch 5 Batch 20/347] loss=0.1420, lr=0.0000050, acc=0.968\n",
      "[Epoch 5 Batch 24/347] loss=0.1321, lr=0.0000050, acc=0.966\n",
      "[Epoch 5 Batch 28/347] loss=0.0693, lr=0.0000050, acc=0.966\n",
      "[Epoch 5 Batch 32/347] loss=0.0912, lr=0.0000050, acc=0.968\n",
      "[Epoch 5 Batch 36/347] loss=0.0224, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 40/347] loss=0.1818, lr=0.0000050, acc=0.966\n",
      "[Epoch 5 Batch 44/347] loss=0.1399, lr=0.0000050, acc=0.965\n",
      "[Epoch 5 Batch 48/347] loss=0.1355, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 52/347] loss=0.0832, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 56/347] loss=0.1754, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 60/347] loss=0.0750, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 64/347] loss=0.0569, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 68/347] loss=0.0780, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 72/347] loss=0.1632, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 76/347] loss=0.0796, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 80/347] loss=0.0433, lr=0.0000050, acc=0.964\n",
      "[Epoch 5 Batch 84/347] loss=0.0645, lr=0.0000050, acc=0.965\n",
      "[Epoch 5 Batch 88/347] loss=0.1257, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 92/347] loss=0.0778, lr=0.0000050, acc=0.964\n",
      "[Epoch 5 Batch 96/347] loss=0.2529, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 100/347] loss=0.0654, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 104/347] loss=0.0674, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 108/347] loss=0.1020, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 112/347] loss=0.0965, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 116/347] loss=0.2046, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 120/347] loss=0.1411, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 124/347] loss=0.0577, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 128/347] loss=0.1231, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 132/347] loss=0.0761, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 136/347] loss=0.2348, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 140/347] loss=0.0706, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 144/347] loss=0.1233, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 148/347] loss=0.1508, lr=0.0000050, acc=0.960\n",
      "[Epoch 5 Batch 152/347] loss=0.1666, lr=0.0000050, acc=0.959\n",
      "[Epoch 5 Batch 156/347] loss=0.0899, lr=0.0000050, acc=0.960\n",
      "[Epoch 5 Batch 160/347] loss=0.0749, lr=0.0000050, acc=0.960\n",
      "[Epoch 5 Batch 164/347] loss=0.0135, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 168/347] loss=0.0356, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 172/347] loss=0.0456, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 176/347] loss=0.0539, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 180/347] loss=0.2279, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 184/347] loss=0.0876, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 188/347] loss=0.0384, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 192/347] loss=0.1293, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 196/347] loss=0.2179, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 200/347] loss=0.1417, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 204/347] loss=0.0162, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 208/347] loss=0.0314, lr=0.0000050, acc=0.964\n",
      "[Epoch 5 Batch 212/347] loss=0.0229, lr=0.0000050, acc=0.964\n",
      "[Epoch 5 Batch 216/347] loss=0.0538, lr=0.0000050, acc=0.964\n",
      "[Epoch 5 Batch 220/347] loss=0.1210, lr=0.0000050, acc=0.964\n",
      "[Epoch 5 Batch 224/347] loss=0.1823, lr=0.0000050, acc=0.964\n",
      "[Epoch 5 Batch 228/347] loss=0.1343, lr=0.0000050, acc=0.964\n",
      "[Epoch 5 Batch 232/347] loss=0.2180, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 236/347] loss=0.0851, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 240/347] loss=0.1381, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 244/347] loss=0.1129, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 248/347] loss=0.0594, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 252/347] loss=0.1240, lr=0.0000050, acc=0.963\n",
      "[Epoch 5 Batch 256/347] loss=0.1570, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 260/347] loss=0.1887, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 264/347] loss=0.1224, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 268/347] loss=0.0183, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 272/347] loss=0.1602, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 276/347] loss=0.0066, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 280/347] loss=0.2155, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 284/347] loss=0.0942, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 288/347] loss=0.1529, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 292/347] loss=0.2794, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 296/347] loss=0.0490, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 300/347] loss=0.0748, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 304/347] loss=0.1018, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 308/347] loss=0.1222, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 312/347] loss=0.1051, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 316/347] loss=0.0670, lr=0.0000050, acc=0.962\n",
      "[Epoch 5 Batch 320/347] loss=0.1451, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 324/347] loss=0.0669, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 328/347] loss=0.1192, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 332/347] loss=0.0945, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 336/347] loss=0.2342, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 340/347] loss=0.0544, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 344/347] loss=0.0972, lr=0.0000050, acc=0.961\n",
      "Test F1: 0.5565749235474006, Accuracy Score: 0.8943918426802622\n",
      "[Epoch 6 Batch 4/347] loss=0.0678, lr=0.0000050, acc=0.969\n",
      "[Epoch 6 Batch 8/347] loss=0.1612, lr=0.0000050, acc=0.961\n",
      "[Epoch 6 Batch 12/347] loss=0.0155, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 16/347] loss=0.0045, lr=0.0000050, acc=0.980\n",
      "[Epoch 6 Batch 20/347] loss=0.1300, lr=0.0000050, acc=0.978\n",
      "[Epoch 6 Batch 24/347] loss=0.0183, lr=0.0000050, acc=0.982\n",
      "[Epoch 6 Batch 28/347] loss=0.0402, lr=0.0000050, acc=0.982\n",
      "[Epoch 6 Batch 32/347] loss=0.0402, lr=0.0000050, acc=0.982\n",
      "[Epoch 6 Batch 36/347] loss=0.0144, lr=0.0000050, acc=0.984\n",
      "[Epoch 6 Batch 40/347] loss=0.0234, lr=0.0000050, acc=0.986\n",
      "[Epoch 6 Batch 44/347] loss=0.0531, lr=0.0000050, acc=0.986\n",
      "[Epoch 6 Batch 48/347] loss=0.1502, lr=0.0000050, acc=0.984\n",
      "[Epoch 6 Batch 52/347] loss=0.1430, lr=0.0000050, acc=0.981\n",
      "[Epoch 6 Batch 56/347] loss=0.0956, lr=0.0000050, acc=0.979\n",
      "[Epoch 6 Batch 60/347] loss=0.0505, lr=0.0000050, acc=0.979\n",
      "[Epoch 6 Batch 64/347] loss=0.0531, lr=0.0000050, acc=0.979\n",
      "[Epoch 6 Batch 68/347] loss=0.1055, lr=0.0000050, acc=0.979\n",
      "[Epoch 6 Batch 72/347] loss=0.1604, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 76/347] loss=0.0418, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 80/347] loss=0.1417, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 84/347] loss=0.0262, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 88/347] loss=0.0747, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 92/347] loss=0.0797, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 96/347] loss=0.0114, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 100/347] loss=0.1097, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 104/347] loss=0.0264, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 108/347] loss=0.0176, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 112/347] loss=0.1995, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 116/347] loss=0.1493, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 120/347] loss=0.0500, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 124/347] loss=0.0243, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 128/347] loss=0.2169, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 132/347] loss=0.0474, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 136/347] loss=0.0260, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 140/347] loss=0.1941, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 144/347] loss=0.0467, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 148/347] loss=0.0400, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 152/347] loss=0.0091, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 156/347] loss=0.0604, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 160/347] loss=0.0527, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 164/347] loss=0.1229, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 168/347] loss=0.0438, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 172/347] loss=0.0094, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 176/347] loss=0.0173, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 180/347] loss=0.0119, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 184/347] loss=0.0101, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 188/347] loss=0.0471, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 192/347] loss=0.0238, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 196/347] loss=0.0733, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 200/347] loss=0.0720, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 204/347] loss=0.0579, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 208/347] loss=0.1472, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 212/347] loss=0.0415, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 216/347] loss=0.2588, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 220/347] loss=0.0636, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 224/347] loss=0.1200, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 228/347] loss=0.0882, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 232/347] loss=0.0668, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 236/347] loss=0.2139, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 240/347] loss=0.1215, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 244/347] loss=0.0824, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 248/347] loss=0.0198, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 252/347] loss=0.0617, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 256/347] loss=0.0398, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 260/347] loss=0.0115, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 264/347] loss=0.0212, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 268/347] loss=0.1714, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 272/347] loss=0.1823, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 276/347] loss=0.1206, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 280/347] loss=0.0215, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 284/347] loss=0.0100, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 288/347] loss=0.0084, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 292/347] loss=0.0442, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 296/347] loss=0.0033, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 300/347] loss=0.0931, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 304/347] loss=0.2061, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 308/347] loss=0.1096, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 312/347] loss=0.1703, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 316/347] loss=0.1272, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 320/347] loss=0.0778, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 324/347] loss=0.2445, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 328/347] loss=0.2545, lr=0.0000050, acc=0.972\n",
      "[Epoch 6 Batch 332/347] loss=0.0827, lr=0.0000050, acc=0.972\n",
      "[Epoch 6 Batch 336/347] loss=0.0740, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 340/347] loss=0.0710, lr=0.0000050, acc=0.972\n",
      "[Epoch 6 Batch 344/347] loss=0.0285, lr=0.0000050, acc=0.973\n",
      "Test F1: 0.5592705167173252, Accuracy Score: 0.8943918426802622\n",
      "[Epoch 7 Batch 4/347] loss=0.0097, lr=0.0000050, acc=1.000\n",
      "[Epoch 7 Batch 8/347] loss=0.0752, lr=0.0000050, acc=0.975\n",
      "[Epoch 7 Batch 12/347] loss=0.0113, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 16/347] loss=0.0968, lr=0.0000050, acc=0.979\n",
      "[Epoch 7 Batch 20/347] loss=0.0985, lr=0.0000050, acc=0.977\n",
      "[Epoch 7 Batch 24/347] loss=0.0567, lr=0.0000050, acc=0.978\n",
      "[Epoch 7 Batch 28/347] loss=0.0178, lr=0.0000050, acc=0.979\n",
      "[Epoch 7 Batch 32/347] loss=0.0028, lr=0.0000050, acc=0.982\n",
      "[Epoch 7 Batch 36/347] loss=0.1273, lr=0.0000050, acc=0.982\n",
      "[Epoch 7 Batch 40/347] loss=0.0224, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 44/347] loss=0.0575, lr=0.0000050, acc=0.982\n",
      "[Epoch 7 Batch 48/347] loss=0.0791, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 52/347] loss=0.0140, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 56/347] loss=0.0088, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 60/347] loss=0.0161, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 64/347] loss=0.1087, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 68/347] loss=0.3063, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 72/347] loss=0.0049, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 76/347] loss=0.0770, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 80/347] loss=0.0691, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 84/347] loss=0.0092, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 88/347] loss=0.0050, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 92/347] loss=0.0909, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 96/347] loss=0.0398, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 100/347] loss=0.0582, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 104/347] loss=0.1093, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 108/347] loss=0.1733, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 112/347] loss=0.1258, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 116/347] loss=0.0012, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 120/347] loss=0.0063, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 124/347] loss=0.0610, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 128/347] loss=0.0602, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 132/347] loss=0.0042, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 136/347] loss=0.0627, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 140/347] loss=0.1596, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 144/347] loss=0.0629, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 148/347] loss=0.0600, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 152/347] loss=0.0068, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 156/347] loss=0.0118, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 160/347] loss=0.0343, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 164/347] loss=0.0012, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 168/347] loss=0.1211, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 172/347] loss=0.1937, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 176/347] loss=0.1079, lr=0.0000050, acc=0.982\n",
      "[Epoch 7 Batch 180/347] loss=0.0072, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 184/347] loss=0.0735, lr=0.0000050, acc=0.982\n",
      "[Epoch 7 Batch 188/347] loss=0.1315, lr=0.0000050, acc=0.982\n",
      "[Epoch 7 Batch 192/347] loss=0.0085, lr=0.0000050, acc=0.982\n",
      "[Epoch 7 Batch 196/347] loss=0.0193, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 200/347] loss=0.0162, lr=0.0000050, acc=0.983\n",
      "[Epoch 7 Batch 204/347] loss=0.0125, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 208/347] loss=0.0007, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 212/347] loss=0.0365, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 216/347] loss=0.0841, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 220/347] loss=0.0304, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 224/347] loss=0.0843, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 228/347] loss=0.0670, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 232/347] loss=0.0171, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 236/347] loss=0.0051, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 240/347] loss=0.0065, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 244/347] loss=0.0651, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 248/347] loss=0.0025, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 252/347] loss=0.2165, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 256/347] loss=0.0075, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 260/347] loss=0.1471, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 264/347] loss=0.0016, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 268/347] loss=0.0230, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 272/347] loss=0.0012, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 276/347] loss=0.1071, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 280/347] loss=0.1068, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 284/347] loss=0.0861, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 288/347] loss=0.1230, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 292/347] loss=0.0058, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 296/347] loss=0.0117, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 300/347] loss=0.0244, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 304/347] loss=0.0568, lr=0.0000050, acc=0.984\n",
      "[Epoch 7 Batch 308/347] loss=0.0103, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 312/347] loss=0.1210, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 316/347] loss=0.0166, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 320/347] loss=0.0649, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 324/347] loss=0.0234, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 328/347] loss=0.1293, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 332/347] loss=0.0751, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 336/347] loss=0.0035, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 340/347] loss=0.0436, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 344/347] loss=0.0012, lr=0.0000050, acc=0.985\n",
      "Test F1: 0.5527950310559006, Accuracy Score: 0.8951201747997086\n",
      "[Epoch 8 Batch 4/347] loss=0.0068, lr=0.0000050, acc=1.000\n",
      "[Epoch 8 Batch 8/347] loss=0.0016, lr=0.0000050, acc=1.000\n",
      "[Epoch 8 Batch 12/347] loss=0.0035, lr=0.0000050, acc=1.000\n",
      "[Epoch 8 Batch 16/347] loss=0.0764, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 20/347] loss=0.0294, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 24/347] loss=0.0029, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 28/347] loss=0.0159, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 32/347] loss=0.0014, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 36/347] loss=0.0492, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 40/347] loss=0.0596, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 44/347] loss=0.0961, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 48/347] loss=0.0817, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 52/347] loss=0.0132, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 56/347] loss=0.0016, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 60/347] loss=0.0042, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 64/347] loss=0.0150, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 68/347] loss=0.0072, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 72/347] loss=0.0245, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 76/347] loss=0.0289, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 80/347] loss=0.0917, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 84/347] loss=0.0698, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 88/347] loss=0.0060, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 92/347] loss=0.0019, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 96/347] loss=0.0031, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 100/347] loss=0.0482, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 104/347] loss=0.0997, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 108/347] loss=0.0009, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 112/347] loss=0.0219, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 116/347] loss=0.0015, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 120/347] loss=0.0100, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 124/347] loss=0.0122, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 128/347] loss=0.0020, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 132/347] loss=0.0098, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 136/347] loss=0.0038, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 140/347] loss=0.0110, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 144/347] loss=0.0025, lr=0.0000050, acc=0.993\n",
      "[Epoch 8 Batch 148/347] loss=0.0152, lr=0.0000050, acc=0.992\n",
      "[Epoch 8 Batch 152/347] loss=0.2231, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 156/347] loss=0.1035, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 160/347] loss=0.0028, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 164/347] loss=0.0012, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 168/347] loss=0.0093, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 172/347] loss=0.1576, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 176/347] loss=0.0012, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 180/347] loss=0.0114, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 184/347] loss=0.2122, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 188/347] loss=0.0014, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 192/347] loss=0.1072, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 196/347] loss=0.0046, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 200/347] loss=0.0438, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 204/347] loss=0.0014, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 208/347] loss=0.1828, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 212/347] loss=0.0030, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 216/347] loss=0.0046, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 220/347] loss=0.1184, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 224/347] loss=0.0012, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 228/347] loss=0.0087, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 232/347] loss=0.0524, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 236/347] loss=0.1128, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 240/347] loss=0.0471, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 244/347] loss=0.0119, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 248/347] loss=0.0073, lr=0.0000050, acc=0.991\n",
      "[Epoch 8 Batch 252/347] loss=0.0665, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 256/347] loss=0.0013, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 260/347] loss=0.0372, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 264/347] loss=0.0676, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 268/347] loss=0.0773, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 272/347] loss=0.0116, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 276/347] loss=0.0278, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 280/347] loss=0.0013, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 284/347] loss=0.0008, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 288/347] loss=0.0027, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 292/347] loss=0.0240, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 296/347] loss=0.0035, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 300/347] loss=0.0143, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 304/347] loss=0.0184, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 308/347] loss=0.0701, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 312/347] loss=0.0022, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 316/347] loss=0.0059, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 320/347] loss=0.0248, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 324/347] loss=0.0007, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 328/347] loss=0.0245, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 332/347] loss=0.0005, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 336/347] loss=0.0668, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 340/347] loss=0.0585, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 344/347] loss=0.0979, lr=0.0000050, acc=0.990\n",
      "Test F1: 0.5706371191135733, Accuracy Score: 0.8871085214857976\n",
      "[Epoch 9 Batch 4/347] loss=0.0019, lr=0.0000050, acc=1.000\n",
      "[Epoch 9 Batch 8/347] loss=0.0084, lr=0.0000050, acc=1.000\n",
      "[Epoch 9 Batch 12/347] loss=0.0105, lr=0.0000050, acc=1.000\n",
      "[Epoch 9 Batch 16/347] loss=0.0133, lr=0.0000050, acc=1.000\n",
      "[Epoch 9 Batch 20/347] loss=0.0940, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 24/347] loss=0.0014, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 28/347] loss=0.0010, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 32/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 36/347] loss=0.0173, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 40/347] loss=0.0010, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 44/347] loss=0.0030, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 48/347] loss=0.0050, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 52/347] loss=0.0065, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 56/347] loss=0.0005, lr=0.0000050, acc=0.997\n",
      "[Epoch 9 Batch 60/347] loss=0.0164, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 64/347] loss=0.0193, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 68/347] loss=0.0009, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 72/347] loss=0.0083, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 76/347] loss=0.0017, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 80/347] loss=0.0010, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 84/347] loss=0.0005, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 88/347] loss=0.0764, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 92/347] loss=0.0084, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 96/347] loss=0.0005, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 100/347] loss=0.0008, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 104/347] loss=0.0396, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 108/347] loss=0.0014, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 112/347] loss=0.0006, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 116/347] loss=0.0285, lr=0.0000050, acc=0.996\n",
      "[Epoch 9 Batch 120/347] loss=0.0970, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 124/347] loss=0.0014, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 128/347] loss=0.0029, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 132/347] loss=0.0006, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 136/347] loss=0.0005, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 140/347] loss=0.1065, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 144/347] loss=0.0100, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 148/347] loss=0.0032, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 152/347] loss=0.0013, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 156/347] loss=0.0755, lr=0.0000050, acc=0.995\n",
      "[Epoch 9 Batch 160/347] loss=0.3228, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 164/347] loss=0.0011, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 168/347] loss=0.0005, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 172/347] loss=0.0037, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 176/347] loss=0.0644, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 180/347] loss=0.0008, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 184/347] loss=0.0010, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 188/347] loss=0.0622, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 192/347] loss=0.0553, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 196/347] loss=0.0009, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 200/347] loss=0.0017, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 204/347] loss=0.0468, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 208/347] loss=0.0003, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 212/347] loss=0.0872, lr=0.0000050, acc=0.993\n",
      "[Epoch 9 Batch 216/347] loss=0.0006, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 220/347] loss=0.0009, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 224/347] loss=0.0851, lr=0.0000050, acc=0.993\n",
      "[Epoch 9 Batch 228/347] loss=0.0069, lr=0.0000050, acc=0.994\n",
      "[Epoch 9 Batch 232/347] loss=0.1090, lr=0.0000050, acc=0.993\n",
      "[Epoch 9 Batch 236/347] loss=0.0043, lr=0.0000050, acc=0.993\n",
      "[Epoch 9 Batch 240/347] loss=0.0471, lr=0.0000050, acc=0.993\n",
      "[Epoch 9 Batch 244/347] loss=0.0006, lr=0.0000050, acc=0.993\n",
      "[Epoch 9 Batch 248/347] loss=0.0016, lr=0.0000050, acc=0.993\n",
      "[Epoch 9 Batch 252/347] loss=0.0377, lr=0.0000050, acc=0.993\n",
      "[Epoch 9 Batch 256/347] loss=0.0976, lr=0.0000050, acc=0.993\n",
      "[Epoch 9 Batch 260/347] loss=0.0509, lr=0.0000050, acc=0.993\n",
      "[Epoch 9 Batch 264/347] loss=0.0806, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 268/347] loss=0.0790, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 272/347] loss=0.0017, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 276/347] loss=0.0004, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 280/347] loss=0.0262, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 284/347] loss=0.0006, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 288/347] loss=0.0684, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 292/347] loss=0.0005, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 296/347] loss=0.0217, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 300/347] loss=0.0003, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 304/347] loss=0.0362, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 308/347] loss=0.0027, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 312/347] loss=0.0004, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 316/347] loss=0.0209, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 320/347] loss=0.0867, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 324/347] loss=0.0002, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 328/347] loss=0.0016, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 332/347] loss=0.0618, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 336/347] loss=0.0002, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 340/347] loss=0.0075, lr=0.0000050, acc=0.992\n",
      "[Epoch 9 Batch 344/347] loss=0.1027, lr=0.0000050, acc=0.992\n",
      "Test F1: 0.5017421602787456, Accuracy Score: 0.8958485069191552\n",
      "[Epoch 10 Batch 4/347] loss=0.0004, lr=0.0000050, acc=1.000\n",
      "[Epoch 10 Batch 8/347] loss=0.0863, lr=0.0000050, acc=0.991\n",
      "[Epoch 10 Batch 12/347] loss=0.0029, lr=0.0000050, acc=0.994\n",
      "[Epoch 10 Batch 16/347] loss=0.0095, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 20/347] loss=0.0045, lr=0.0000050, acc=0.997\n",
      "[Epoch 10 Batch 24/347] loss=0.0172, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 28/347] loss=0.0328, lr=0.0000050, acc=0.993\n",
      "[Epoch 10 Batch 32/347] loss=0.0002, lr=0.0000050, acc=0.994\n",
      "[Epoch 10 Batch 36/347] loss=0.0028, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 40/347] loss=0.0028, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 44/347] loss=0.0187, lr=0.0000050, acc=0.994\n",
      "[Epoch 10 Batch 48/347] loss=0.0093, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 52/347] loss=0.0014, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 56/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 60/347] loss=0.0034, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 64/347] loss=0.0094, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 68/347] loss=0.0008, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 72/347] loss=0.0010, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 76/347] loss=0.0147, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 80/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 84/347] loss=0.0004, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 88/347] loss=0.0012, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 92/347] loss=0.0010, lr=0.0000050, acc=0.997\n",
      "[Epoch 10 Batch 96/347] loss=0.0008, lr=0.0000050, acc=0.997\n",
      "[Epoch 10 Batch 100/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 10 Batch 104/347] loss=0.0025, lr=0.0000050, acc=0.997\n",
      "[Epoch 10 Batch 108/347] loss=0.0006, lr=0.0000050, acc=0.997\n",
      "[Epoch 10 Batch 112/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 10 Batch 116/347] loss=0.0008, lr=0.0000050, acc=0.997\n",
      "[Epoch 10 Batch 120/347] loss=0.0756, lr=0.0000050, acc=0.997\n",
      "[Epoch 10 Batch 124/347] loss=0.0505, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 128/347] loss=0.0010, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 132/347] loss=0.0015, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 136/347] loss=0.0114, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 140/347] loss=0.0010, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 144/347] loss=0.0012, lr=0.0000050, acc=0.997\n",
      "[Epoch 10 Batch 148/347] loss=0.0226, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 152/347] loss=0.0729, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 156/347] loss=0.0019, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 160/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 164/347] loss=0.0027, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 168/347] loss=0.0182, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 172/347] loss=0.0051, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 176/347] loss=0.0979, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 180/347] loss=0.0012, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 184/347] loss=0.0005, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 188/347] loss=0.0070, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 192/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 196/347] loss=0.0184, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 200/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 204/347] loss=0.0156, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 208/347] loss=0.0235, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 212/347] loss=0.0482, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 216/347] loss=0.1239, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 220/347] loss=0.0834, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 224/347] loss=0.0032, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 228/347] loss=0.0019, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 232/347] loss=0.0775, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 236/347] loss=0.0083, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 240/347] loss=0.0006, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 244/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 248/347] loss=0.0098, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 252/347] loss=0.0496, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 256/347] loss=0.0012, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 260/347] loss=0.0004, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 264/347] loss=0.0024, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 268/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 272/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 276/347] loss=0.0009, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 280/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 10 Batch 284/347] loss=0.1331, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 288/347] loss=0.1158, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 292/347] loss=0.0074, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 296/347] loss=0.0212, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 300/347] loss=0.0019, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 304/347] loss=0.0125, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 308/347] loss=0.0011, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 312/347] loss=0.1454, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 316/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 320/347] loss=0.0006, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 324/347] loss=0.0123, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 328/347] loss=0.0004, lr=0.0000050, acc=0.995\n",
      "[Epoch 10 Batch 332/347] loss=0.1488, lr=0.0000050, acc=0.994\n",
      "[Epoch 10 Batch 336/347] loss=0.0534, lr=0.0000050, acc=0.994\n",
      "[Epoch 10 Batch 340/347] loss=0.0010, lr=0.0000050, acc=0.994\n",
      "[Epoch 10 Batch 344/347] loss=0.0305, lr=0.0000050, acc=0.994\n",
      "Test F1: 0.5572755417956656, Accuracy Score: 0.8958485069191552\n",
      "[Epoch 11 Batch 4/347] loss=0.0007, lr=0.0000050, acc=1.000\n",
      "[Epoch 11 Batch 8/347] loss=0.0060, lr=0.0000050, acc=1.000\n",
      "[Epoch 11 Batch 12/347] loss=0.0004, lr=0.0000050, acc=1.000\n",
      "[Epoch 11 Batch 16/347] loss=0.0003, lr=0.0000050, acc=1.000\n",
      "[Epoch 11 Batch 20/347] loss=0.0463, lr=0.0000050, acc=0.997\n",
      "[Epoch 11 Batch 24/347] loss=0.0007, lr=0.0000050, acc=0.997\n",
      "[Epoch 11 Batch 28/347] loss=0.0004, lr=0.0000050, acc=0.998\n",
      "[Epoch 11 Batch 32/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 11 Batch 36/347] loss=0.0796, lr=0.0000050, acc=0.996\n",
      "[Epoch 11 Batch 40/347] loss=0.0665, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 44/347] loss=0.0005, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 48/347] loss=0.0035, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 52/347] loss=0.0174, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 56/347] loss=0.0003, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 60/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 64/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 68/347] loss=0.0911, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 72/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 76/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 80/347] loss=0.0006, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 84/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 88/347] loss=0.0063, lr=0.0000050, acc=0.996\n",
      "[Epoch 11 Batch 92/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 11 Batch 96/347] loss=0.0095, lr=0.0000050, acc=0.996\n",
      "[Epoch 11 Batch 100/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 11 Batch 104/347] loss=0.0387, lr=0.0000050, acc=0.996\n",
      "[Epoch 11 Batch 108/347] loss=0.0900, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 112/347] loss=0.0059, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 116/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 11 Batch 120/347] loss=0.0181, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 124/347] loss=0.0377, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 128/347] loss=0.0009, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 132/347] loss=0.0752, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 136/347] loss=0.0881, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 140/347] loss=0.0792, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 144/347] loss=0.0005, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 148/347] loss=0.0309, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 152/347] loss=0.0003, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 156/347] loss=0.1054, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 160/347] loss=0.0011, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 164/347] loss=0.0014, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 168/347] loss=0.0004, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 172/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 176/347] loss=0.0950, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 180/347] loss=0.0005, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 184/347] loss=0.0010, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 188/347] loss=0.0011, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 192/347] loss=0.0036, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 196/347] loss=0.0383, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 200/347] loss=0.0350, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 204/347] loss=0.0086, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 208/347] loss=0.0005, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 212/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 216/347] loss=0.1113, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 220/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 224/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 228/347] loss=0.0086, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 232/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 236/347] loss=0.0004, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 240/347] loss=0.0015, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 244/347] loss=0.0127, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 248/347] loss=0.0491, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 252/347] loss=0.0527, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 256/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 260/347] loss=0.1282, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 264/347] loss=0.0005, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 268/347] loss=0.0015, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 272/347] loss=0.0004, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 276/347] loss=0.0017, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 280/347] loss=0.0192, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 284/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 288/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 292/347] loss=0.0054, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 296/347] loss=0.0840, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 300/347] loss=0.0980, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 304/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 308/347] loss=0.0004, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 312/347] loss=0.0040, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 316/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 320/347] loss=0.0513, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 324/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 328/347] loss=0.0841, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 332/347] loss=0.0004, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 336/347] loss=0.0011, lr=0.0000050, acc=0.995\n",
      "[Epoch 11 Batch 340/347] loss=0.0408, lr=0.0000050, acc=0.994\n",
      "[Epoch 11 Batch 344/347] loss=0.0012, lr=0.0000050, acc=0.994\n",
      "Test F1: 0.5498489425981873, Accuracy Score: 0.8914785142024764\n",
      "[Epoch 12 Batch 4/347] loss=0.0021, lr=0.0000050, acc=1.000\n",
      "[Epoch 12 Batch 8/347] loss=0.0007, lr=0.0000050, acc=1.000\n",
      "[Epoch 12 Batch 12/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 12 Batch 16/347] loss=0.0050, lr=0.0000050, acc=1.000\n",
      "[Epoch 12 Batch 20/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 12 Batch 24/347] loss=0.0153, lr=0.0000050, acc=1.000\n",
      "[Epoch 12 Batch 28/347] loss=0.0005, lr=0.0000050, acc=1.000\n",
      "[Epoch 12 Batch 32/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 12 Batch 36/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 12 Batch 40/347] loss=0.0554, lr=0.0000050, acc=0.998\n",
      "[Epoch 12 Batch 44/347] loss=0.0004, lr=0.0000050, acc=0.999\n",
      "[Epoch 12 Batch 48/347] loss=0.1290, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 52/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 12 Batch 56/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 12 Batch 60/347] loss=0.0399, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 64/347] loss=0.0004, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 68/347] loss=0.0375, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 72/347] loss=0.0004, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 76/347] loss=0.0434, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 80/347] loss=0.0002, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 84/347] loss=0.0008, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 88/347] loss=0.0159, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 92/347] loss=0.0011, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 96/347] loss=0.0561, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 100/347] loss=0.0740, lr=0.0000050, acc=0.993\n",
      "[Epoch 12 Batch 104/347] loss=0.0003, lr=0.0000050, acc=0.993\n",
      "[Epoch 12 Batch 108/347] loss=0.0039, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 112/347] loss=0.0697, lr=0.0000050, acc=0.993\n",
      "[Epoch 12 Batch 116/347] loss=0.0002, lr=0.0000050, acc=0.993\n",
      "[Epoch 12 Batch 120/347] loss=0.0001, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 124/347] loss=0.0001, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 128/347] loss=0.0009, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 132/347] loss=0.0003, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 136/347] loss=0.0001, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 140/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 144/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 148/347] loss=0.0043, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 152/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 156/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 160/347] loss=0.0811, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 164/347] loss=0.0365, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 168/347] loss=0.0097, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 172/347] loss=0.0001, lr=0.0000050, acc=0.994\n",
      "[Epoch 12 Batch 176/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 180/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 184/347] loss=0.0007, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 188/347] loss=0.0069, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 192/347] loss=0.0004, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 196/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 200/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 204/347] loss=0.0004, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 208/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 12 Batch 212/347] loss=0.0019, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 216/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 220/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 224/347] loss=0.0010, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 228/347] loss=0.0004, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 232/347] loss=0.0019, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 236/347] loss=0.0022, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 240/347] loss=0.0088, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 244/347] loss=0.0030, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 248/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 252/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 256/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 260/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 264/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 268/347] loss=0.0021, lr=0.0000050, acc=0.996\n",
      "[Epoch 12 Batch 272/347] loss=0.0030, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 276/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 280/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 284/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 288/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 292/347] loss=0.0115, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 296/347] loss=0.0132, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 300/347] loss=0.0426, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 304/347] loss=0.0007, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 308/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 312/347] loss=0.0046, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 316/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 320/347] loss=0.0087, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 324/347] loss=0.0415, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 328/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 332/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 336/347] loss=0.1099, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 340/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 12 Batch 344/347] loss=0.0005, lr=0.0000050, acc=0.997\n",
      "Test F1: 0.4693140794223826, Accuracy Score: 0.8929351784413693\n",
      "[Epoch 13 Batch 4/347] loss=0.1814, lr=0.0000050, acc=0.969\n",
      "[Epoch 13 Batch 8/347] loss=0.0071, lr=0.0000050, acc=0.984\n",
      "[Epoch 13 Batch 12/347] loss=0.0007, lr=0.0000050, acc=0.990\n",
      "[Epoch 13 Batch 16/347] loss=0.0001, lr=0.0000050, acc=0.992\n",
      "[Epoch 13 Batch 20/347] loss=0.0418, lr=0.0000050, acc=0.991\n",
      "[Epoch 13 Batch 24/347] loss=0.0006, lr=0.0000050, acc=0.992\n",
      "[Epoch 13 Batch 28/347] loss=0.0149, lr=0.0000050, acc=0.991\n",
      "[Epoch 13 Batch 32/347] loss=0.0006, lr=0.0000050, acc=0.992\n",
      "[Epoch 13 Batch 36/347] loss=0.0011, lr=0.0000050, acc=0.993\n",
      "[Epoch 13 Batch 40/347] loss=0.0003, lr=0.0000050, acc=0.994\n",
      "[Epoch 13 Batch 44/347] loss=0.0001, lr=0.0000050, acc=0.994\n",
      "[Epoch 13 Batch 48/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 52/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 56/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 60/347] loss=0.0870, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 64/347] loss=0.0841, lr=0.0000050, acc=0.994\n",
      "[Epoch 13 Batch 68/347] loss=0.0002, lr=0.0000050, acc=0.994\n",
      "[Epoch 13 Batch 72/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 76/347] loss=0.0011, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 80/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 84/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 88/347] loss=0.0020, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 92/347] loss=0.0350, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 96/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 100/347] loss=0.0352, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 104/347] loss=0.0052, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 108/347] loss=0.0053, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 112/347] loss=0.0033, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 116/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 120/347] loss=0.0023, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 124/347] loss=0.0519, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 128/347] loss=0.0005, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 132/347] loss=0.2370, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 136/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 140/347] loss=0.0152, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 144/347] loss=0.0341, lr=0.0000050, acc=0.994\n",
      "[Epoch 13 Batch 148/347] loss=0.0002, lr=0.0000050, acc=0.994\n",
      "[Epoch 13 Batch 152/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 156/347] loss=0.0534, lr=0.0000050, acc=0.994\n",
      "[Epoch 13 Batch 160/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 164/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 168/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 172/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 176/347] loss=0.0244, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 180/347] loss=0.0377, lr=0.0000050, acc=0.994\n",
      "[Epoch 13 Batch 184/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 188/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 192/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 196/347] loss=0.0693, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 200/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 204/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 208/347] loss=0.0345, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 212/347] loss=0.0009, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 216/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 220/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 224/347] loss=0.0041, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 228/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 232/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 236/347] loss=0.0046, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 240/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 244/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 248/347] loss=0.0006, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 252/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 256/347] loss=0.0133, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 260/347] loss=0.2774, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 264/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 268/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 272/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 276/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 280/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 284/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 288/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 292/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 13 Batch 296/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 300/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 304/347] loss=0.0004, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 308/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 312/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 316/347] loss=0.0248, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 320/347] loss=0.0007, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 324/347] loss=0.0019, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 328/347] loss=0.1306, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 332/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 336/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 340/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 13 Batch 344/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "Test F1: 0.5482866043613708, Accuracy Score: 0.8943918426802622\n",
      "[Epoch 14 Batch 4/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 14 Batch 8/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 14 Batch 12/347] loss=0.0003, lr=0.0000050, acc=1.000\n",
      "[Epoch 14 Batch 16/347] loss=0.0357, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 20/347] loss=0.0078, lr=0.0000050, acc=0.997\n",
      "[Epoch 14 Batch 24/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 14 Batch 28/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 14 Batch 32/347] loss=0.0005, lr=0.0000050, acc=0.998\n",
      "[Epoch 14 Batch 36/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 14 Batch 40/347] loss=0.0003, lr=0.0000050, acc=0.998\n",
      "[Epoch 14 Batch 44/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 14 Batch 48/347] loss=0.1075, lr=0.0000050, acc=0.997\n",
      "[Epoch 14 Batch 52/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 14 Batch 56/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 14 Batch 60/347] loss=0.0866, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 64/347] loss=0.0033, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 68/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 72/347] loss=0.0161, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 76/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 80/347] loss=0.0371, lr=0.0000050, acc=0.995\n",
      "[Epoch 14 Batch 84/347] loss=0.0483, lr=0.0000050, acc=0.995\n",
      "[Epoch 14 Batch 88/347] loss=0.0128, lr=0.0000050, acc=0.994\n",
      "[Epoch 14 Batch 92/347] loss=0.0811, lr=0.0000050, acc=0.994\n",
      "[Epoch 14 Batch 96/347] loss=0.0003, lr=0.0000050, acc=0.994\n",
      "[Epoch 14 Batch 100/347] loss=0.0001, lr=0.0000050, acc=0.994\n",
      "[Epoch 14 Batch 104/347] loss=0.0001, lr=0.0000050, acc=0.994\n",
      "[Epoch 14 Batch 108/347] loss=0.0029, lr=0.0000050, acc=0.995\n",
      "[Epoch 14 Batch 112/347] loss=0.0005, lr=0.0000050, acc=0.995\n",
      "[Epoch 14 Batch 116/347] loss=0.0461, lr=0.0000050, acc=0.994\n",
      "[Epoch 14 Batch 120/347] loss=0.0012, lr=0.0000050, acc=0.994\n",
      "[Epoch 14 Batch 124/347] loss=0.0002, lr=0.0000050, acc=0.994\n",
      "[Epoch 14 Batch 128/347] loss=0.0004, lr=0.0000050, acc=0.995\n",
      "[Epoch 14 Batch 132/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 14 Batch 136/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 14 Batch 140/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 14 Batch 144/347] loss=0.0006, lr=0.0000050, acc=0.995\n",
      "[Epoch 14 Batch 148/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 14 Batch 152/347] loss=0.0004, lr=0.0000050, acc=0.995\n",
      "[Epoch 14 Batch 156/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 160/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 164/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 168/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 172/347] loss=0.0034, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 176/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 180/347] loss=0.0010, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 184/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 188/347] loss=0.1039, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 192/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 196/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 200/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 204/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 208/347] loss=0.0636, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 212/347] loss=0.0055, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 216/347] loss=0.0011, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 220/347] loss=0.0326, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 224/347] loss=0.0872, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 228/347] loss=0.0014, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 232/347] loss=0.0067, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 236/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 240/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 244/347] loss=0.0405, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 248/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 252/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 256/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 260/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 264/347] loss=0.0277, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 268/347] loss=0.0400, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 272/347] loss=0.0017, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 276/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 280/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 284/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 288/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 292/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 296/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 300/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 304/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 308/347] loss=0.0004, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 312/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 316/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 320/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 324/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 14 Batch 328/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 14 Batch 332/347] loss=0.0034, lr=0.0000050, acc=0.997\n",
      "[Epoch 14 Batch 336/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 14 Batch 340/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 14 Batch 344/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "Test F1: 0.5436893203883495, Accuracy Score: 0.8973051711580481\n",
      "[Epoch 15 Batch 4/347] loss=0.1163, lr=0.0000050, acc=0.984\n",
      "[Epoch 15 Batch 8/347] loss=0.0020, lr=0.0000050, acc=0.992\n",
      "[Epoch 15 Batch 12/347] loss=0.0589, lr=0.0000050, acc=0.984\n",
      "[Epoch 15 Batch 16/347] loss=0.0001, lr=0.0000050, acc=0.988\n",
      "[Epoch 15 Batch 20/347] loss=0.0001, lr=0.0000050, acc=0.991\n",
      "[Epoch 15 Batch 24/347] loss=0.0009, lr=0.0000050, acc=0.992\n",
      "[Epoch 15 Batch 28/347] loss=0.0002, lr=0.0000050, acc=0.993\n",
      "[Epoch 15 Batch 32/347] loss=0.0018, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 36/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 40/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 44/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 48/347] loss=0.0028, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 52/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 56/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 15 Batch 60/347] loss=0.0671, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 64/347] loss=0.0454, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 68/347] loss=0.0336, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 72/347] loss=0.0557, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 76/347] loss=0.0001, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 80/347] loss=0.0001, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 84/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 88/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 92/347] loss=0.0376, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 96/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 100/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 104/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 108/347] loss=0.0375, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 116/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 120/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 124/347] loss=0.0437, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 128/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 132/347] loss=0.0006, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 136/347] loss=0.1269, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 140/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 144/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 148/347] loss=0.1025, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 152/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 156/347] loss=0.0559, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 160/347] loss=0.0001, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 164/347] loss=0.0003, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 168/347] loss=0.0003, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 172/347] loss=0.0001, lr=0.0000050, acc=0.994\n",
      "[Epoch 15 Batch 176/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 180/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 184/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 188/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 192/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 196/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 200/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 204/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 208/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 216/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 220/347] loss=0.0391, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 224/347] loss=0.0052, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 228/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 232/347] loss=0.0005, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 236/347] loss=0.1911, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 240/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 244/347] loss=0.0002, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 248/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 252/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 256/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 260/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 264/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 268/347] loss=0.0220, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 272/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 276/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 280/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 284/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 288/347] loss=0.0702, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 292/347] loss=0.0009, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 296/347] loss=0.0014, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 300/347] loss=0.0059, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 304/347] loss=0.0101, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 308/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 312/347] loss=0.0706, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 316/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 320/347] loss=0.0098, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 324/347] loss=0.0304, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 328/347] loss=0.0179, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 332/347] loss=0.1465, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 336/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 15 Batch 340/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 15 Batch 344/347] loss=0.1907, lr=0.0000050, acc=0.995\n",
      "Test F1: 0.537037037037037, Accuracy Score: 0.8907501820830298\n",
      "[Epoch 16 Batch 4/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 16 Batch 8/347] loss=0.0006, lr=0.0000050, acc=1.000\n",
      "[Epoch 16 Batch 12/347] loss=0.0122, lr=0.0000050, acc=0.995\n",
      "[Epoch 16 Batch 16/347] loss=0.0033, lr=0.0000050, acc=0.996\n",
      "[Epoch 16 Batch 20/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 24/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 28/347] loss=0.0247, lr=0.0000050, acc=0.995\n",
      "[Epoch 16 Batch 32/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 16 Batch 36/347] loss=0.0003, lr=0.0000050, acc=0.996\n",
      "[Epoch 16 Batch 40/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 44/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 48/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 52/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 16 Batch 56/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 16 Batch 60/347] loss=0.1363, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 64/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 72/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 76/347] loss=0.0006, lr=0.0000050, acc=0.998\n",
      "[Epoch 16 Batch 80/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 16 Batch 84/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 16 Batch 88/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 16 Batch 92/347] loss=0.0011, lr=0.0000050, acc=0.998\n",
      "[Epoch 16 Batch 96/347] loss=0.0463, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 100/347] loss=0.0449, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 104/347] loss=0.0006, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 108/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 112/347] loss=0.0019, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 116/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 120/347] loss=0.0007, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 124/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 128/347] loss=0.0012, lr=0.0000050, acc=0.998\n",
      "[Epoch 16 Batch 132/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 16 Batch 136/347] loss=0.1374, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 140/347] loss=0.0254, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 144/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 148/347] loss=0.0216, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 152/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 156/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 160/347] loss=0.0004, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 164/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 168/347] loss=0.1047, lr=0.0000050, acc=0.996\n",
      "[Epoch 16 Batch 172/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 16 Batch 176/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 16 Batch 180/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 184/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 188/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 192/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 196/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 200/347] loss=0.0052, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 204/347] loss=0.0288, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 208/347] loss=0.0009, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 216/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 220/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 224/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 228/347] loss=0.0244, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 232/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 236/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 240/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 244/347] loss=0.0009, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 248/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 252/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 256/347] loss=0.0728, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 260/347] loss=0.0054, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 264/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 268/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 272/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 276/347] loss=0.0007, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 280/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 284/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 288/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 292/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 296/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 300/347] loss=0.0321, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 304/347] loss=0.0235, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 308/347] loss=0.0005, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 312/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 320/347] loss=0.0108, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 324/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 328/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 332/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 336/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 340/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 16 Batch 344/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "Test F1: 0.535031847133758, Accuracy Score: 0.8936635105608157\n",
      "[Epoch 17 Batch 4/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 17 Batch 8/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 17 Batch 12/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 17 Batch 16/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 17 Batch 20/347] loss=0.0245, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 24/347] loss=0.0263, lr=0.0000050, acc=0.995\n",
      "[Epoch 17 Batch 28/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 17 Batch 32/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 17 Batch 36/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 17 Batch 40/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 44/347] loss=0.0029, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 48/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 52/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 56/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 60/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 64/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 68/347] loss=0.1271, lr=0.0000050, acc=0.995\n",
      "[Epoch 17 Batch 72/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 17 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 17 Batch 80/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 17 Batch 84/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 17 Batch 88/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 17 Batch 92/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 96/347] loss=0.0006, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 100/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 104/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 108/347] loss=0.0092, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 116/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 120/347] loss=0.0007, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 124/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 128/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 132/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 136/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 140/347] loss=0.0046, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 144/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 148/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 152/347] loss=0.0010, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 156/347] loss=0.0165, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 160/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 164/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 168/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 172/347] loss=0.0960, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 176/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 180/347] loss=0.0632, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 184/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 188/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 192/347] loss=0.0464, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 196/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 200/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 204/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 208/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 212/347] loss=0.0089, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 216/347] loss=0.0045, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 220/347] loss=0.0056, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 224/347] loss=0.0222, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 228/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 232/347] loss=0.0005, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 236/347] loss=0.1057, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 240/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 244/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 248/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 252/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 260/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 264/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 272/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 276/347] loss=0.0006, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 280/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 284/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 288/347] loss=0.0162, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 292/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 296/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 304/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 308/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 17 Batch 312/347] loss=0.1169, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 316/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 320/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 324/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 332/347] loss=0.0168, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 336/347] loss=0.0026, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 340/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 17 Batch 344/347] loss=0.2291, lr=0.0000050, acc=0.997\n",
      "Test F1: 0.5705329153605015, Accuracy Score: 0.9002184996358339\n",
      "[Epoch 18 Batch 4/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 8/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 12/347] loss=0.0066, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 16/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 20/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 24/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 28/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 32/347] loss=0.0008, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 36/347] loss=0.0059, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 40/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 44/347] loss=0.0007, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 48/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 52/347] loss=0.0035, lr=0.0000050, acc=1.000\n",
      "[Epoch 18 Batch 56/347] loss=0.1230, lr=0.0000050, acc=0.998\n",
      "[Epoch 18 Batch 60/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 18 Batch 64/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 18 Batch 68/347] loss=0.0019, lr=0.0000050, acc=0.998\n",
      "[Epoch 18 Batch 72/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 18 Batch 76/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 18 Batch 80/347] loss=0.0004, lr=0.0000050, acc=0.998\n",
      "[Epoch 18 Batch 84/347] loss=0.1549, lr=0.0000050, acc=0.998\n",
      "[Epoch 18 Batch 88/347] loss=0.0588, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 92/347] loss=0.1131, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 96/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 100/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 104/347] loss=0.0468, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 108/347] loss=0.0006, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 116/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 120/347] loss=0.0370, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 124/347] loss=0.0781, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 128/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 132/347] loss=0.0198, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 136/347] loss=0.0015, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 140/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 144/347] loss=0.0018, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 148/347] loss=0.1187, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 152/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 156/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 160/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 164/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 168/347] loss=0.0991, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 172/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 176/347] loss=0.0004, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 180/347] loss=0.0193, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 184/347] loss=0.0008, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 188/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 192/347] loss=0.0034, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 196/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 200/347] loss=0.0015, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 204/347] loss=0.0786, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 208/347] loss=0.0004, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 212/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 216/347] loss=0.0826, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 220/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 224/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 228/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 232/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 236/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 240/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 244/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 248/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 260/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 264/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 268/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 272/347] loss=0.0029, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 276/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 280/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 284/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 288/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 292/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 296/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 300/347] loss=0.0147, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 304/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 308/347] loss=0.0006, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 312/347] loss=0.3209, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 320/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 324/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 328/347] loss=0.0006, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 332/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 336/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 18 Batch 340/347] loss=0.0667, lr=0.0000050, acc=0.996\n",
      "[Epoch 18 Batch 344/347] loss=0.0146, lr=0.0000050, acc=0.996\n",
      "Test F1: 0.5437500000000001, Accuracy Score: 0.8936635105608157\n",
      "[Epoch 19 Batch 4/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 19 Batch 8/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 19 Batch 12/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 19 Batch 16/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 19 Batch 20/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 19 Batch 24/347] loss=0.0795, lr=0.0000050, acc=0.997\n",
      "[Epoch 19 Batch 28/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 32/347] loss=0.0012, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 36/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 40/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 44/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 48/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 52/347] loss=0.0308, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 56/347] loss=0.0003, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 64/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 72/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 80/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 84/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 88/347] loss=0.0101, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 92/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 100/347] loss=0.0003, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 104/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 120/347] loss=0.0039, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 124/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 128/347] loss=0.0768, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 132/347] loss=0.0146, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 136/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 140/347] loss=0.0004, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 144/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 148/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 152/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 156/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 160/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 164/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 168/347] loss=0.0063, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 172/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 176/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 180/347] loss=0.0419, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 184/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 188/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 192/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 196/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 200/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 204/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 208/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 216/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 220/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 224/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 228/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 232/347] loss=0.0008, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 236/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 240/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 244/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 248/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 256/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 260/347] loss=0.1695, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 264/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 19 Batch 272/347] loss=0.0523, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 276/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 280/347] loss=0.0159, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 284/347] loss=0.0127, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 288/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 292/347] loss=0.0036, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 296/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 300/347] loss=0.0005, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 304/347] loss=0.0400, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 308/347] loss=0.0714, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 312/347] loss=0.0243, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 320/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 324/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 328/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 332/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 336/347] loss=0.0017, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 340/347] loss=0.0632, lr=0.0000050, acc=0.998\n",
      "[Epoch 19 Batch 344/347] loss=0.0070, lr=0.0000050, acc=0.998\n",
      "Test F1: 0.5666666666666667, Accuracy Score: 0.8863801893663511\n",
      "[Epoch 20 Batch 4/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 8/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 12/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 16/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 20/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 24/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 28/347] loss=0.0005, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 32/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 36/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 40/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 44/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 48/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 52/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 56/347] loss=0.0492, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 60/347] loss=0.0006, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 64/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 68/347] loss=0.0023, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 72/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 80/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 84/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 88/347] loss=0.0013, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 92/347] loss=0.0006, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 100/347] loss=0.0006, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 104/347] loss=0.0012, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 112/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 120/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 124/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 128/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 132/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 136/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 140/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 144/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 148/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 152/347] loss=0.0003, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 156/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 20 Batch 160/347] loss=0.0277, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 164/347] loss=0.0074, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 168/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 172/347] loss=0.0328, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 176/347] loss=0.0094, lr=0.0000050, acc=0.999\n",
      "[Epoch 20 Batch 180/347] loss=0.2373, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 184/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 188/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 192/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 196/347] loss=0.0104, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 200/347] loss=0.0784, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 204/347] loss=0.0092, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 208/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 212/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 216/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 220/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 224/347] loss=0.1737, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 228/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 232/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 236/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 240/347] loss=0.1015, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 244/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 248/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 256/347] loss=0.0223, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 260/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 264/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 268/347] loss=0.0016, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 272/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 276/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 280/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 284/347] loss=0.1050, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 288/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 292/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 296/347] loss=0.0004, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 304/347] loss=0.0583, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 308/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 312/347] loss=0.0007, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 316/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 320/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 324/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 332/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 336/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 340/347] loss=0.1594, lr=0.0000050, acc=0.998\n",
      "[Epoch 20 Batch 344/347] loss=0.0266, lr=0.0000050, acc=0.997\n",
      "Test F1: 0.5375000000000001, Accuracy Score: 0.8922068463219228\n",
      "[Epoch 21 Batch 4/347] loss=0.0786, lr=0.0000050, acc=0.969\n",
      "[Epoch 21 Batch 8/347] loss=0.0000, lr=0.0000050, acc=0.984\n",
      "[Epoch 21 Batch 12/347] loss=0.0000, lr=0.0000050, acc=0.990\n",
      "[Epoch 21 Batch 16/347] loss=0.1014, lr=0.0000050, acc=0.988\n",
      "[Epoch 21 Batch 20/347] loss=0.0037, lr=0.0000050, acc=0.990\n",
      "[Epoch 21 Batch 24/347] loss=0.0001, lr=0.0000050, acc=0.992\n",
      "[Epoch 21 Batch 28/347] loss=0.0001, lr=0.0000050, acc=0.993\n",
      "[Epoch 21 Batch 32/347] loss=0.0000, lr=0.0000050, acc=0.994\n",
      "[Epoch 21 Batch 36/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 21 Batch 40/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 21 Batch 44/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 21 Batch 48/347] loss=0.0006, lr=0.0000050, acc=0.996\n",
      "[Epoch 21 Batch 52/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 21 Batch 56/347] loss=0.0753, lr=0.0000050, acc=0.995\n",
      "[Epoch 21 Batch 60/347] loss=0.0114, lr=0.0000050, acc=0.996\n",
      "[Epoch 21 Batch 64/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 21 Batch 68/347] loss=0.0089, lr=0.0000050, acc=0.996\n",
      "[Epoch 21 Batch 72/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 21 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 80/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 84/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 88/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 92/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 100/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 104/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 108/347] loss=0.0008, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 116/347] loss=0.0010, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 120/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 124/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 128/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 132/347] loss=0.0079, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 136/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 140/347] loss=0.0219, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 144/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 148/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 152/347] loss=0.1214, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 156/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 160/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 164/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 168/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 172/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 176/347] loss=0.0004, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 180/347] loss=0.0902, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 184/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 188/347] loss=0.0022, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 192/347] loss=0.0008, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 196/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 200/347] loss=0.0563, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 204/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 208/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 212/347] loss=0.0022, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 216/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 220/347] loss=0.0032, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 224/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 228/347] loss=0.0004, lr=0.0000050, acc=0.997\n",
      "[Epoch 21 Batch 232/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 236/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 240/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 244/347] loss=0.0029, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 248/347] loss=0.0020, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 260/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 264/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 268/347] loss=0.0781, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 272/347] loss=0.0007, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 276/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 280/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 288/347] loss=0.0061, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 292/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 296/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 304/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 308/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 312/347] loss=0.0003, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 320/347] loss=0.0415, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 324/347] loss=0.0398, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 332/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 336/347] loss=0.0302, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 340/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 21 Batch 344/347] loss=0.0159, lr=0.0000050, acc=0.997\n",
      "Test F1: 0.535031847133758, Accuracy Score: 0.8936635105608157\n",
      "[Epoch 22 Batch 4/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 8/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 12/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 16/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 20/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 24/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 28/347] loss=0.0082, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 32/347] loss=0.0003, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 36/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 40/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 44/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 48/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 52/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 56/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 60/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 64/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 68/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 72/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 76/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 80/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 84/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 88/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 92/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 96/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 100/347] loss=0.0003, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 104/347] loss=0.0003, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 108/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 112/347] loss=0.0340, lr=0.0000050, acc=0.999\n",
      "[Epoch 22 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 22 Batch 120/347] loss=0.0003, lr=0.0000050, acc=0.999\n",
      "[Epoch 22 Batch 124/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 22 Batch 128/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 132/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 136/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 140/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 144/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 148/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 152/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 156/347] loss=0.0013, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 160/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 164/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 168/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 172/347] loss=0.0068, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 176/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 180/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 184/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 188/347] loss=0.0068, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 192/347] loss=0.0005, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 196/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 200/347] loss=0.0095, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 204/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 208/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 212/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 216/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 220/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 224/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 228/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 232/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 236/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 240/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 244/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 248/347] loss=0.0003, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 252/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 256/347] loss=0.0018, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 260/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 264/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 268/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 272/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 276/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 280/347] loss=0.0053, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 284/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 288/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 292/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 296/347] loss=0.0006, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 300/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 304/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 308/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 312/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 316/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 320/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 324/347] loss=0.0117, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 328/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 332/347] loss=0.0138, lr=0.0000050, acc=1.000\n",
      "[Epoch 22 Batch 336/347] loss=0.0391, lr=0.0000050, acc=0.999\n",
      "[Epoch 22 Batch 340/347] loss=0.0072, lr=0.0000050, acc=0.999\n",
      "[Epoch 22 Batch 344/347] loss=0.1216, lr=0.0000050, acc=0.999\n",
      "Test F1: 0.5938375350140056, Accuracy Score: 0.8943918426802622\n",
      "[Epoch 23 Batch 4/347] loss=0.0075, lr=0.0000050, acc=1.000\n",
      "[Epoch 23 Batch 8/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 23 Batch 12/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 23 Batch 16/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 23 Batch 20/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 23 Batch 24/347] loss=0.0103, lr=0.0000050, acc=1.000\n",
      "[Epoch 23 Batch 28/347] loss=0.0627, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 32/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 36/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 40/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 44/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 23 Batch 48/347] loss=0.4058, lr=0.0000050, acc=0.996\n",
      "[Epoch 23 Batch 52/347] loss=0.2506, lr=0.0000050, acc=0.994\n",
      "[Epoch 23 Batch 56/347] loss=0.0000, lr=0.0000050, acc=0.994\n",
      "[Epoch 23 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 23 Batch 64/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 23 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 23 Batch 72/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 23 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 23 Batch 80/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 23 Batch 84/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 23 Batch 88/347] loss=0.0040, lr=0.0000050, acc=0.996\n",
      "[Epoch 23 Batch 92/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 100/347] loss=0.0155, lr=0.0000050, acc=0.996\n",
      "[Epoch 23 Batch 104/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 23 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 23 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 120/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 124/347] loss=0.0015, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 128/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 132/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 136/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 140/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 144/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 148/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 152/347] loss=0.0012, lr=0.0000050, acc=0.997\n",
      "[Epoch 23 Batch 156/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 160/347] loss=0.0005, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 164/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 168/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 172/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 176/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 180/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 184/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 188/347] loss=0.0005, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 192/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 196/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 200/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 204/347] loss=0.0125, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 208/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 216/347] loss=0.0014, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 220/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 224/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 228/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 232/347] loss=0.0043, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 236/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 240/347] loss=0.0139, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 244/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 248/347] loss=0.0045, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 260/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 264/347] loss=0.0049, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 272/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 276/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 280/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 288/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 292/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 296/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 304/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 308/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 312/347] loss=0.0946, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 316/347] loss=0.0040, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 320/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 324/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 332/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 336/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 340/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 23 Batch 344/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "Test F1: 0.5590062111801242, Accuracy Score: 0.8965768390386016\n",
      "[Epoch 24 Batch 4/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 8/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 12/347] loss=0.0039, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 16/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 20/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 24/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 28/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 32/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 36/347] loss=0.0044, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 40/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 44/347] loss=0.0021, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 48/347] loss=0.0902, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 52/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 56/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 64/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 72/347] loss=0.0008, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 80/347] loss=0.0085, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 84/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 88/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 92/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 100/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 104/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 116/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 120/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 124/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 128/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 132/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 136/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 140/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 144/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 148/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 152/347] loss=0.0018, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 156/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 24 Batch 160/347] loss=0.0199, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 164/347] loss=0.1502, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 168/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 172/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 176/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 180/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 184/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 188/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 192/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 196/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 200/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 204/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 208/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 216/347] loss=0.0848, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 220/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 224/347] loss=0.0009, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 228/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 232/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 236/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 240/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 244/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 248/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 256/347] loss=0.0681, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 260/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 264/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 268/347] loss=0.1190, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 272/347] loss=0.0005, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 276/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 280/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 288/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 292/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 296/347] loss=0.1700, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 304/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 308/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 24 Batch 312/347] loss=0.1196, lr=0.0000050, acc=0.998\n",
      "[Epoch 24 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 24 Batch 320/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 24 Batch 324/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 24 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 24 Batch 332/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 24 Batch 336/347] loss=0.0329, lr=0.0000050, acc=0.998\n",
      "[Epoch 24 Batch 340/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 24 Batch 344/347] loss=0.0005, lr=0.0000050, acc=0.998\n",
      "Test F1: 0.4797297297297297, Accuracy Score: 0.887836853605244\n",
      "[Epoch 25 Batch 4/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 25 Batch 8/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 25 Batch 12/347] loss=0.1395, lr=0.0000050, acc=0.995\n",
      "[Epoch 25 Batch 16/347] loss=0.1260, lr=0.0000050, acc=0.992\n",
      "[Epoch 25 Batch 20/347] loss=0.0047, lr=0.0000050, acc=0.994\n",
      "[Epoch 25 Batch 24/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 25 Batch 28/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 25 Batch 32/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 25 Batch 36/347] loss=0.0016, lr=0.0000050, acc=0.997\n",
      "[Epoch 25 Batch 40/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 25 Batch 44/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 25 Batch 48/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 25 Batch 52/347] loss=0.0404, lr=0.0000050, acc=0.996\n",
      "[Epoch 25 Batch 56/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 25 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 25 Batch 64/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 25 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 25 Batch 72/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 25 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 80/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 84/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 88/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 92/347] loss=0.0023, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 100/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 104/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 120/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 124/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 128/347] loss=0.0014, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 132/347] loss=0.0017, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 136/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 140/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 144/347] loss=0.0022, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 148/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 152/347] loss=0.0679, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 156/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 160/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 164/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 168/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 172/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 176/347] loss=0.0043, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 180/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 184/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 188/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 192/347] loss=0.1344, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 196/347] loss=0.0007, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 200/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 204/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 208/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 25 Batch 216/347] loss=0.0481, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 220/347] loss=0.1470, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 224/347] loss=0.0014, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 228/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 232/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 236/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 240/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 244/347] loss=0.0018, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 248/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 252/347] loss=0.0047, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 260/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 264/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 272/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 276/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 280/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 288/347] loss=0.0004, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 292/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 296/347] loss=0.1221, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 304/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 308/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 312/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 316/347] loss=0.0023, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 320/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 324/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 332/347] loss=0.1671, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 336/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 340/347] loss=0.0005, lr=0.0000050, acc=0.998\n",
      "[Epoch 25 Batch 344/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "Test F1: 0.5721925133689839, Accuracy Score: 0.8834668608885652\n",
      "[Epoch 26 Batch 4/347] loss=0.0030, lr=0.0000050, acc=1.000\n",
      "[Epoch 26 Batch 8/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 26 Batch 12/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 26 Batch 16/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 26 Batch 20/347] loss=0.1277, lr=0.0000050, acc=0.994\n",
      "[Epoch 26 Batch 24/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 26 Batch 28/347] loss=0.0996, lr=0.0000050, acc=0.993\n",
      "[Epoch 26 Batch 32/347] loss=0.0762, lr=0.0000050, acc=0.992\n",
      "[Epoch 26 Batch 36/347] loss=0.0000, lr=0.0000050, acc=0.993\n",
      "[Epoch 26 Batch 40/347] loss=0.0245, lr=0.0000050, acc=0.992\n",
      "[Epoch 26 Batch 44/347] loss=0.0048, lr=0.0000050, acc=0.993\n",
      "[Epoch 26 Batch 48/347] loss=0.0145, lr=0.0000050, acc=0.992\n",
      "[Epoch 26 Batch 52/347] loss=0.0001, lr=0.0000050, acc=0.993\n",
      "[Epoch 26 Batch 56/347] loss=0.0000, lr=0.0000050, acc=0.993\n",
      "[Epoch 26 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.994\n",
      "[Epoch 26 Batch 64/347] loss=0.0000, lr=0.0000050, acc=0.994\n",
      "[Epoch 26 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.994\n",
      "[Epoch 26 Batch 72/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 26 Batch 76/347] loss=0.0001, lr=0.0000050, acc=0.995\n",
      "[Epoch 26 Batch 80/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 26 Batch 84/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 26 Batch 88/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 26 Batch 92/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 26 Batch 96/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 26 Batch 100/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 26 Batch 104/347] loss=0.0002, lr=0.0000050, acc=0.996\n",
      "[Epoch 26 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 116/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 120/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 124/347] loss=0.0193, lr=0.0000050, acc=0.996\n",
      "[Epoch 26 Batch 128/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 132/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 136/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 140/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 144/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 148/347] loss=0.0006, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 152/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 156/347] loss=0.0037, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 160/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 164/347] loss=0.0370, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 168/347] loss=0.1356, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 172/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 176/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 180/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 184/347] loss=0.0300, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 188/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 192/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 196/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 200/347] loss=0.0495, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 204/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 208/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 216/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 220/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 224/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 228/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 232/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 236/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 240/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 244/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 248/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 252/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 260/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 264/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 272/347] loss=0.0007, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 276/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 26 Batch 280/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 288/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 292/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 296/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 304/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 308/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 312/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 320/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 324/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 332/347] loss=0.0012, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 336/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 340/347] loss=0.1844, lr=0.0000050, acc=0.998\n",
      "[Epoch 26 Batch 344/347] loss=0.0726, lr=0.0000050, acc=0.997\n",
      "Test F1: 0.5389610389610389, Accuracy Score: 0.8965768390386016\n",
      "[Epoch 27 Batch 4/347] loss=0.1334, lr=0.0000050, acc=0.984\n",
      "[Epoch 27 Batch 8/347] loss=0.0007, lr=0.0000050, acc=0.992\n",
      "[Epoch 27 Batch 12/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 27 Batch 16/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 27 Batch 20/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 24/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 28/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 32/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 36/347] loss=0.0061, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 40/347] loss=0.0625, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 44/347] loss=0.0192, lr=0.0000050, acc=0.996\n",
      "[Epoch 27 Batch 48/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 27 Batch 52/347] loss=0.0001, lr=0.0000050, acc=0.996\n",
      "[Epoch 27 Batch 56/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 64/347] loss=0.0006, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 72/347] loss=0.0448, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 80/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 84/347] loss=0.0014, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 88/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 92/347] loss=0.0004, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 100/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 104/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 108/347] loss=0.0009, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 112/347] loss=0.0006, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 120/347] loss=0.1116, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 124/347] loss=0.0010, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 128/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 132/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 136/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 140/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 144/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 148/347] loss=0.1522, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 152/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 156/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 160/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 164/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 168/347] loss=0.0010, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 172/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 176/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 180/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 184/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 188/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 192/347] loss=0.0159, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 196/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 200/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 204/347] loss=0.0005, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 208/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 216/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 220/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 224/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 228/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 232/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 236/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 240/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 244/347] loss=0.0845, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 248/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 260/347] loss=0.0961, lr=0.0000050, acc=0.998\n",
      "[Epoch 27 Batch 264/347] loss=0.0624, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 272/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 276/347] loss=0.1634, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 280/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 288/347] loss=0.0894, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 292/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 296/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 304/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 308/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 312/347] loss=0.0364, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 320/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 324/347] loss=0.0006, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 328/347] loss=0.0010, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 332/347] loss=0.0135, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 336/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 340/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 27 Batch 344/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "Test F1: 0.5205479452054794, Accuracy Score: 0.8980335032774945\n",
      "[Epoch 28 Batch 4/347] loss=0.0666, lr=0.0000050, acc=0.984\n",
      "[Epoch 28 Batch 8/347] loss=0.0000, lr=0.0000050, acc=0.992\n",
      "[Epoch 28 Batch 12/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 28 Batch 16/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 28 Batch 20/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 24/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 28/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 28 Batch 32/347] loss=0.0009, lr=0.0000050, acc=0.998\n",
      "[Epoch 28 Batch 36/347] loss=0.0006, lr=0.0000050, acc=0.998\n",
      "[Epoch 28 Batch 40/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 28 Batch 44/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 28 Batch 48/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 28 Batch 52/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 28 Batch 56/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 28 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 28 Batch 64/347] loss=0.0422, lr=0.0000050, acc=0.998\n",
      "[Epoch 28 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 28 Batch 72/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 28 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 28 Batch 80/347] loss=0.0495, lr=0.0000050, acc=0.998\n",
      "[Epoch 28 Batch 84/347] loss=0.1482, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 88/347] loss=0.0495, lr=0.0000050, acc=0.996\n",
      "[Epoch 28 Batch 92/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 100/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 104/347] loss=0.0008, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 108/347] loss=0.0910, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 120/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 124/347] loss=0.1098, lr=0.0000050, acc=0.996\n",
      "[Epoch 28 Batch 128/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 132/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 136/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 140/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 144/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 148/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 152/347] loss=0.0007, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 156/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 160/347] loss=0.0010, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 164/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 168/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 172/347] loss=0.1081, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 176/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 180/347] loss=0.0398, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 184/347] loss=0.1543, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 188/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 192/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 196/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 200/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 204/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 208/347] loss=0.1355, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 216/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 220/347] loss=0.0270, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 224/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 228/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 232/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 236/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 240/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 244/347] loss=0.0014, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 248/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 252/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 260/347] loss=0.0066, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 264/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 272/347] loss=0.1349, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 276/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 280/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 288/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 292/347] loss=0.0004, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 296/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 304/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 308/347] loss=0.0001, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 312/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 320/347] loss=0.0115, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 324/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 332/347] loss=0.0003, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 336/347] loss=0.0047, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 340/347] loss=0.0005, lr=0.0000050, acc=0.997\n",
      "[Epoch 28 Batch 344/347] loss=0.0002, lr=0.0000050, acc=0.997\n",
      "Test F1: 0.5333333333333333, Accuracy Score: 0.8929351784413693\n",
      "[Epoch 29 Batch 4/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 8/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 12/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 16/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 20/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 24/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 28/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 32/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 36/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 40/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 44/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 48/347] loss=0.0005, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 52/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 56/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 60/347] loss=0.0076, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 64/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 68/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 72/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 76/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 80/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 84/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 88/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 92/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 96/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 100/347] loss=0.0011, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 104/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 108/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 112/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 116/347] loss=0.0007, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 120/347] loss=0.0007, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 124/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 128/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 132/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 136/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 140/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 144/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 148/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 152/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 156/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 160/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 164/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 168/347] loss=0.0011, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 172/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 176/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 180/347] loss=0.0142, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 184/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 29 Batch 188/347] loss=0.0917, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 192/347] loss=0.0003, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 196/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 200/347] loss=0.0003, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 204/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 208/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 216/347] loss=0.1746, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 220/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 224/347] loss=0.0003, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 228/347] loss=0.0003, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 232/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 236/347] loss=0.0003, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 240/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 244/347] loss=0.0082, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 248/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 260/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 264/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 268/347] loss=0.0009, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 272/347] loss=0.0007, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 276/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 280/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 288/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 292/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 296/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 304/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 308/347] loss=0.0347, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 312/347] loss=0.0004, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 320/347] loss=0.1619, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 324/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 332/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 336/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 340/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 29 Batch 344/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "Test F1: 0.5307443365695792, Accuracy Score: 0.8943918426802622\n",
      "[Epoch 30 Batch 4/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 30 Batch 8/347] loss=0.0101, lr=0.0000050, acc=1.000\n",
      "[Epoch 30 Batch 12/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 30 Batch 16/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 30 Batch 20/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 30 Batch 24/347] loss=0.0029, lr=0.0000050, acc=1.000\n",
      "[Epoch 30 Batch 28/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 30 Batch 32/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 30 Batch 36/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 30 Batch 40/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 30 Batch 44/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 30 Batch 48/347] loss=0.1405, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 52/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 56/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 64/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 72/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 80/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 84/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 88/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 92/347] loss=0.0290, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 100/347] loss=0.0005, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 104/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 112/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 116/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 120/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 124/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 128/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 132/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 136/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 140/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 144/347] loss=0.0035, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 148/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 152/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 156/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 160/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 164/347] loss=0.0010, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 168/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 172/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 176/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 180/347] loss=0.0640, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 184/347] loss=0.0014, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 188/347] loss=0.0529, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 192/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 196/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 200/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 204/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 208/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 216/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 220/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 224/347] loss=0.0004, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 228/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 232/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 236/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 240/347] loss=0.0005, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 244/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 248/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 252/347] loss=0.0046, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 260/347] loss=0.0015, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 264/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 272/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 276/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 280/347] loss=0.0754, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 284/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 288/347] loss=0.1021, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 292/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 296/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 300/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 304/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 308/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 312/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 320/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 324/347] loss=0.0907, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 328/347] loss=0.0018, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 332/347] loss=0.0006, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 336/347] loss=0.0006, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 340/347] loss=0.0098, lr=0.0000050, acc=0.999\n",
      "[Epoch 30 Batch 344/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "Test F1: 0.4822695035460993, Accuracy Score: 0.8936635105608157\n",
      "[Epoch 31 Batch 4/347] loss=0.0004, lr=0.0000050, acc=1.000\n",
      "[Epoch 31 Batch 8/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 31 Batch 12/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 31 Batch 16/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 31 Batch 20/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 31 Batch 24/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 31 Batch 28/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 31 Batch 32/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 31 Batch 36/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 31 Batch 40/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 31 Batch 44/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 31 Batch 48/347] loss=0.0151, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 52/347] loss=0.1141, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 56/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 64/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 72/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 80/347] loss=0.1345, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 84/347] loss=0.0030, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 88/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 92/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 100/347] loss=0.1060, lr=0.0000050, acc=0.997\n",
      "[Epoch 31 Batch 104/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 112/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 120/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 124/347] loss=0.0004, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 128/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 132/347] loss=0.0086, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 136/347] loss=0.1595, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 140/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 144/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 148/347] loss=0.0003, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 152/347] loss=0.0003, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 156/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 160/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 164/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 168/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 172/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 176/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 180/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 184/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 188/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 192/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 196/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 200/347] loss=0.0856, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 204/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 208/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 212/347] loss=0.0009, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 216/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 220/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 224/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 228/347] loss=0.0095, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 232/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 236/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 240/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 244/347] loss=0.0022, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 248/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 260/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 264/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 272/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 276/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 280/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 288/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 292/347] loss=0.0892, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 296/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 304/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 308/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 312/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 320/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 324/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 332/347] loss=0.0642, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 336/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 31 Batch 340/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 31 Batch 344/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "Test F1: 0.5644171779141103, Accuracy Score: 0.8965768390386016\n",
      "[Epoch 32 Batch 4/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 8/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 12/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 16/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 20/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 24/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 28/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 32/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 36/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 40/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 44/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 48/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 52/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 56/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 60/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 64/347] loss=0.0166, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 72/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 76/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 80/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 84/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 88/347] loss=0.0003, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 92/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 100/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 104/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 120/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 124/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 128/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 132/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 136/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 140/347] loss=0.0055, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 144/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 148/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 152/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 156/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 160/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 164/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 168/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 172/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 176/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 180/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 184/347] loss=0.0003, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 188/347] loss=0.0075, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 192/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 196/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 200/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 204/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 208/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 212/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 32 Batch 216/347] loss=0.1057, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 220/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 224/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 228/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 232/347] loss=0.0026, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 236/347] loss=0.0032, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 240/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 244/347] loss=0.0152, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 248/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 260/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 264/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 272/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 276/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 280/347] loss=0.0003, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 288/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 292/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 296/347] loss=0.0537, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 304/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 308/347] loss=0.1131, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 312/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 320/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 324/347] loss=0.0003, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 332/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 336/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 340/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 32 Batch 344/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "Test F1: 0.5628742514970059, Accuracy Score: 0.8936635105608157\n",
      "[Epoch 33 Batch 4/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 33 Batch 8/347] loss=0.0002, lr=0.0000050, acc=1.000\n",
      "[Epoch 33 Batch 12/347] loss=0.0779, lr=0.0000050, acc=0.995\n",
      "[Epoch 33 Batch 16/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 33 Batch 20/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 33 Batch 24/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 33 Batch 28/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 32/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 36/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 40/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 44/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 48/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 52/347] loss=0.1363, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 56/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 64/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 72/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 76/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 80/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 84/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 88/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 92/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 100/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 104/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 120/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 124/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 128/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 132/347] loss=0.0005, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 136/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 140/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 144/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 148/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 152/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 156/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 160/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 164/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 168/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 172/347] loss=0.0384, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 176/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 180/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 184/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 188/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 192/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 196/347] loss=0.0011, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 200/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 204/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 208/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 212/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 216/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 220/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 224/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 228/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 232/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 236/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 240/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 244/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 248/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 252/347] loss=0.1051, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 256/347] loss=0.0003, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 260/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 264/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 272/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 276/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 280/347] loss=0.0926, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 288/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 292/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 296/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 300/347] loss=0.1475, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 304/347] loss=0.0235, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 308/347] loss=0.0260, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 312/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 320/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 324/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 328/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 332/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 336/347] loss=0.0005, lr=0.0000050, acc=0.998\n",
      "[Epoch 33 Batch 340/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 33 Batch 344/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "Test F1: 0.5780346820809249, Accuracy Score: 0.8936635105608157\n",
      "[Epoch 34 Batch 4/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 34 Batch 8/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 34 Batch 12/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 34 Batch 16/347] loss=0.0135, lr=0.0000050, acc=0.996\n",
      "[Epoch 34 Batch 20/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 34 Batch 24/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 34 Batch 28/347] loss=0.0026, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 32/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 36/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 40/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 44/347] loss=0.0103, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 48/347] loss=0.0008, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 52/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 56/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 64/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 68/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 72/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 76/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 80/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 84/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 88/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 92/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 100/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 104/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 120/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 124/347] loss=0.0006, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 128/347] loss=0.1356, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 132/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 136/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 140/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 144/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 148/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 152/347] loss=0.0004, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 156/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 160/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 164/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 168/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 172/347] loss=0.0465, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 176/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 180/347] loss=0.0019, lr=0.0000050, acc=0.999\n",
      "[Epoch 34 Batch 184/347] loss=0.0244, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 188/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 192/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 196/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 200/347] loss=0.0222, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 204/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 208/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 212/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 216/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 220/347] loss=0.0940, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 224/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 228/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 232/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 236/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 240/347] loss=0.0441, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 244/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 248/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 256/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 260/347] loss=0.0566, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 264/347] loss=0.0037, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 268/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 272/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 276/347] loss=0.1516, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 280/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 288/347] loss=0.0001, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 292/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 296/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 300/347] loss=0.0002, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 304/347] loss=0.0009, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 308/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 312/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 320/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 324/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 328/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 332/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 336/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 340/347] loss=0.0141, lr=0.0000050, acc=0.998\n",
      "[Epoch 34 Batch 344/347] loss=0.0000, lr=0.0000050, acc=0.998\n",
      "Test F1: 0.5316455696202531, Accuracy Score: 0.8922068463219228\n",
      "[Epoch 35 Batch 4/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 8/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 12/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 16/347] loss=0.0102, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 20/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 24/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 28/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 32/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 36/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 40/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 44/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 48/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 52/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 56/347] loss=0.0049, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 60/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 64/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 68/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 72/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 76/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 80/347] loss=0.1515, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 84/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 88/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 92/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 96/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 100/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 104/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 108/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 112/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 116/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 120/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 124/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 128/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 132/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 136/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 140/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 144/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 148/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 152/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 156/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 160/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 164/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 168/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 172/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 176/347] loss=0.0120, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 180/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 184/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 188/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 192/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 196/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 200/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 204/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 208/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 212/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 216/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 220/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 224/347] loss=0.1570, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 228/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 232/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 236/347] loss=0.0044, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 240/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 244/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 248/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 252/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 256/347] loss=0.0017, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 260/347] loss=0.0001, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 264/347] loss=0.0004, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 268/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 272/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 35 Batch 276/347] loss=0.1018, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 280/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 284/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 288/347] loss=0.0002, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 292/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 296/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 300/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 304/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 308/347] loss=0.1039, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 312/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 316/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 320/347] loss=0.0008, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 324/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 328/347] loss=0.0001, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 332/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 336/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 340/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "[Epoch 35 Batch 344/347] loss=0.0000, lr=0.0000050, acc=0.999\n",
      "Test F1: 0.5312499999999999, Accuracy Score: 0.8907501820830298\n",
      "[Epoch 36 Batch 4/347] loss=0.0000, lr=0.0000050, acc=1.000\n",
      "[Epoch 36 Batch 8/347] loss=0.0428, lr=0.0000050, acc=0.992\n",
      "[Epoch 36 Batch 12/347] loss=0.0000, lr=0.0000050, acc=0.995\n",
      "[Epoch 36 Batch 16/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 36 Batch 20/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 36 Batch 24/347] loss=0.0350, lr=0.0000050, acc=0.995\n",
      "[Epoch 36 Batch 28/347] loss=0.0003, lr=0.0000050, acc=0.995\n",
      "[Epoch 36 Batch 32/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 36 Batch 36/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 36 Batch 40/347] loss=0.0938, lr=0.0000050, acc=0.995\n",
      "[Epoch 36 Batch 44/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 36 Batch 48/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 36 Batch 52/347] loss=0.0000, lr=0.0000050, acc=0.996\n",
      "[Epoch 36 Batch 56/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 36 Batch 60/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 36 Batch 64/347] loss=0.0000, lr=0.0000050, acc=0.997\n",
      "[Epoch 36 Batch 68/347] loss=0.0030, lr=0.0000050, acc=0.997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-16751c325619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallreduce_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_global_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/gluonnlp/utils/parameter.py\u001b[0m in \u001b[0;36mclip_grad_global_norm\u001b[0;34m(parameters, max_norm, check_isfinite)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_isfinite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_finite\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             warnings.warn(\n\u001b[1;32m    149\u001b[0m                 UserWarning('nan or inf is detected. '\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnum_elements\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             raise ValueError(\"The truth value of an NDArray with multiple elements \" \\\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2552\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2533\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2535\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   2536\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The hyperparameters\n",
    "batch_size = 16\n",
    "lr = 5e-6\n",
    "\n",
    "\n",
    "\n",
    "trainer = mx.gluon.Trainer(bert_classifier.collect_params(), 'adam',\n",
    "                           {'learning_rate': lr, 'epsilon': 1e-9})\n",
    "\n",
    "# Collect all differentiable parameters\n",
    "# `grad_req == 'null'` indicates no gradients are calculated (e.g. constant parameters)\n",
    "# The gradients for these params are clipped later\n",
    "params = [p for p in bert_classifier.collect_params().values() if p.grad_req != 'null']\n",
    "grad_clip = 1\n",
    "\n",
    "# Training the model with only three epochs\n",
    "log_interval = 4\n",
    "num_epochs = 40\n",
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, segment_ids, valid_length, label) in enumerate(X_train_dataloader):\n",
    "        with mx.autograd.record():\n",
    "\n",
    "            # Load the data to the GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # Forward computation\n",
    "            out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # And backwards computation\n",
    "        ls.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(params, 1)\n",
    "        trainer.update(1)\n",
    "\n",
    "        step_loss += ls.asscalar()\n",
    "#         import ipdb; ipdb.set_trace() # debugging starts here\n",
    "        metric.update([label], [out])\n",
    "\n",
    "        # Printing vital information\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "                         .format(epoch_id, batch_id + 1, len(X_train_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1]))\n",
    "            step_loss = 0\n",
    "            \n",
    "    result = []\n",
    "    gt_label = []\n",
    "    for _, seqs in enumerate(X_val_dataloader):\n",
    "        token_ids, segment_ids, valid_length, label = seqs\n",
    "        token_ids = token_ids.as_in_context(ctx)\n",
    "        valid_length = valid_length.as_in_context(ctx)\n",
    "        segment_ids = segment_ids.as_in_context(ctx)\n",
    "        out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "        batch_labels = np.argmax(out, axis=1)\n",
    "        result += batch_labels.asnumpy().astype(int).tolist()\n",
    "        gt_label += label.asnumpy().flatten().tolist()\n",
    "    print(f'Test F1: {f1_score(gt_label, result)}, Accuracy Score: {accuracy_score(gt_label, result)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "Now we have all the pieces to put together, and we can finally start fine-tuning the\n",
    "model with very few epochs. For demonstration, we use a fixed learning rate and\n",
    "skip the validation steps. For the optimizer, we leverage the ADAM optimizer which\n",
    "performs very well for NLP data and for BERT models in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 4/434] loss=0.7231, lr=0.0000050, acc=0.469\n",
      "[Epoch 0 Batch 8/434] loss=0.6147, lr=0.0000050, acc=0.609\n",
      "[Epoch 0 Batch 12/434] loss=0.6109, lr=0.0000050, acc=0.677\n",
      "[Epoch 0 Batch 16/434] loss=0.6227, lr=0.0000050, acc=0.674\n",
      "[Epoch 0 Batch 20/434] loss=0.5898, lr=0.0000050, acc=0.683\n",
      "[Epoch 0 Batch 24/434] loss=0.5403, lr=0.0000050, acc=0.720\n",
      "[Epoch 0 Batch 28/434] loss=0.4930, lr=0.0000050, acc=0.752\n",
      "[Epoch 0 Batch 32/434] loss=0.5040, lr=0.0000050, acc=0.762\n",
      "[Epoch 0 Batch 36/434] loss=0.4732, lr=0.0000050, acc=0.777\n",
      "[Epoch 0 Batch 40/434] loss=0.4627, lr=0.0000050, acc=0.790\n",
      "[Epoch 0 Batch 44/434] loss=0.4754, lr=0.0000050, acc=0.797\n",
      "[Epoch 0 Batch 48/434] loss=0.5080, lr=0.0000050, acc=0.798\n",
      "[Epoch 0 Batch 52/434] loss=0.4316, lr=0.0000050, acc=0.805\n",
      "[Epoch 0 Batch 56/434] loss=0.4887, lr=0.0000050, acc=0.807\n",
      "[Epoch 0 Batch 60/434] loss=0.4164, lr=0.0000050, acc=0.812\n",
      "[Epoch 0 Batch 64/434] loss=0.3525, lr=0.0000050, acc=0.819\n",
      "[Epoch 0 Batch 68/434] loss=0.3794, lr=0.0000050, acc=0.824\n",
      "[Epoch 0 Batch 72/434] loss=0.5316, lr=0.0000050, acc=0.823\n",
      "[Epoch 0 Batch 76/434] loss=0.5331, lr=0.0000050, acc=0.820\n",
      "[Epoch 0 Batch 80/434] loss=0.4102, lr=0.0000050, acc=0.822\n",
      "[Epoch 0 Batch 84/434] loss=0.4599, lr=0.0000050, acc=0.823\n",
      "[Epoch 0 Batch 88/434] loss=0.2417, lr=0.0000050, acc=0.829\n",
      "[Epoch 0 Batch 92/434] loss=0.4687, lr=0.0000050, acc=0.829\n",
      "[Epoch 0 Batch 96/434] loss=0.3149, lr=0.0000050, acc=0.832\n",
      "[Epoch 0 Batch 100/434] loss=0.2341, lr=0.0000050, acc=0.837\n",
      "[Epoch 0 Batch 104/434] loss=0.4195, lr=0.0000050, acc=0.837\n",
      "[Epoch 0 Batch 108/434] loss=0.4393, lr=0.0000050, acc=0.837\n",
      "[Epoch 0 Batch 112/434] loss=0.3969, lr=0.0000050, acc=0.838\n",
      "[Epoch 0 Batch 116/434] loss=0.2784, lr=0.0000050, acc=0.840\n",
      "[Epoch 0 Batch 120/434] loss=0.2537, lr=0.0000050, acc=0.843\n",
      "[Epoch 0 Batch 124/434] loss=0.2436, lr=0.0000050, acc=0.846\n",
      "[Epoch 0 Batch 128/434] loss=0.2881, lr=0.0000050, acc=0.848\n",
      "[Epoch 0 Batch 132/434] loss=0.2882, lr=0.0000050, acc=0.850\n",
      "[Epoch 0 Batch 136/434] loss=0.3365, lr=0.0000050, acc=0.851\n",
      "[Epoch 0 Batch 140/434] loss=0.3122, lr=0.0000050, acc=0.852\n",
      "[Epoch 0 Batch 144/434] loss=0.4596, lr=0.0000050, acc=0.852\n",
      "[Epoch 0 Batch 148/434] loss=0.3450, lr=0.0000050, acc=0.852\n",
      "[Epoch 0 Batch 152/434] loss=0.2292, lr=0.0000050, acc=0.854\n",
      "[Epoch 0 Batch 156/434] loss=0.2679, lr=0.0000050, acc=0.856\n",
      "[Epoch 0 Batch 160/434] loss=0.4827, lr=0.0000050, acc=0.855\n",
      "[Epoch 0 Batch 164/434] loss=0.3131, lr=0.0000050, acc=0.856\n",
      "[Epoch 0 Batch 168/434] loss=0.3905, lr=0.0000050, acc=0.856\n",
      "[Epoch 0 Batch 172/434] loss=0.2800, lr=0.0000050, acc=0.857\n",
      "[Epoch 0 Batch 176/434] loss=0.2515, lr=0.0000050, acc=0.858\n",
      "[Epoch 0 Batch 180/434] loss=0.3734, lr=0.0000050, acc=0.859\n",
      "[Epoch 0 Batch 184/434] loss=0.3622, lr=0.0000050, acc=0.858\n",
      "[Epoch 0 Batch 188/434] loss=0.2638, lr=0.0000050, acc=0.860\n",
      "[Epoch 0 Batch 192/434] loss=0.4823, lr=0.0000050, acc=0.858\n",
      "[Epoch 0 Batch 196/434] loss=0.3090, lr=0.0000050, acc=0.859\n",
      "[Epoch 0 Batch 200/434] loss=0.3650, lr=0.0000050, acc=0.859\n",
      "[Epoch 0 Batch 204/434] loss=0.3404, lr=0.0000050, acc=0.859\n",
      "[Epoch 0 Batch 208/434] loss=0.3397, lr=0.0000050, acc=0.860\n",
      "[Epoch 0 Batch 212/434] loss=0.2841, lr=0.0000050, acc=0.860\n",
      "[Epoch 0 Batch 216/434] loss=0.4064, lr=0.0000050, acc=0.860\n",
      "[Epoch 0 Batch 220/434] loss=0.3686, lr=0.0000050, acc=0.860\n",
      "[Epoch 0 Batch 224/434] loss=0.3168, lr=0.0000050, acc=0.860\n",
      "[Epoch 0 Batch 228/434] loss=0.3260, lr=0.0000050, acc=0.861\n",
      "[Epoch 0 Batch 232/434] loss=0.4420, lr=0.0000050, acc=0.860\n",
      "[Epoch 0 Batch 236/434] loss=0.4487, lr=0.0000050, acc=0.859\n",
      "[Epoch 0 Batch 240/434] loss=0.2429, lr=0.0000050, acc=0.860\n",
      "[Epoch 0 Batch 244/434] loss=0.2223, lr=0.0000050, acc=0.862\n",
      "[Epoch 0 Batch 248/434] loss=0.2020, lr=0.0000050, acc=0.863\n",
      "[Epoch 0 Batch 252/434] loss=0.3175, lr=0.0000050, acc=0.863\n",
      "[Epoch 0 Batch 256/434] loss=0.2629, lr=0.0000050, acc=0.864\n",
      "[Epoch 0 Batch 260/434] loss=0.4307, lr=0.0000050, acc=0.864\n",
      "[Epoch 0 Batch 264/434] loss=0.1445, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 268/434] loss=0.3496, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 272/434] loss=0.3536, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 276/434] loss=0.5013, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 280/434] loss=0.2648, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 284/434] loss=0.2854, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 288/434] loss=0.4266, lr=0.0000050, acc=0.864\n",
      "[Epoch 0 Batch 292/434] loss=0.2307, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 296/434] loss=0.2829, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 300/434] loss=0.4238, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 304/434] loss=0.3230, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 308/434] loss=0.3043, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 312/434] loss=0.2799, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 316/434] loss=0.2187, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 320/434] loss=0.4728, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 324/434] loss=0.2680, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 328/434] loss=0.2602, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 332/434] loss=0.2927, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 336/434] loss=0.2551, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 340/434] loss=0.3075, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 344/434] loss=0.2630, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 348/434] loss=0.4162, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 352/434] loss=0.1323, lr=0.0000050, acc=0.865\n",
      "[Epoch 0 Batch 356/434] loss=0.2680, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 360/434] loss=0.1302, lr=0.0000050, acc=0.867\n",
      "[Epoch 0 Batch 364/434] loss=0.1800, lr=0.0000050, acc=0.867\n",
      "[Epoch 0 Batch 368/434] loss=0.5879, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 372/434] loss=0.3454, lr=0.0000050, acc=0.867\n",
      "[Epoch 0 Batch 376/434] loss=0.3844, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 380/434] loss=0.3194, lr=0.0000050, acc=0.867\n",
      "[Epoch 0 Batch 384/434] loss=0.2253, lr=0.0000050, acc=0.867\n",
      "[Epoch 0 Batch 388/434] loss=0.3359, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 392/434] loss=0.4731, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 396/434] loss=0.2607, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 400/434] loss=0.3144, lr=0.0000050, acc=0.866\n",
      "[Epoch 0 Batch 404/434] loss=0.2156, lr=0.0000050, acc=0.867\n",
      "[Epoch 0 Batch 408/434] loss=0.1229, lr=0.0000050, acc=0.868\n",
      "[Epoch 0 Batch 412/434] loss=0.4258, lr=0.0000050, acc=0.867\n",
      "[Epoch 0 Batch 416/434] loss=0.3651, lr=0.0000050, acc=0.867\n",
      "[Epoch 0 Batch 420/434] loss=0.2852, lr=0.0000050, acc=0.868\n",
      "[Epoch 0 Batch 424/434] loss=0.4483, lr=0.0000050, acc=0.867\n",
      "[Epoch 0 Batch 428/434] loss=0.1413, lr=0.0000050, acc=0.868\n",
      "[Epoch 0 Batch 432/434] loss=0.5171, lr=0.0000050, acc=0.868\n",
      "[Epoch 1 Batch 4/434] loss=0.2859, lr=0.0000050, acc=0.891\n",
      "[Epoch 1 Batch 8/434] loss=0.5621, lr=0.0000050, acc=0.877\n",
      "[Epoch 1 Batch 12/434] loss=0.3001, lr=0.0000050, acc=0.888\n",
      "[Epoch 1 Batch 16/434] loss=0.2685, lr=0.0000050, acc=0.888\n",
      "[Epoch 1 Batch 20/434] loss=0.2145, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 24/434] loss=0.3225, lr=0.0000050, acc=0.884\n",
      "[Epoch 1 Batch 28/434] loss=0.1562, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 32/434] loss=0.2842, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 36/434] loss=0.3030, lr=0.0000050, acc=0.891\n",
      "[Epoch 1 Batch 40/434] loss=0.3093, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 44/434] loss=0.3717, lr=0.0000050, acc=0.888\n",
      "[Epoch 1 Batch 48/434] loss=0.2423, lr=0.0000050, acc=0.890\n",
      "[Epoch 1 Batch 52/434] loss=0.1730, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 56/434] loss=0.1738, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 60/434] loss=0.2488, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 64/434] loss=0.1933, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 68/434] loss=0.2005, lr=0.0000050, acc=0.902\n",
      "[Epoch 1 Batch 72/434] loss=0.3776, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 76/434] loss=0.2080, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 80/434] loss=0.3491, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 84/434] loss=0.1255, lr=0.0000050, acc=0.901\n",
      "[Epoch 1 Batch 88/434] loss=0.2681, lr=0.0000050, acc=0.901\n",
      "[Epoch 1 Batch 92/434] loss=0.3645, lr=0.0000050, acc=0.899\n",
      "[Epoch 1 Batch 96/434] loss=0.2271, lr=0.0000050, acc=0.900\n",
      "[Epoch 1 Batch 100/434] loss=0.4056, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 104/434] loss=0.2932, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 108/434] loss=0.2989, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 112/434] loss=0.3193, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 116/434] loss=0.2537, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 120/434] loss=0.4034, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 124/434] loss=0.2230, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 128/434] loss=0.1752, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 132/434] loss=0.2010, lr=0.0000050, acc=0.898\n",
      "[Epoch 1 Batch 136/434] loss=0.3168, lr=0.0000050, acc=0.897\n",
      "[Epoch 1 Batch 140/434] loss=0.3189, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 144/434] loss=0.3756, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 148/434] loss=0.5362, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 152/434] loss=0.2459, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 156/434] loss=0.3339, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 160/434] loss=0.2714, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 164/434] loss=0.2196, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 168/434] loss=0.1806, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 172/434] loss=0.2408, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 176/434] loss=0.3201, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 180/434] loss=0.1660, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 184/434] loss=0.3252, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 188/434] loss=0.1305, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 192/434] loss=0.2870, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 196/434] loss=0.3421, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 200/434] loss=0.2404, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 204/434] loss=0.3623, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 208/434] loss=0.3016, lr=0.0000050, acc=0.891\n",
      "[Epoch 1 Batch 212/434] loss=0.1833, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 216/434] loss=0.2323, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 220/434] loss=0.2519, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 224/434] loss=0.3139, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 228/434] loss=0.3706, lr=0.0000050, acc=0.890\n",
      "[Epoch 1 Batch 232/434] loss=0.1855, lr=0.0000050, acc=0.891\n",
      "[Epoch 1 Batch 236/434] loss=0.2721, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 240/434] loss=0.2758, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 244/434] loss=0.4492, lr=0.0000050, acc=0.891\n",
      "[Epoch 1 Batch 248/434] loss=0.2508, lr=0.0000050, acc=0.890\n",
      "[Epoch 1 Batch 252/434] loss=0.2118, lr=0.0000050, acc=0.890\n",
      "[Epoch 1 Batch 256/434] loss=0.2408, lr=0.0000050, acc=0.890\n",
      "[Epoch 1 Batch 260/434] loss=0.1671, lr=0.0000050, acc=0.891\n",
      "[Epoch 1 Batch 264/434] loss=0.1757, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 268/434] loss=0.2769, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 272/434] loss=0.1555, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 276/434] loss=0.3881, lr=0.0000050, acc=0.892\n",
      "[Epoch 1 Batch 280/434] loss=0.2139, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 284/434] loss=0.3532, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 288/434] loss=0.3088, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 292/434] loss=0.1338, lr=0.0000050, acc=0.893\n",
      "[Epoch 1 Batch 296/434] loss=0.1732, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 300/434] loss=0.2530, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 304/434] loss=0.2126, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 308/434] loss=0.2866, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 312/434] loss=0.1713, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 316/434] loss=0.3197, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 320/434] loss=0.2341, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 324/434] loss=0.1857, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 328/434] loss=0.4394, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 332/434] loss=0.1858, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 336/434] loss=0.1587, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 340/434] loss=0.1655, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 344/434] loss=0.2310, lr=0.0000050, acc=0.896\n",
      "[Epoch 1 Batch 348/434] loss=0.2619, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 352/434] loss=0.2524, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 356/434] loss=0.2010, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 360/434] loss=0.2591, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 364/434] loss=0.2945, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 368/434] loss=0.2579, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 372/434] loss=0.2188, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 376/434] loss=0.3099, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 380/434] loss=0.2131, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 384/434] loss=0.2516, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 388/434] loss=0.2818, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 392/434] loss=0.1906, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 396/434] loss=0.2650, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 400/434] loss=0.3698, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 404/434] loss=0.1004, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 408/434] loss=0.3403, lr=0.0000050, acc=0.894\n",
      "[Epoch 1 Batch 412/434] loss=0.1986, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 416/434] loss=0.1445, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 420/434] loss=0.1264, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 424/434] loss=0.2814, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 428/434] loss=0.2350, lr=0.0000050, acc=0.895\n",
      "[Epoch 1 Batch 432/434] loss=0.2712, lr=0.0000050, acc=0.896\n",
      "[Epoch 2 Batch 4/434] loss=0.2024, lr=0.0000050, acc=0.922\n",
      "[Epoch 2 Batch 8/434] loss=0.2810, lr=0.0000050, acc=0.891\n",
      "[Epoch 2 Batch 12/434] loss=0.2009, lr=0.0000050, acc=0.885\n",
      "[Epoch 2 Batch 16/434] loss=0.1703, lr=0.0000050, acc=0.895\n",
      "[Epoch 2 Batch 20/434] loss=0.1362, lr=0.0000050, acc=0.909\n",
      "[Epoch 2 Batch 24/434] loss=0.2831, lr=0.0000050, acc=0.893\n",
      "[Epoch 2 Batch 28/434] loss=0.2993, lr=0.0000050, acc=0.888\n",
      "[Epoch 2 Batch 32/434] loss=0.2345, lr=0.0000050, acc=0.891\n",
      "[Epoch 2 Batch 36/434] loss=0.3384, lr=0.0000050, acc=0.892\n",
      "[Epoch 2 Batch 40/434] loss=0.2032, lr=0.0000050, acc=0.896\n",
      "[Epoch 2 Batch 44/434] loss=0.2405, lr=0.0000050, acc=0.897\n",
      "[Epoch 2 Batch 48/434] loss=0.2773, lr=0.0000050, acc=0.899\n",
      "[Epoch 2 Batch 52/434] loss=0.1857, lr=0.0000050, acc=0.899\n",
      "[Epoch 2 Batch 56/434] loss=0.2254, lr=0.0000050, acc=0.899\n",
      "[Epoch 2 Batch 60/434] loss=0.2716, lr=0.0000050, acc=0.896\n",
      "[Epoch 2 Batch 64/434] loss=0.1510, lr=0.0000050, acc=0.897\n",
      "[Epoch 2 Batch 68/434] loss=0.2128, lr=0.0000050, acc=0.899\n",
      "[Epoch 2 Batch 72/434] loss=0.1730, lr=0.0000050, acc=0.901\n",
      "[Epoch 2 Batch 76/434] loss=0.2054, lr=0.0000050, acc=0.902\n",
      "[Epoch 2 Batch 80/434] loss=0.2001, lr=0.0000050, acc=0.901\n",
      "[Epoch 2 Batch 84/434] loss=0.1475, lr=0.0000050, acc=0.903\n",
      "[Epoch 2 Batch 88/434] loss=0.2268, lr=0.0000050, acc=0.901\n",
      "[Epoch 2 Batch 92/434] loss=0.2129, lr=0.0000050, acc=0.902\n",
      "[Epoch 2 Batch 96/434] loss=0.1264, lr=0.0000050, acc=0.903\n",
      "[Epoch 2 Batch 100/434] loss=0.3716, lr=0.0000050, acc=0.902\n",
      "[Epoch 2 Batch 104/434] loss=0.1569, lr=0.0000050, acc=0.904\n",
      "[Epoch 2 Batch 108/434] loss=0.2104, lr=0.0000050, acc=0.905\n",
      "[Epoch 2 Batch 112/434] loss=0.2867, lr=0.0000050, acc=0.904\n",
      "[Epoch 2 Batch 116/434] loss=0.1992, lr=0.0000050, acc=0.905\n",
      "[Epoch 2 Batch 120/434] loss=0.2262, lr=0.0000050, acc=0.905\n",
      "[Epoch 2 Batch 124/434] loss=0.1038, lr=0.0000050, acc=0.905\n",
      "[Epoch 2 Batch 128/434] loss=0.1793, lr=0.0000050, acc=0.905\n",
      "[Epoch 2 Batch 132/434] loss=0.1948, lr=0.0000050, acc=0.907\n",
      "[Epoch 2 Batch 136/434] loss=0.2382, lr=0.0000050, acc=0.907\n",
      "[Epoch 2 Batch 140/434] loss=0.0613, lr=0.0000050, acc=0.909\n",
      "[Epoch 2 Batch 144/434] loss=0.0716, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 148/434] loss=0.2649, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 152/434] loss=0.2144, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 156/434] loss=0.2125, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 160/434] loss=0.0817, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 164/434] loss=0.1721, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 168/434] loss=0.1506, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 172/434] loss=0.1934, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 176/434] loss=0.4124, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 180/434] loss=0.2635, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 184/434] loss=0.3814, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 188/434] loss=0.0916, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 192/434] loss=0.2357, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 196/434] loss=0.2999, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 200/434] loss=0.2070, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 204/434] loss=0.1666, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 208/434] loss=0.2389, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 212/434] loss=0.2142, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 216/434] loss=0.2248, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 220/434] loss=0.2213, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 224/434] loss=0.1338, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 228/434] loss=0.1953, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 232/434] loss=0.2767, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 236/434] loss=0.2103, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 240/434] loss=0.2827, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 244/434] loss=0.2330, lr=0.0000050, acc=0.910\n",
      "[Epoch 2 Batch 248/434] loss=0.1959, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 252/434] loss=0.1767, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 256/434] loss=0.2984, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 260/434] loss=0.1656, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 264/434] loss=0.1716, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 268/434] loss=0.1482, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 272/434] loss=0.0827, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 276/434] loss=0.3834, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 280/434] loss=0.1230, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 284/434] loss=0.2734, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 288/434] loss=0.2264, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 292/434] loss=0.2021, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 296/434] loss=0.2072, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 300/434] loss=0.3913, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 304/434] loss=0.1540, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 308/434] loss=0.3115, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 312/434] loss=0.1760, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 316/434] loss=0.1361, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 320/434] loss=0.0815, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 324/434] loss=0.0657, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 328/434] loss=0.2296, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 332/434] loss=0.2986, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 336/434] loss=0.2952, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 340/434] loss=0.1903, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 344/434] loss=0.1896, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 348/434] loss=0.2836, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 352/434] loss=0.2259, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 356/434] loss=0.1380, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 360/434] loss=0.3422, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 364/434] loss=0.2413, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 368/434] loss=0.1617, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 372/434] loss=0.0806, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 376/434] loss=0.2378, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 380/434] loss=0.2049, lr=0.0000050, acc=0.911\n",
      "[Epoch 2 Batch 384/434] loss=0.1074, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 388/434] loss=0.0662, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 392/434] loss=0.2773, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 396/434] loss=0.2368, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 400/434] loss=0.1632, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 404/434] loss=0.2705, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 408/434] loss=0.2827, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 412/434] loss=0.2624, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 416/434] loss=0.1313, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 420/434] loss=0.2420, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 424/434] loss=0.1970, lr=0.0000050, acc=0.912\n",
      "[Epoch 2 Batch 428/434] loss=0.0992, lr=0.0000050, acc=0.913\n",
      "[Epoch 2 Batch 432/434] loss=0.3489, lr=0.0000050, acc=0.913\n",
      "[Epoch 3 Batch 4/434] loss=0.1401, lr=0.0000050, acc=0.922\n",
      "[Epoch 3 Batch 8/434] loss=0.2235, lr=0.0000050, acc=0.891\n",
      "[Epoch 3 Batch 12/434] loss=0.2256, lr=0.0000050, acc=0.891\n",
      "[Epoch 3 Batch 16/434] loss=0.1232, lr=0.0000050, acc=0.902\n",
      "[Epoch 3 Batch 20/434] loss=0.1932, lr=0.0000050, acc=0.906\n",
      "[Epoch 3 Batch 24/434] loss=0.1812, lr=0.0000050, acc=0.905\n",
      "[Epoch 3 Batch 28/434] loss=0.1151, lr=0.0000050, acc=0.910\n",
      "[Epoch 3 Batch 32/434] loss=0.0955, lr=0.0000050, acc=0.915\n",
      "[Epoch 3 Batch 36/434] loss=0.0888, lr=0.0000050, acc=0.921\n",
      "[Epoch 3 Batch 40/434] loss=0.1963, lr=0.0000050, acc=0.921\n",
      "[Epoch 3 Batch 44/434] loss=0.0750, lr=0.0000050, acc=0.924\n",
      "[Epoch 3 Batch 48/434] loss=0.0790, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 52/434] loss=0.3161, lr=0.0000050, acc=0.925\n",
      "[Epoch 3 Batch 56/434] loss=0.1205, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 60/434] loss=0.1763, lr=0.0000050, acc=0.928\n",
      "[Epoch 3 Batch 64/434] loss=0.1550, lr=0.0000050, acc=0.928\n",
      "[Epoch 3 Batch 68/434] loss=0.2725, lr=0.0000050, acc=0.927\n",
      "[Epoch 3 Batch 72/434] loss=0.1564, lr=0.0000050, acc=0.929\n",
      "[Epoch 3 Batch 76/434] loss=0.0549, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 80/434] loss=0.3141, lr=0.0000050, acc=0.929\n",
      "[Epoch 3 Batch 84/434] loss=0.2030, lr=0.0000050, acc=0.928\n",
      "[Epoch 3 Batch 88/434] loss=0.1544, lr=0.0000050, acc=0.929\n",
      "[Epoch 3 Batch 92/434] loss=0.1153, lr=0.0000050, acc=0.931\n",
      "[Epoch 3 Batch 96/434] loss=0.1679, lr=0.0000050, acc=0.930\n",
      "[Epoch 3 Batch 100/434] loss=0.1688, lr=0.0000050, acc=0.930\n",
      "[Epoch 3 Batch 104/434] loss=0.1532, lr=0.0000050, acc=0.931\n",
      "[Epoch 3 Batch 108/434] loss=0.0946, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 112/434] loss=0.3043, lr=0.0000050, acc=0.931\n",
      "[Epoch 3 Batch 116/434] loss=0.1758, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 120/434] loss=0.2252, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 124/434] loss=0.1702, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 128/434] loss=0.1491, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 132/434] loss=0.0695, lr=0.0000050, acc=0.935\n",
      "[Epoch 3 Batch 136/434] loss=0.0532, lr=0.0000050, acc=0.936\n",
      "[Epoch 3 Batch 140/434] loss=0.0839, lr=0.0000050, acc=0.937\n",
      "[Epoch 3 Batch 144/434] loss=0.1747, lr=0.0000050, acc=0.937\n",
      "[Epoch 3 Batch 148/434] loss=0.2221, lr=0.0000050, acc=0.936\n",
      "[Epoch 3 Batch 152/434] loss=0.0817, lr=0.0000050, acc=0.936\n",
      "[Epoch 3 Batch 156/434] loss=0.1364, lr=0.0000050, acc=0.937\n",
      "[Epoch 3 Batch 160/434] loss=0.1484, lr=0.0000050, acc=0.937\n",
      "[Epoch 3 Batch 164/434] loss=0.1650, lr=0.0000050, acc=0.937\n",
      "[Epoch 3 Batch 168/434] loss=0.2294, lr=0.0000050, acc=0.935\n",
      "[Epoch 3 Batch 172/434] loss=0.1125, lr=0.0000050, acc=0.936\n",
      "[Epoch 3 Batch 176/434] loss=0.1675, lr=0.0000050, acc=0.935\n",
      "[Epoch 3 Batch 180/434] loss=0.2243, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 184/434] loss=0.1715, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 188/434] loss=0.2084, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 192/434] loss=0.2063, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 196/434] loss=0.1287, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 200/434] loss=0.0475, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 204/434] loss=0.3557, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 208/434] loss=0.1982, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 212/434] loss=0.2203, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 216/434] loss=0.1214, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 220/434] loss=0.1421, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 224/434] loss=0.0812, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 228/434] loss=0.2344, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 232/434] loss=0.1662, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 236/434] loss=0.1367, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 240/434] loss=0.1414, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 244/434] loss=0.1643, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 248/434] loss=0.0661, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 252/434] loss=0.1909, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 256/434] loss=0.1903, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 260/434] loss=0.1601, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 264/434] loss=0.1549, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 268/434] loss=0.4095, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 272/434] loss=0.1353, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 276/434] loss=0.2417, lr=0.0000050, acc=0.931\n",
      "[Epoch 3 Batch 280/434] loss=0.1440, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 284/434] loss=0.1100, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 288/434] loss=0.1928, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 292/434] loss=0.0731, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 296/434] loss=0.1114, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 300/434] loss=0.0811, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 304/434] loss=0.1691, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 308/434] loss=0.1042, lr=0.0000050, acc=0.935\n",
      "[Epoch 3 Batch 312/434] loss=0.1643, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 316/434] loss=0.0917, lr=0.0000050, acc=0.935\n",
      "[Epoch 3 Batch 320/434] loss=0.3835, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 324/434] loss=0.2647, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 328/434] loss=0.0542, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 332/434] loss=0.1500, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 336/434] loss=0.3602, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 340/434] loss=0.0408, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 344/434] loss=0.1635, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 348/434] loss=0.1333, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 352/434] loss=0.3217, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 356/434] loss=0.2874, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 360/434] loss=0.0531, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 364/434] loss=0.2811, lr=0.0000050, acc=0.932\n",
      "[Epoch 3 Batch 368/434] loss=0.1370, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 372/434] loss=0.1627, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 376/434] loss=0.2261, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 380/434] loss=0.0306, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 384/434] loss=0.2492, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 388/434] loss=0.1856, lr=0.0000050, acc=0.933\n",
      "[Epoch 3 Batch 392/434] loss=0.0754, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 396/434] loss=0.1273, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 400/434] loss=0.1245, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 404/434] loss=0.1574, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 408/434] loss=0.1003, lr=0.0000050, acc=0.935\n",
      "[Epoch 3 Batch 412/434] loss=0.1378, lr=0.0000050, acc=0.935\n",
      "[Epoch 3 Batch 416/434] loss=0.1605, lr=0.0000050, acc=0.935\n",
      "[Epoch 3 Batch 420/434] loss=0.1612, lr=0.0000050, acc=0.935\n",
      "[Epoch 3 Batch 424/434] loss=0.2526, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 428/434] loss=0.2427, lr=0.0000050, acc=0.934\n",
      "[Epoch 3 Batch 432/434] loss=0.1652, lr=0.0000050, acc=0.934\n",
      "[Epoch 4 Batch 4/434] loss=0.1948, lr=0.0000050, acc=0.922\n",
      "[Epoch 4 Batch 8/434] loss=0.0160, lr=0.0000050, acc=0.960\n",
      "[Epoch 4 Batch 12/434] loss=0.1210, lr=0.0000050, acc=0.952\n",
      "[Epoch 4 Batch 16/434] loss=0.1113, lr=0.0000050, acc=0.948\n",
      "[Epoch 4 Batch 20/434] loss=0.1235, lr=0.0000050, acc=0.949\n",
      "[Epoch 4 Batch 24/434] loss=0.1378, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 28/434] loss=0.0652, lr=0.0000050, acc=0.957\n",
      "[Epoch 4 Batch 32/434] loss=0.0915, lr=0.0000050, acc=0.959\n",
      "[Epoch 4 Batch 36/434] loss=0.2069, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 40/434] loss=0.1435, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 44/434] loss=0.0499, lr=0.0000050, acc=0.957\n",
      "[Epoch 4 Batch 48/434] loss=0.1243, lr=0.0000050, acc=0.957\n",
      "[Epoch 4 Batch 52/434] loss=0.0818, lr=0.0000050, acc=0.959\n",
      "[Epoch 4 Batch 56/434] loss=0.1730, lr=0.0000050, acc=0.959\n",
      "[Epoch 4 Batch 60/434] loss=0.1160, lr=0.0000050, acc=0.957\n",
      "[Epoch 4 Batch 64/434] loss=0.2298, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 68/434] loss=0.1620, lr=0.0000050, acc=0.952\n",
      "[Epoch 4 Batch 72/434] loss=0.1337, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 76/434] loss=0.0729, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 80/434] loss=0.1470, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 84/434] loss=0.1498, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 88/434] loss=0.0371, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 92/434] loss=0.0209, lr=0.0000050, acc=0.957\n",
      "[Epoch 4 Batch 96/434] loss=0.0780, lr=0.0000050, acc=0.958\n",
      "[Epoch 4 Batch 100/434] loss=0.0856, lr=0.0000050, acc=0.959\n",
      "[Epoch 4 Batch 104/434] loss=0.1014, lr=0.0000050, acc=0.959\n",
      "[Epoch 4 Batch 108/434] loss=0.1690, lr=0.0000050, acc=0.958\n",
      "[Epoch 4 Batch 112/434] loss=0.1515, lr=0.0000050, acc=0.957\n",
      "[Epoch 4 Batch 116/434] loss=0.4303, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 120/434] loss=0.1566, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 124/434] loss=0.1573, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 128/434] loss=0.0275, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 132/434] loss=0.1102, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 136/434] loss=0.1556, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 140/434] loss=0.1660, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 144/434] loss=0.2129, lr=0.0000050, acc=0.951\n",
      "[Epoch 4 Batch 148/434] loss=0.1245, lr=0.0000050, acc=0.951\n",
      "[Epoch 4 Batch 152/434] loss=0.1479, lr=0.0000050, acc=0.951\n",
      "[Epoch 4 Batch 156/434] loss=0.0821, lr=0.0000050, acc=0.952\n",
      "[Epoch 4 Batch 160/434] loss=0.1705, lr=0.0000050, acc=0.952\n",
      "[Epoch 4 Batch 164/434] loss=0.0344, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 168/434] loss=0.0303, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 172/434] loss=0.2251, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 176/434] loss=0.2261, lr=0.0000050, acc=0.952\n",
      "[Epoch 4 Batch 180/434] loss=0.1033, lr=0.0000050, acc=0.952\n",
      "[Epoch 4 Batch 184/434] loss=0.2390, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 188/434] loss=0.1460, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 192/434] loss=0.0643, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 196/434] loss=0.0216, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 200/434] loss=0.2082, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 204/434] loss=0.1205, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 208/434] loss=0.0981, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 212/434] loss=0.0460, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 216/434] loss=0.1318, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 220/434] loss=0.0778, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 224/434] loss=0.1638, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 228/434] loss=0.0517, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 232/434] loss=0.1158, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 236/434] loss=0.0286, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 240/434] loss=0.2347, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 244/434] loss=0.0495, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 248/434] loss=0.1330, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 252/434] loss=0.0822, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 256/434] loss=0.1257, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 260/434] loss=0.2706, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 264/434] loss=0.3203, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 268/434] loss=0.1103, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 272/434] loss=0.0587, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 276/434] loss=0.1024, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 280/434] loss=0.2449, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 284/434] loss=0.1986, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 288/434] loss=0.0564, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 292/434] loss=0.1912, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 296/434] loss=0.1338, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 300/434] loss=0.1454, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 304/434] loss=0.0120, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 308/434] loss=0.0388, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 312/434] loss=0.0540, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 316/434] loss=0.0593, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 320/434] loss=0.2404, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 324/434] loss=0.1565, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 328/434] loss=0.0940, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 332/434] loss=0.0970, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 336/434] loss=0.1592, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 340/434] loss=0.1372, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 344/434] loss=0.0912, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 348/434] loss=0.1355, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 352/434] loss=0.0641, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 356/434] loss=0.0513, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 360/434] loss=0.1837, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 364/434] loss=0.2525, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 368/434] loss=0.1384, lr=0.0000050, acc=0.956\n",
      "[Epoch 4 Batch 372/434] loss=0.4767, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 376/434] loss=0.1748, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 380/434] loss=0.0325, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 384/434] loss=0.1388, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 388/434] loss=0.3134, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 392/434] loss=0.0948, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 396/434] loss=0.0985, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 400/434] loss=0.0718, lr=0.0000050, acc=0.955\n",
      "[Epoch 4 Batch 404/434] loss=0.2282, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 408/434] loss=0.1162, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 412/434] loss=0.2826, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 416/434] loss=0.1472, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 420/434] loss=0.0740, lr=0.0000050, acc=0.954\n",
      "[Epoch 4 Batch 424/434] loss=0.2691, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 428/434] loss=0.1392, lr=0.0000050, acc=0.953\n",
      "[Epoch 4 Batch 432/434] loss=0.3717, lr=0.0000050, acc=0.953\n",
      "[Epoch 5 Batch 4/434] loss=0.1233, lr=0.0000050, acc=0.969\n",
      "[Epoch 5 Batch 8/434] loss=0.1180, lr=0.0000050, acc=0.969\n",
      "[Epoch 5 Batch 12/434] loss=0.0105, lr=0.0000050, acc=0.979\n",
      "[Epoch 5 Batch 16/434] loss=0.0621, lr=0.0000050, acc=0.980\n",
      "[Epoch 5 Batch 20/434] loss=0.0497, lr=0.0000050, acc=0.981\n",
      "[Epoch 5 Batch 24/434] loss=0.1480, lr=0.0000050, acc=0.977\n",
      "[Epoch 5 Batch 28/434] loss=0.0401, lr=0.0000050, acc=0.978\n",
      "[Epoch 5 Batch 32/434] loss=0.0436, lr=0.0000050, acc=0.979\n",
      "[Epoch 5 Batch 36/434] loss=0.0368, lr=0.0000050, acc=0.981\n",
      "[Epoch 5 Batch 40/434] loss=0.0759, lr=0.0000050, acc=0.980\n",
      "[Epoch 5 Batch 44/434] loss=0.0644, lr=0.0000050, acc=0.980\n",
      "[Epoch 5 Batch 48/434] loss=0.0861, lr=0.0000050, acc=0.980\n",
      "[Epoch 5 Batch 52/434] loss=0.0227, lr=0.0000050, acc=0.982\n",
      "[Epoch 5 Batch 56/434] loss=0.0279, lr=0.0000050, acc=0.982\n",
      "[Epoch 5 Batch 60/434] loss=0.1224, lr=0.0000050, acc=0.979\n",
      "[Epoch 5 Batch 64/434] loss=0.1708, lr=0.0000050, acc=0.977\n",
      "[Epoch 5 Batch 68/434] loss=0.0834, lr=0.0000050, acc=0.975\n",
      "[Epoch 5 Batch 72/434] loss=0.0550, lr=0.0000050, acc=0.976\n",
      "[Epoch 5 Batch 76/434] loss=0.1477, lr=0.0000050, acc=0.975\n",
      "[Epoch 5 Batch 80/434] loss=0.0905, lr=0.0000050, acc=0.975\n",
      "[Epoch 5 Batch 84/434] loss=0.1042, lr=0.0000050, acc=0.974\n",
      "[Epoch 5 Batch 88/434] loss=0.0992, lr=0.0000050, acc=0.973\n",
      "[Epoch 5 Batch 92/434] loss=0.0772, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 96/434] loss=0.1372, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 100/434] loss=0.1137, lr=0.0000050, acc=0.970\n",
      "[Epoch 5 Batch 104/434] loss=0.0417, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 108/434] loss=0.1252, lr=0.0000050, acc=0.970\n",
      "[Epoch 5 Batch 112/434] loss=0.0689, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 116/434] loss=0.0859, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 120/434] loss=0.0339, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 124/434] loss=0.1216, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 128/434] loss=0.0462, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 132/434] loss=0.0921, lr=0.0000050, acc=0.973\n",
      "[Epoch 5 Batch 136/434] loss=0.2719, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 140/434] loss=0.1042, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 144/434] loss=0.0874, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 148/434] loss=0.0071, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 152/434] loss=0.1132, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 156/434] loss=0.0383, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 160/434] loss=0.0839, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 164/434] loss=0.0198, lr=0.0000050, acc=0.973\n",
      "[Epoch 5 Batch 168/434] loss=0.3136, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 172/434] loss=0.1341, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 176/434] loss=0.0780, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 180/434] loss=0.0072, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 184/434] loss=0.0844, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 188/434] loss=0.0585, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 192/434] loss=0.0663, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 196/434] loss=0.2657, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 200/434] loss=0.0264, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 204/434] loss=0.0255, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 208/434] loss=0.0307, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 212/434] loss=0.0401, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 216/434] loss=0.0602, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 220/434] loss=0.1569, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 224/434] loss=0.0184, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 228/434] loss=0.0651, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 232/434] loss=0.0416, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 236/434] loss=0.2415, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 240/434] loss=0.0497, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 244/434] loss=0.0752, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 248/434] loss=0.0902, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 252/434] loss=0.0171, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 256/434] loss=0.1477, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 260/434] loss=0.0748, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 264/434] loss=0.0685, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 268/434] loss=0.1061, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 272/434] loss=0.0754, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 276/434] loss=0.0241, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 280/434] loss=0.2175, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 284/434] loss=0.0257, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 288/434] loss=0.1879, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 292/434] loss=0.1040, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 296/434] loss=0.0509, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 300/434] loss=0.0161, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 304/434] loss=0.0156, lr=0.0000050, acc=0.973\n",
      "[Epoch 5 Batch 308/434] loss=0.1361, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 312/434] loss=0.0850, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 316/434] loss=0.1617, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 320/434] loss=0.1016, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 324/434] loss=0.1750, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 328/434] loss=0.0327, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 332/434] loss=0.1541, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 336/434] loss=0.0966, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 340/434] loss=0.1050, lr=0.0000050, acc=0.972\n",
      "[Epoch 5 Batch 344/434] loss=0.1196, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 348/434] loss=0.0461, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 352/434] loss=0.3497, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 356/434] loss=0.0389, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 360/434] loss=0.2771, lr=0.0000050, acc=0.970\n",
      "[Epoch 5 Batch 364/434] loss=0.0547, lr=0.0000050, acc=0.970\n",
      "[Epoch 5 Batch 368/434] loss=0.1742, lr=0.0000050, acc=0.970\n",
      "[Epoch 5 Batch 372/434] loss=0.0472, lr=0.0000050, acc=0.970\n",
      "[Epoch 5 Batch 376/434] loss=0.0072, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 380/434] loss=0.1214, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 384/434] loss=0.1018, lr=0.0000050, acc=0.970\n",
      "[Epoch 5 Batch 388/434] loss=0.0146, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 392/434] loss=0.1340, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 396/434] loss=0.0053, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 400/434] loss=0.0162, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 404/434] loss=0.0847, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 408/434] loss=0.0936, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 412/434] loss=0.1150, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 416/434] loss=0.2249, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 420/434] loss=0.1678, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 424/434] loss=0.0147, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 428/434] loss=0.2408, lr=0.0000050, acc=0.971\n",
      "[Epoch 5 Batch 432/434] loss=0.1540, lr=0.0000050, acc=0.970\n",
      "[Epoch 6 Batch 4/434] loss=0.0842, lr=0.0000050, acc=0.969\n",
      "[Epoch 6 Batch 8/434] loss=0.0107, lr=0.0000050, acc=0.984\n",
      "[Epoch 6 Batch 12/434] loss=0.1509, lr=0.0000050, acc=0.969\n",
      "[Epoch 6 Batch 16/434] loss=0.0091, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 20/434] loss=0.0491, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 24/434] loss=0.1250, lr=0.0000050, acc=0.971\n",
      "[Epoch 6 Batch 28/434] loss=0.0296, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 32/434] loss=0.0543, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 36/434] loss=0.0190, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 40/434] loss=0.1809, lr=0.0000050, acc=0.973\n",
      "[Epoch 6 Batch 44/434] loss=0.0064, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 48/434] loss=0.0374, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 52/434] loss=0.1480, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 56/434] loss=0.0116, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 60/434] loss=0.1620, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 64/434] loss=0.0543, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 68/434] loss=0.0177, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 72/434] loss=0.1156, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 76/434] loss=0.0394, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 80/434] loss=0.0700, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 84/434] loss=0.0129, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 88/434] loss=0.1455, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 92/434] loss=0.0084, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 96/434] loss=0.0738, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 100/434] loss=0.0853, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 104/434] loss=0.0222, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 108/434] loss=0.2105, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 112/434] loss=0.0384, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 116/434] loss=0.2058, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 120/434] loss=0.0600, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 124/434] loss=0.0549, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 128/434] loss=0.0398, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 132/434] loss=0.1081, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 136/434] loss=0.0161, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 140/434] loss=0.0719, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 144/434] loss=0.1644, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 148/434] loss=0.0320, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 152/434] loss=0.0735, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 156/434] loss=0.2071, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 160/434] loss=0.1147, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 164/434] loss=0.0935, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 168/434] loss=0.1594, lr=0.0000050, acc=0.974\n",
      "[Epoch 6 Batch 172/434] loss=0.0048, lr=0.0000050, acc=0.975\n",
      "[Epoch 6 Batch 176/434] loss=0.0051, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 180/434] loss=0.0079, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 184/434] loss=0.0546, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 188/434] loss=0.0623, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 192/434] loss=0.0809, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 196/434] loss=0.1168, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 200/434] loss=0.0531, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 204/434] loss=0.2340, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 208/434] loss=0.1017, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 212/434] loss=0.1221, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 216/434] loss=0.1193, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 220/434] loss=0.0364, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 224/434] loss=0.0092, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 228/434] loss=0.0373, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 232/434] loss=0.2174, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 236/434] loss=0.0872, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 240/434] loss=0.1494, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 244/434] loss=0.0297, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 248/434] loss=0.0085, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 252/434] loss=0.0047, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 256/434] loss=0.1414, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 260/434] loss=0.1453, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 264/434] loss=0.0361, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 268/434] loss=0.0703, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 272/434] loss=0.0633, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 276/434] loss=0.0073, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 280/434] loss=0.0145, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 284/434] loss=0.0206, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 288/434] loss=0.2021, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 292/434] loss=0.0423, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 296/434] loss=0.0968, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 300/434] loss=0.1109, lr=0.0000050, acc=0.976\n",
      "[Epoch 6 Batch 304/434] loss=0.0081, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 308/434] loss=0.0203, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 312/434] loss=0.0066, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 316/434] loss=0.0291, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 320/434] loss=0.1743, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 324/434] loss=0.0039, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 328/434] loss=0.0353, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 332/434] loss=0.1079, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 336/434] loss=0.0944, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 340/434] loss=0.0732, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 344/434] loss=0.1239, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 348/434] loss=0.0844, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 352/434] loss=0.1000, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 356/434] loss=0.0605, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 360/434] loss=0.0487, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 364/434] loss=0.0191, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 368/434] loss=0.1467, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 372/434] loss=0.0991, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 376/434] loss=0.0870, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 380/434] loss=0.0155, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 384/434] loss=0.0745, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 388/434] loss=0.0060, lr=0.0000050, acc=0.978\n",
      "[Epoch 6 Batch 392/434] loss=0.0587, lr=0.0000050, acc=0.978\n",
      "[Epoch 6 Batch 396/434] loss=0.2141, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 400/434] loss=0.0193, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 404/434] loss=0.1011, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 408/434] loss=0.1112, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 412/434] loss=0.0057, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 416/434] loss=0.1284, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 420/434] loss=0.0021, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 424/434] loss=0.0568, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 428/434] loss=0.0560, lr=0.0000050, acc=0.977\n",
      "[Epoch 6 Batch 432/434] loss=0.0184, lr=0.0000050, acc=0.977\n",
      "[Epoch 7 Batch 4/434] loss=0.0011, lr=0.0000050, acc=1.000\n",
      "[Epoch 7 Batch 8/434] loss=0.0023, lr=0.0000050, acc=1.000\n",
      "[Epoch 7 Batch 12/434] loss=0.0093, lr=0.0000050, acc=1.000\n",
      "[Epoch 7 Batch 16/434] loss=0.0067, lr=0.0000050, acc=1.000\n",
      "[Epoch 7 Batch 20/434] loss=0.0028, lr=0.0000050, acc=1.000\n",
      "[Epoch 7 Batch 24/434] loss=0.0470, lr=0.0000050, acc=0.997\n",
      "[Epoch 7 Batch 28/434] loss=0.0031, lr=0.0000050, acc=0.998\n",
      "[Epoch 7 Batch 32/434] loss=0.0030, lr=0.0000050, acc=0.998\n",
      "[Epoch 7 Batch 36/434] loss=0.0096, lr=0.0000050, acc=0.998\n",
      "[Epoch 7 Batch 40/434] loss=0.0964, lr=0.0000050, acc=0.995\n",
      "[Epoch 7 Batch 44/434] loss=0.0298, lr=0.0000050, acc=0.993\n",
      "[Epoch 7 Batch 48/434] loss=0.0106, lr=0.0000050, acc=0.993\n",
      "[Epoch 7 Batch 52/434] loss=0.0164, lr=0.0000050, acc=0.993\n",
      "[Epoch 7 Batch 56/434] loss=0.1129, lr=0.0000050, acc=0.992\n",
      "[Epoch 7 Batch 60/434] loss=0.0727, lr=0.0000050, acc=0.991\n",
      "[Epoch 7 Batch 64/434] loss=0.1825, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 68/434] loss=0.0105, lr=0.0000050, acc=0.989\n",
      "[Epoch 7 Batch 72/434] loss=0.0776, lr=0.0000050, acc=0.989\n",
      "[Epoch 7 Batch 76/434] loss=0.0100, lr=0.0000050, acc=0.989\n",
      "[Epoch 7 Batch 80/434] loss=0.0679, lr=0.0000050, acc=0.989\n",
      "[Epoch 7 Batch 84/434] loss=0.0039, lr=0.0000050, acc=0.990\n",
      "[Epoch 7 Batch 88/434] loss=0.0477, lr=0.0000050, acc=0.989\n",
      "[Epoch 7 Batch 92/434] loss=0.1051, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 96/434] loss=0.3078, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 100/434] loss=0.0573, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 104/434] loss=0.0633, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 108/434] loss=0.0517, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 112/434] loss=0.0573, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 116/434] loss=0.0050, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 120/434] loss=0.0123, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 124/434] loss=0.0227, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 128/434] loss=0.0021, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 132/434] loss=0.0015, lr=0.0000050, acc=0.989\n",
      "[Epoch 7 Batch 136/434] loss=0.0066, lr=0.0000050, acc=0.989\n",
      "[Epoch 7 Batch 140/434] loss=0.0958, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 144/434] loss=0.0218, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 148/434] loss=0.0058, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 152/434] loss=0.0085, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 156/434] loss=0.0824, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 160/434] loss=0.0729, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 164/434] loss=0.1449, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 168/434] loss=0.0346, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 172/434] loss=0.0016, lr=0.0000050, acc=0.988\n",
      "[Epoch 7 Batch 176/434] loss=0.0904, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 180/434] loss=0.0022, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 184/434] loss=0.0602, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 188/434] loss=0.0500, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 192/434] loss=0.0990, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 196/434] loss=0.0084, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 200/434] loss=0.0018, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 204/434] loss=0.0053, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 208/434] loss=0.0445, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 212/434] loss=0.0884, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 216/434] loss=0.0681, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 220/434] loss=0.1798, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 224/434] loss=0.1202, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 228/434] loss=0.1231, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 232/434] loss=0.0235, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 236/434] loss=0.0174, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 240/434] loss=0.0135, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 244/434] loss=0.1383, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 248/434] loss=0.0066, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 252/434] loss=0.0141, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 256/434] loss=0.0018, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 260/434] loss=0.0124, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 264/434] loss=0.0022, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 268/434] loss=0.0909, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 272/434] loss=0.0291, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 276/434] loss=0.0081, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 280/434] loss=0.0041, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 284/434] loss=0.0019, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 288/434] loss=0.0029, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 292/434] loss=0.0029, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 296/434] loss=0.0541, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 300/434] loss=0.1788, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 304/434] loss=0.0244, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 308/434] loss=0.0035, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 312/434] loss=0.0791, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 316/434] loss=0.1351, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 320/434] loss=0.0026, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 324/434] loss=0.0081, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 328/434] loss=0.0890, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 332/434] loss=0.0415, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 336/434] loss=0.0545, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 340/434] loss=0.0018, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 344/434] loss=0.0797, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 348/434] loss=0.1015, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 352/434] loss=0.0013, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 356/434] loss=0.0473, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 360/434] loss=0.0040, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 364/434] loss=0.0016, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 368/434] loss=0.0024, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 372/434] loss=0.0055, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 376/434] loss=0.0790, lr=0.0000050, acc=0.987\n",
      "[Epoch 7 Batch 380/434] loss=0.1223, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 384/434] loss=0.0717, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 388/434] loss=0.1117, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 392/434] loss=0.0931, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 396/434] loss=0.0737, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 400/434] loss=0.2107, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 404/434] loss=0.1855, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 408/434] loss=0.0946, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 412/434] loss=0.0043, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 416/434] loss=0.0069, lr=0.0000050, acc=0.986\n",
      "[Epoch 7 Batch 420/434] loss=0.2860, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 424/434] loss=0.0393, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 428/434] loss=0.1205, lr=0.0000050, acc=0.985\n",
      "[Epoch 7 Batch 432/434] loss=0.0012, lr=0.0000050, acc=0.985\n",
      "[Epoch 8 Batch 4/434] loss=0.0327, lr=0.0000050, acc=0.984\n",
      "[Epoch 8 Batch 8/434] loss=0.0551, lr=0.0000050, acc=0.977\n",
      "[Epoch 8 Batch 12/434] loss=0.0449, lr=0.0000050, acc=0.979\n",
      "[Epoch 8 Batch 16/434] loss=0.0121, lr=0.0000050, acc=0.984\n",
      "[Epoch 8 Batch 20/434] loss=0.0772, lr=0.0000050, acc=0.984\n",
      "[Epoch 8 Batch 24/434] loss=0.0072, lr=0.0000050, acc=0.987\n",
      "[Epoch 8 Batch 28/434] loss=0.1123, lr=0.0000050, acc=0.982\n",
      "[Epoch 8 Batch 32/434] loss=0.0612, lr=0.0000050, acc=0.982\n",
      "[Epoch 8 Batch 36/434] loss=0.0022, lr=0.0000050, acc=0.984\n",
      "[Epoch 8 Batch 40/434] loss=0.0362, lr=0.0000050, acc=0.984\n",
      "[Epoch 8 Batch 44/434] loss=0.0055, lr=0.0000050, acc=0.986\n",
      "[Epoch 8 Batch 48/434] loss=0.0025, lr=0.0000050, acc=0.987\n",
      "[Epoch 8 Batch 52/434] loss=0.1018, lr=0.0000050, acc=0.986\n",
      "[Epoch 8 Batch 56/434] loss=0.0027, lr=0.0000050, acc=0.987\n",
      "[Epoch 8 Batch 60/434] loss=0.0061, lr=0.0000050, acc=0.988\n",
      "[Epoch 8 Batch 64/434] loss=0.0142, lr=0.0000050, acc=0.988\n",
      "[Epoch 8 Batch 68/434] loss=0.0084, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 72/434] loss=0.1051, lr=0.0000050, acc=0.987\n",
      "[Epoch 8 Batch 76/434] loss=0.0004, lr=0.0000050, acc=0.988\n",
      "[Epoch 8 Batch 80/434] loss=0.0054, lr=0.0000050, acc=0.988\n",
      "[Epoch 8 Batch 84/434] loss=0.0011, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 88/434] loss=0.0017, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 92/434] loss=0.0483, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 96/434] loss=0.0014, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 100/434] loss=0.6021, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 104/434] loss=0.0344, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 108/434] loss=0.0020, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 112/434] loss=0.0072, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 116/434] loss=0.0839, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 120/434] loss=0.0980, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 124/434] loss=0.0126, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 128/434] loss=0.0345, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 132/434] loss=0.0047, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 136/434] loss=0.0121, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 140/434] loss=0.0209, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 144/434] loss=0.0151, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 148/434] loss=0.0264, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 152/434] loss=0.0010, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 156/434] loss=0.0903, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 160/434] loss=0.0009, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 164/434] loss=0.0708, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 168/434] loss=0.0395, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 172/434] loss=0.0011, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 176/434] loss=0.0235, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 180/434] loss=0.0008, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 184/434] loss=0.0024, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 188/434] loss=0.0008, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 192/434] loss=0.0125, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 196/434] loss=0.0513, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 200/434] loss=0.0009, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 204/434] loss=0.0449, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 208/434] loss=0.0470, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 212/434] loss=0.0796, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 216/434] loss=0.0494, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 220/434] loss=0.2729, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 224/434] loss=0.0008, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 228/434] loss=0.0009, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 232/434] loss=0.0030, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 236/434] loss=0.0015, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 240/434] loss=0.0624, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 244/434] loss=0.0043, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 248/434] loss=0.0283, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 252/434] loss=0.0558, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 256/434] loss=0.0021, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 260/434] loss=0.0197, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 264/434] loss=0.0046, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 268/434] loss=0.0021, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 272/434] loss=0.0664, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 276/434] loss=0.0013, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 280/434] loss=0.0163, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 284/434] loss=0.0817, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 288/434] loss=0.0929, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 292/434] loss=0.0106, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 296/434] loss=0.1354, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 300/434] loss=0.0010, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 304/434] loss=0.0015, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 308/434] loss=0.1000, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 312/434] loss=0.1202, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 316/434] loss=0.0009, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 320/434] loss=0.0424, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 324/434] loss=0.0017, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 328/434] loss=0.0612, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 332/434] loss=0.0024, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 336/434] loss=0.0008, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 340/434] loss=0.0008, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 344/434] loss=0.0008, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 348/434] loss=0.1386, lr=0.0000050, acc=0.989\n",
      "[Epoch 8 Batch 352/434] loss=0.0012, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 356/434] loss=0.0014, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 360/434] loss=0.0015, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 364/434] loss=0.0850, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 368/434] loss=0.0042, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 372/434] loss=0.0041, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 376/434] loss=0.0286, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 380/434] loss=0.0498, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 384/434] loss=0.0004, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 388/434] loss=0.1780, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 392/434] loss=0.0340, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 396/434] loss=0.0613, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 400/434] loss=0.0017, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 404/434] loss=0.0003, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 408/434] loss=0.0678, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 412/434] loss=0.0006, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 416/434] loss=0.0006, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 420/434] loss=0.0007, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 424/434] loss=0.0718, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 428/434] loss=0.1796, lr=0.0000050, acc=0.990\n",
      "[Epoch 8 Batch 432/434] loss=0.0659, lr=0.0000050, acc=0.990\n"
     ]
    }
   ],
   "source": [
    "# The hyperparameters\n",
    "batch_size = 16\n",
    "lr = 5e-6\n",
    "\n",
    "trainer = mx.gluon.Trainer(bert_classifier.collect_params(), 'adam',\n",
    "                           {'learning_rate': lr, 'epsilon': 1e-9})\n",
    "\n",
    "# Collect all differentiable parameters\n",
    "# `grad_req == 'null'` indicates no gradients are calculated (e.g. constant parameters)\n",
    "# The gradients for these params are clipped later\n",
    "params = [p for p in bert_classifier.collect_params().values() if p.grad_req != 'null']\n",
    "grad_clip = 1\n",
    "\n",
    "# Training the model with only three epochs\n",
    "log_interval = 4\n",
    "num_epochs = 9\n",
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, segment_ids, valid_length, label) in enumerate(bert_dataloader):\n",
    "        with mx.autograd.record():\n",
    "\n",
    "            # Load the data to the GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # Forward computation\n",
    "            out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # And backwards computation\n",
    "        ls.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(params, 1)\n",
    "        trainer.update(1)\n",
    "\n",
    "        step_loss += ls.asscalar()\n",
    "#         import ipdb; ipdb.set_trace() # debugging starts here\n",
    "        metric.update([label], [out])\n",
    "\n",
    "        # Printing vital information\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "                         .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1]))\n",
    "            step_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch_id in range(5, 8):\n",
    "#     metric.reset()\n",
    "#     step_loss = 0\n",
    "#     for batch_id, (token_ids, segment_ids, valid_length, label) in enumerate(bert_dataloader):\n",
    "#         with mx.autograd.record():\n",
    "\n",
    "#             # Load the data to the GPU\n",
    "#             token_ids = token_ids.as_in_context(ctx)\n",
    "#             valid_length = valid_length.as_in_context(ctx)\n",
    "#             segment_ids = segment_ids.as_in_context(ctx)\n",
    "#             label = label.as_in_context(ctx)\n",
    "\n",
    "#             # Forward computation\n",
    "#             out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "#             ls = loss_function(out, label).mean()\n",
    "\n",
    "#         # And backwards computation\n",
    "#         ls.backward()\n",
    "\n",
    "#         # Gradient clipping\n",
    "#         trainer.allreduce_grads()\n",
    "#         nlp.utils.clip_grad_global_norm(params, 1)\n",
    "#         trainer.update(1)\n",
    "\n",
    "#         step_loss += ls.asscalar()\n",
    "#         metric.update([label], [out])\n",
    "\n",
    "#         # Printing vital information\n",
    "#         if (batch_id + 1) % (log_interval) == 0:\n",
    "#             print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "#                          .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "#                                  step_loss / log_interval,\n",
    "#                                  trainer.learning_rate, metric.get()[1]))\n",
    "#             step_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('public_test_features.csv')\n",
    "test_df['question'] = test_df['question'].astype(str)\n",
    "test_df['answer'] = test_df['answer'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917</td>\n",
       "      <td>when does the electoral college votes</td>\n",
       "      <td>The Twelfth Amendment specifies how a Presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6587</td>\n",
       "      <td>what year lord of rings made ?</td>\n",
       "      <td>Tolkien 's work has been the subject of extens...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                               question  \\\n",
       "0   917  when does the electoral college votes   \n",
       "1  6587         what year lord of rings made ?   \n",
       "\n",
       "                                              answer  \n",
       "0  The Twelfth Amendment specifies how a Presiden...  \n",
       "1  Tolkien 's work has been the subject of extens...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2941"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trans = data.transform.BERTDatasetTransform(bert_tokenizer, \n",
    "                                                 max_len,\n",
    "                                                 class_labels=None,\n",
    "                                                 pad=True, \n",
    "                                                 pair=pair,\n",
    "                                                 has_label=False)\n",
    "\n",
    "test_data_raw = test_df[['question', 'answer']].values\n",
    "data_test_raw = mx.gluon.data.SimpleDataset(test_data_raw)\n",
    "data_test = data_test_raw.transform(test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ids = \n",
      "[    2  2043  2515  1996  6092  2267  4494     3  1996 11313  7450 27171\n",
      "  2129  1037  2343  1998  3580  2343  2024  2700  1998  5942  2169 20374\n",
      "  2000  3459  2028  3789  2005  2343  1998  2178  3789  2005  3580  2343\n",
      "  1012     3     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "valid length = \n",
      "38\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('token ids = \\n%s'%data_test[0][0])\n",
    "print('segment ids = \\n%s'%data_test[0][1])\n",
    "print('valid length = \\n%s'%data_test[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler for evaluation\n",
    "pad_val = vocabulary[vocabulary.padding_token]\n",
    "batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(axis=0, pad_val=pad_val),  # input\n",
    "    nlp.data.batchify.Pad(axis=0, pad_val=0),  # segment\n",
    "    nlp.data.batchify.Stack())  # lenght\n",
    "\n",
    "\n",
    "dev_dataloader = mx.gluon.data.DataLoader(data_test, batch_size=batch_size, num_workers=4,\n",
    "                                           shuffle=False, batchify_fn=batchify_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2941"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for _, seqs in enumerate(dev_dataloader):\n",
    "    token_ids, segment_ids, valid_length = seqs\n",
    "    token_ids = token_ids.as_in_context(ctx)\n",
    "    valid_length = valid_length.as_in_context(ctx)\n",
    "    segment_ids = segment_ids.as_in_context(ctx)\n",
    "    out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "    batch_labels = np.argmax(out, axis=1)\n",
    "    result += list(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2941"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = [i.as_in_context(mx.cpu()) for i in result]\n",
    "answer = np.array([i.asnumpy() for i in answer])\n",
    "answer = list(answer.flatten().astype(int))\n",
    "# dct = {0: 'N', 1: 'Y'}\n",
    "# answer = [dct[k] for k in answer]\n",
    "test_df['relevance'] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917</td>\n",
       "      <td>when does the electoral college votes</td>\n",
       "      <td>The Twelfth Amendment specifies how a Presiden...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6587</td>\n",
       "      <td>what year lord of rings made ?</td>\n",
       "      <td>Tolkien 's work has been the subject of extens...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5227</td>\n",
       "      <td>what countries are under the buddhism religion</td>\n",
       "      <td>Estimate of the worldwide Buddhist population ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4707</td>\n",
       "      <td>what does ( sic ) mean ?</td>\n",
       "      <td>Sic may also refer to:</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>700</td>\n",
       "      <td>when is it memorial day</td>\n",
       "      <td>In cases involving a family graveyard where re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>5590</td>\n",
       "      <td>how many ports are there in networking</td>\n",
       "      <td>That is , data packets are routed across the n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>5320</td>\n",
       "      <td>what genre is bloody beetroots</td>\n",
       "      <td>In fact , the only identifying public feature ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>1664</td>\n",
       "      <td>where is green bay packers from</td>\n",
       "      <td>They are members of the North Division of the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>1245</td>\n",
       "      <td>when did the civil war start and where</td>\n",
       "      <td>The Union marshaled the resources and manpower...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>4278</td>\n",
       "      <td>what are the three ossicles</td>\n",
       "      <td>The term `` ossicles '' literally means `` tin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2941 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                        question  \\\n",
       "0      917           when does the electoral college votes   \n",
       "1     6587                  what year lord of rings made ?   \n",
       "2     5227  what countries are under the buddhism religion   \n",
       "3     4707                        what does ( sic ) mean ?   \n",
       "4      700                         when is it memorial day   \n",
       "...    ...                                             ...   \n",
       "2936  5590          how many ports are there in networking   \n",
       "2937  5320                  what genre is bloody beetroots   \n",
       "2938  1664                 where is green bay packers from   \n",
       "2939  1245          when did the civil war start and where   \n",
       "2940  4278                     what are the three ossicles   \n",
       "\n",
       "                                                 answer  relevance  \n",
       "0     The Twelfth Amendment specifies how a Presiden...          0  \n",
       "1     Tolkien 's work has been the subject of extens...          0  \n",
       "2     Estimate of the worldwide Buddhist population ...          0  \n",
       "3                                Sic may also refer to:          0  \n",
       "4     In cases involving a family graveyard where re...          0  \n",
       "...                                                 ...        ...  \n",
       "2936  That is , data packets are routed across the n...          0  \n",
       "2937  In fact , the only identifying public feature ...          0  \n",
       "2938  They are members of the North Division of the ...          0  \n",
       "2939  The Union marshaled the resources and manpower...          0  \n",
       "2940  The term `` ossicles '' literally means `` tin...          0  \n",
       "\n",
       "[2941 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_df['relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060183611016661"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "177/2941"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = test_df[[\"ID\", \"relevance\"]]\n",
    "result_df.to_csv(\"test_submission_nlp2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6587</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4707</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>5590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>5320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>1664</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>1245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>4278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2941 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  relevance\n",
       "0      917          0\n",
       "1     6587          0\n",
       "2     5227          0\n",
       "3     4707          0\n",
       "4      700          0\n",
       "...    ...        ...\n",
       "2936  5590          0\n",
       "2937  5320          0\n",
       "2938  1664          0\n",
       "2939  1245          0\n",
       "2940  4278          0\n",
       "\n",
       "[2941 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
